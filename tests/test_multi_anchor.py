"""
Test du systÃ¨me Multi-Anchor pour EVA2SPORT
VÃ©rifie que toutes les frames d'annotation dans la plage sont utilisÃ©es
"""

import sys
import json
from pathlib import Path

# Ajouter le module eva2sport au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from eva2sport import EVA2SportPipeline
from eva2sport.config import Config
from eva2sport.utils import eva_logger


def test_multi_anchor_detection():
    """Test de dÃ©tection des frames multi-anchor"""
    
    print("ğŸ” TEST DÃ‰TECTION MULTI-ANCHOR")
    print("=" * 50)
    
    # Configuration test
    video_name = "SD_13_06_2025_cam1_PdB_S1_T959s_1"
    
    try:
        # 1. Charger la configuration pour analyser les annotations
        config = Config(
            video_name,
            segment_offset_before_seconds=2.0,
            segment_offset_after_seconds=2.0
        )
        
        print(f"1. ğŸ“„ Analyse des annotations pour: {video_name}")
        
        # Charger le fichier de configuration
        config_path = config.videos_dir / f"{video_name}_objects.json"
        if not config_path.exists():
            config_path = config.config_path
        
        with open(config_path, 'r', encoding='utf-8') as f:
            project_config = json.load(f)
        
        initial_annotations = project_config.get('initial_annotations', [])
        print(f"   ğŸ“Š Total annotations dans config: {len(initial_annotations)}")
        
        for i, ann in enumerate(initial_annotations):
            frame = ann.get('frame', 0)
            annotations_count = len(ann.get('annotations', []))
            print(f"   ğŸ“ Annotation {i+1}: Frame {frame} â†’ {annotations_count} objets")
        
        # 2. Tester get_all_annotations_in_range
        print("\n2. ğŸ¯ Test mÃ©thode multi-anchor...")
        
        # Calculer la plage de traitement
        fps = config.get_video_fps()
        reference_frame = initial_annotations[0].get('frame', 0) if initial_annotations else 0
        
        print(f"   ğŸ“Š FPS vidÃ©o: {fps}")
        print(f"   ğŸ“ Frame de rÃ©fÃ©rence: {reference_frame}")
        
        # Calculer les bornes du segment
        start_frame, end_frame, _ = config.calculate_segment_bounds_and_anchor(
            reference_frame, verbose=True
        )
        
        print(f"   ğŸ“ Plage de traitement: [{start_frame}, {end_frame}]")
        
        # Utiliser get_all_annotations_in_range
        anchor_annotations = config.get_all_annotations_in_range(initial_annotations)
        
        print(f"\nâœ… RÃ‰SULTAT MULTI-ANCHOR:")
        print(f"   ğŸ¯ Frames d'ancrage trouvÃ©es: {len(anchor_annotations)}")
        
        total_objects = 0
        for anchor in anchor_annotations:
            frame = anchor.get('frame', 0)
            objects_count = len(anchor.get('annotations', []))
            total_objects += objects_count
            print(f"   ğŸ“ Anchor frame {frame}: {objects_count} objets")
        
        print(f"   ğŸ“Š Total objets sur toutes les anchors: {total_objects}")
        
        # 3. Comparaison avec mÃ©thode single-anchor
        print("\n3. ğŸ”„ Comparaison single vs multi-anchor...")
        
        single_frame = config.get_closest_initial_annotation_frame(initial_annotations)
        single_annotation = next(
            (ann for ann in initial_annotations if ann.get('frame') == single_frame), 
            None
        )
        
        if single_annotation:
            single_objects = len(single_annotation.get('annotations', []))
            print(f"   ğŸ¯ MÃ©thode SINGLE-ANCHOR:")
            print(f"      ğŸ“ Frame unique: {single_frame}")
            print(f"      ğŸ“Š Objets: {single_objects}")
            
            print(f"\n   ğŸ“ˆ GAIN MULTI-ANCHOR:")
            print(f"      ğŸ¯ Frames: {len(anchor_annotations)} vs 1 (+{len(anchor_annotations)-1})")
            print(f"      ğŸ“Š Objets: {total_objects} vs {single_objects} (+{total_objects-single_objects})")
            
            improvement = ((total_objects - single_objects) / single_objects * 100) if single_objects > 0 else 0
            print(f"      ğŸ“ˆ AmÃ©lioration: +{improvement:.1f}% d'objets de rÃ©fÃ©rence")
        
        return {
            'status': 'success',
            'anchor_frames': len(anchor_annotations),
            'total_objects': total_objects,
            'frames_detected': [ann.get('frame') for ann in anchor_annotations]
        }
        
    except Exception as e:
        print(f"âŒ Erreur dans test dÃ©tection: {e}")
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'error': str(e)}


def test_multi_anchor_pipeline():
    """Test complet de la pipeline avec multi-anchor"""
    
    print("\nğŸš€ TEST PIPELINE MULTI-ANCHOR")
    print("=" * 50)
    
    # Configuration
    video_name = "SD_13_06_2025_cam1_PdB_S1_T959s_1"
    
    try:
        # 1. CrÃ©er la pipeline
        print("1. ğŸ—ï¸ CrÃ©ation de la pipeline multi-anchor...")
        pipeline = EVA2SportPipeline(
            video_name,
            segment_offset_before_seconds=3.0,
            segment_offset_after_seconds=3.0
        )
        print(f"   âœ… Pipeline crÃ©Ã©e pour: {video_name}")
        
        # 2. ExÃ©cution avec monitoring multi-anchor
        print("\n2. ğŸ¯ ExÃ©cution pipeline avec multi-anchor...")
        print("   âš¡ Surveillez les messages multi-anchor dans les logs...")
        
        results = pipeline.run_full_pipeline(
            force_extraction=True,
            export_video=True,
            video_params={
                'fps': 5,                      # FPS rÃ©duit pour test rapide
                'show_minimap': True,          # Inclure minimap
                'cleanup_frames': False,        # Nettoyer aprÃ¨s
                'force_regenerate': True       # Utiliser frames existantes
            }
        )
        
        # 3. VÃ©rification des rÃ©sultats
        print("\n3. ğŸ“Š RÃ‰SULTATS FINAUX")
        print("------------------------------")
        
        if results['status'] == 'success':
            print(f"   âœ… Status: {results['status']}")
            print(f"   ğŸ“Š Frames annotÃ©es: {results.get('frames_annotated', 'N/A')}")
            print(f"   ğŸ¯ Objets suivis: {results.get('objects_tracked', 'N/A')}")
            print(f"   ğŸ“„ Fichier JSON: {results['export_paths'].get('json', 'N/A')}")
            
            # Analyser le fichier JSON produit pour vÃ©rifier la qualitÃ©
            json_path = results['export_paths'].get('json')
            if json_path and Path(json_path).exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    tracking_data = json.load(f)
                
                annotations = tracking_data.get('annotations', {})
                total_annotations = sum(len(frame_anns) for frame_anns in annotations.values())
                
                print(f"   ğŸ“ˆ Annotations gÃ©nÃ©rÃ©es: {total_annotations}")
                print(f"   ğŸ¬ Frames traitÃ©es: {len(annotations)}")
                
                # VÃ©rifier s'il y a des mÃ©tadonnÃ©es multi-anchor
                metadata = tracking_data.get('metadata', {})
                if 'multi_anchor_used' in metadata:
                    print(f"   ğŸ¯ Multi-anchor utilisÃ©: {metadata['multi_anchor_used']}")
            
            return results
        else:
            print(f"   âŒ Status: {results['status']}")
            print(f"   ğŸ’¥ Erreur: {results.get('error', 'N/A')}")
            return results
            
    except Exception as e:
        print(f"âŒ Erreur dans pipeline multi-anchor: {e}")
        import traceback
        traceback.print_exc()
        return {'status': 'error', 'error': str(e)}


def test_multi_anchor_vs_single():
    """Test comparatif multi-anchor vs single-anchor"""
    
    print("\nâš–ï¸ TEST COMPARATIF MULTI vs SINGLE-ANCHOR")
    print("=" * 50)
    
    video_name = "SD_13_06_2025_cam1_PdB_S1_T959s_1"
    
    try:
        # Cette comparaison nÃ©cessiterait de modifier temporairement le code
        # pour dÃ©sactiver le multi-anchor, ce qui est complexe pour un test simple
        
        print("ğŸ“ COMPARAISON THÃ‰ORIQUE:")
        print("   ğŸ¯ Multi-anchor: Utilise TOUTES les frames d'annotation dans la plage")  
        print("   ğŸ¯ Single-anchor: Utilise SEULEMENT la frame la plus proche de l'Ã©vÃ©nement")
        print("   ğŸ“ˆ Avantage multi-anchor: Plus de points de vÃ©ritÃ© terrain = tracking plus robuste")
        print("   âš¡ CoÃ»t: LÃ©gÃ¨rement plus de calcul initial, mais meilleur rÃ©sultat global")
        
        # Analyser la configuration pour montrer la diffÃ©rence
        config = Config(
            video_name,
            segment_offset_before_seconds=2.0,
            segment_offset_after_seconds=2.0
        )
        
        config_path = config.videos_dir / f"{video_name}_objects.json"
        if not config_path.exists():
            config_path = config.config_path
            
        with open(config_path, 'r', encoding='utf-8') as f:
            project_config = json.load(f)
        
        initial_annotations = project_config.get('initial_annotations', [])
        multi_anchors = config.get_all_annotations_in_range(initial_annotations)
        single_frame = config.get_closest_initial_annotation_frame(initial_annotations)
        
        print(f"\nğŸ“Š ANALYSE POUR {video_name}:")
        print(f"   ğŸ¯ Single-anchor utiliserait: 1 frame ({single_frame})")
        print(f"   ğŸ¯ Multi-anchor utilise: {len(multi_anchors)} frames")
        print(f"   ğŸ“ˆ AmÃ©lioration: {len(multi_anchors)}x plus de donnÃ©es de rÃ©fÃ©rence!")
        
        return {
            'status': 'success',
            'single_anchor_frames': 1,
            'multi_anchor_frames': len(multi_anchors),
            'improvement_factor': len(multi_anchors)
        }
        
    except Exception as e:
        print(f"âŒ Erreur dans test comparatif: {e}")
        return {'status': 'error', 'error': str(e)}


if __name__ == "__main__":
    print("ğŸ§ª TESTS MULTI-ANCHOR EVA2SPORT")
    print("=" * 50)
    
    # Menu de choix
    print("Choisissez le test Ã  exÃ©cuter:")
    print("1. Test dÃ©tection multi-anchor (rapide)")
    print("2. Test pipeline complÃ¨te multi-anchor")
    print("3. Test comparatif multi vs single-anchor")
    print("4. ExÃ©cuter tous les tests")
    
    choice = input("\nVotre choix (1, 2, 3 ou 4): ").strip()
    
    results = []
    
    if choice == "1":
        print("\nğŸ¯ EXÃ‰CUTION TEST DÃ‰TECTION")
        results.append(test_multi_anchor_detection())
        
    elif choice == "2":
        print("\nğŸ¯ EXÃ‰CUTION TEST PIPELINE COMPLÃˆTE")
        results.append(test_multi_anchor_pipeline())
        
    elif choice == "3":
        print("\nğŸ¯ EXÃ‰CUTION TEST COMPARATIF")
        results.append(test_multi_anchor_vs_single())
        
    elif choice == "4":
        print("\nğŸ¯ EXÃ‰CUTION DE TOUS LES TESTS")
        results.append(test_multi_anchor_detection())
        results.append(test_multi_anchor_pipeline())
        results.append(test_multi_anchor_vs_single())
    
    else:
        print("âŒ Choix invalide")
        sys.exit(1)
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 50)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
    print("=" * 50)
    
    success_count = sum(1 for r in results if r.get('status') == 'success')
    total_tests = len(results)
    
    for i, result in enumerate(results, 1):
        status_icon = "âœ…" if result.get('status') == 'success' else "âŒ"
        print(f"{status_icon} Test {i}: {result.get('status', 'unknown')}")
        if result.get('status') == 'error':
            print(f"   ğŸ’¥ Erreur: {result.get('error', 'N/A')}")
    
    print(f"\nğŸ“Š RÃ©sultat global: {success_count}/{total_tests} tests rÃ©ussis")
    
    if success_count == total_tests:
        print("ğŸ‰ TOUS LES TESTS MULTI-ANCHOR RÃ‰USSIS!")
    else:
        print("âš ï¸ CERTAINS TESTS ONT Ã‰CHOUÃ‰")
        sys.exit(1) 