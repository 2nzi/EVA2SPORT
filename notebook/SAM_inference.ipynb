{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BTdC1gyYAdf0",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ SAM2 Video Segmentation Pipeline\n",
        "\n",
        "Pipeline compl√®te pour la segmentation vid√©o avec SAM2 utilisant un fichier de configuration JSON.\n",
        "\n",
        "## üìã Pr√©requis\n",
        "- Vid√©o source : `videos/nom_video.mp4`\n",
        "- Fichier config : `videos/nom_video_config.json`\n",
        "- Format de config : calibration cam√©ra + annotations initiales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A64nesoErZY",
        "outputId": "8ee71eb3-2531-4592-9a0e-2084ed53f57f"
      },
      "outputs": [],
      "source": [
        "using_colab = False\n",
        "\n",
        "def is_sam2_installed():\n",
        "    \"\"\"V√©rifie si SAM 2 est d√©j√† install√©\"\"\"\n",
        "    try:\n",
        "        import sam2\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def is_checkpoint_available():\n",
        "    \"\"\"V√©rifie si le checkpoint SAM 2 est disponible\"\"\"\n",
        "    import os\n",
        "    checkpoint_path = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "    return os.path.exists(checkpoint_path)\n",
        "\n",
        "# Condition combin√©e : Colab ET SAM 2 pas encore install√©\n",
        "if using_colab and (not is_sam2_installed() or not is_checkpoint_available()):\n",
        "    print(\"üîß Installation de SAM 2 en cours...\")\n",
        "    \n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git' #05/06/2025 main branch\n",
        "\n",
        "    !mkdir -p ../checkpoints/\n",
        "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "    # !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
        "\n",
        "    !rm -rf /content/sample_data/* # should remove all folders in your \"./content\" folder\n",
        "    import gc\n",
        "    import torch\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"‚úÖ Installation de SAM 2 termin√©e\")\n",
        "    \n",
        "elif using_colab and is_sam2_installed() and is_checkpoint_available():\n",
        "    print(\"‚úÖ SAM 2 d√©j√† install√© et checkpoint disponible - SKIP installation\")\n",
        "    \n",
        "elif not using_colab:\n",
        "    print(\"üñ•Ô∏è Mode local - Installation SAM 2 skipp√©e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k83nNbwoAdf1",
        "outputId": "4f54ec69-dede-40eb-ff63-cac024309b91"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üì¶ IMPORTS ET CONFIGURATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from pycocotools.mask import encode as encode_rle\n",
        "\n",
        "# Configuration device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"üñ•Ô∏è Device utilis√©: {device}\")\n",
        "\n",
        "# Optimisations CUDA\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"‚úÖ Optimisations CUDA activ√©es\")\n",
        "\n",
        "print(\"‚úÖ Environnement configur√©\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqk5rZs4Adf2",
        "outputId": "e69c45b1-bc44-4f79-9ad9-4707170d12b1"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ‚öôÔ∏è CONFIGURATION DU PROJET\n",
        "# =============================================================================\n",
        "\n",
        "# üìã CONFIGURATION PRINCIPALE - MODIFIEZ CES VALEURS\n",
        "VIDEO_NAME = \"SD_13_06_2025_1_PdB_S1_T959s\"  # ‚ö†Ô∏è Nom de la vid√©o (sans extension)\n",
        "if using_colab:\n",
        "    VIDEOS_DIR = \"./videos\"   # üìÅ Dossier contenant les vid√©o\n",
        "else:\n",
        "    VIDEOS_DIR = \"../data/videos\"   # üìÅ Dossier contenant les vid√©o\n",
        "\n",
        "FRAME_INTERVAL = 3       # üé¨ Intervalle entre frames (1=toutes, 10=1 sur 10)\n",
        "\n",
        "# üé¨ OPTIONS D'EXTRACTION DES FRAMES\n",
        "EXTRACT_FRAMES = True     # ‚úÖ True=Extraire, False=Skip extraction\n",
        "FORCE_EXTRACTION = False  # üîÑ True=Forcer m√™me si frames existent, False=Skip si existent\n",
        "\n",
        "# üóÇÔ∏è Construction des chemins automatiques\n",
        "video_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}.mp4\"\n",
        "config_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}_config.json\"\n",
        "output_dir = Path(VIDEOS_DIR) / \"outputs\" / VIDEO_NAME\n",
        "frames_dir = output_dir / \"frames\"\n",
        "masks_dir = output_dir / \"masks\"\n",
        "output_video_path = output_dir / f\"{VIDEO_NAME}_annotated.mp4\"\n",
        "output_json_path = output_dir / f\"{VIDEO_NAME}_project.json\"\n",
        "\n",
        "# üèóÔ∏è Cr√©ation des dossiers\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "frames_dir.mkdir(exist_ok=True)\n",
        "masks_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# üîç V√©rification des fichiers\n",
        "print(f\"üìã CONFIGURATION DU PROJET:\")\n",
        "print(f\"   üé¨ Vid√©o: {video_path}\")\n",
        "print(f\"   üìÑ Config: {config_path}\")\n",
        "print(f\"   üìÅ Sortie: {output_dir}\")\n",
        "print(f\"   ‚èØÔ∏è  Intervalle frames: {FRAME_INTERVAL}\")\n",
        "print(f\"   üé¨ Extraction: {'‚úÖ Activ√©e' if EXTRACT_FRAMES else '‚ùå D√©sactiv√©e'}\")\n",
        "print(f\"   üîÑ Force extraction: {'‚úÖ Oui' if FORCE_EXTRACTION else '‚ùå Non'}\")\n",
        "\n",
        "# V√©rifications\n",
        "if not video_path.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Vid√©o non trouv√©e: {video_path}\")\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(f\"‚ùå Fichier config non trouv√©: {config_path}\")\n",
        "\n",
        "print(\"‚úÖ Configuration valid√©e\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klEyFoK9Adf2",
        "outputId": "e7abac42-8710-4f68-8988-51659bd44c33"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üìÑ CHARGEMENT DE LA CONFIGURATION JSON\n",
        "# =============================================================================\n",
        "\n",
        "def load_config_file(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Charge et valide le fichier de configuration JSON.\"\"\"\n",
        "    print(f\"üìÑ Chargement de la configuration: {config_path}\")\n",
        "\n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Validation de la structure\n",
        "    required_sections = ['calibration', 'objects', 'initial_annotations']\n",
        "    for section in required_sections:\n",
        "        if section not in config:\n",
        "            raise ValueError(f\"‚ùå Section '{section}' manquante dans le config\")\n",
        "\n",
        "    # Statistiques\n",
        "    num_objects = len(config['objects'])\n",
        "    num_annotations = 0\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        num_annotations += len(frame_data['annotations'])\n",
        "\n",
        "    print(f\"‚úÖ Configuration charg√©e:\")\n",
        "    print(f\"   üì∑ Calibration cam√©ra: OK\")\n",
        "    print(f\"   üéØ Objets d√©finis: {num_objects}\")\n",
        "    print(f\"   üìç Annotations initiales: {num_annotations}\")\n",
        "\n",
        "    # R√©sum√© des types d'objets\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_type = obj['obj_type']\n",
        "        obj_types[obj_type] = obj_types.get(obj_type, 0) + 1\n",
        "    print(f\"   üè∑Ô∏è  Types: {dict(obj_types)}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# Chargement de la configuration\n",
        "config = load_config_file(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzoA46BbAdf2",
        "outputId": "01f21875-44ba-4a30-8978-23025db22c4a"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üé¨ EXTRACTION DES FRAMES\n",
        "# =============================================================================\n",
        "\n",
        "def extract_frames(video_path: Path, frames_dir: Path, frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"Extrait les frames de la vid√©o selon l'intervalle sp√©cifi√©.\"\"\"\n",
        "\n",
        "    print(f\"üé¨ Extraction des frames...\")\n",
        "    print(f\"   üìπ Source: {video_path}\")\n",
        "    print(f\"   üìÅ Destination: {frames_dir}\")\n",
        "    print(f\"   ‚èØÔ∏è  Intervalle: {frame_interval}\")\n",
        "    print(f\"   üîÑ Force extraction: {'‚úÖ Oui' if force_extraction else '‚ùå Non'}\")\n",
        "\n",
        "    # V√©rification si extraction d√©j√† faite\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    if existing_frames and not force_extraction:\n",
        "        print(f\"üìÇ {len(existing_frames)} frames d√©j√† extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"üîÑ {len(existing_frames)} frames existantes - SUPPRESSION et r√©-extraction...\")\n",
        "        # Supprimer les frames existantes\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"üóëÔ∏è  Frames existantes supprim√©es\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"üìä Vid√©o: {total_frames} frames, {fps:.1f} FPS\")\n",
        "\n",
        "    extracted_count = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extraire seulement selon l'intervalle\n",
        "            if frame_idx % frame_interval == 0:\n",
        "                output_idx = frame_idx // frame_interval\n",
        "                filename = frames_dir / f\"{output_idx:05d}.jpg\"\n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "\n",
        "                if extracted_count % 50 == 0:\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    print(f\"üìä Progr√®s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"‚úÖ {extracted_count} frames extraites\")\n",
        "    return extracted_count\n",
        "\n",
        "# Extraction des frames\n",
        "if EXTRACT_FRAMES:\n",
        "    extracted_frames_count = extract_frames(video_path, frames_dir, FRAME_INTERVAL, FORCE_EXTRACTION)\n",
        "else:\n",
        "    # Skip extraction - compter les frames existantes\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    extracted_frames_count = len(existing_frames)\n",
        "\n",
        "    print(f\"‚è≠Ô∏è  Extraction d√©sactiv√©e\")\n",
        "    if extracted_frames_count > 0:\n",
        "        print(f\"üìÇ Utilisation de {extracted_frames_count} frames existantes\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Aucune frame trouv√©e dans {frames_dir}\")\n",
        "        print(f\"üí° Conseil: Activez EXTRACT_FRAMES=True pour extraire les frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cziqr2A3Adf2",
        "outputId": "13efa7fc-40f4-4e0d-9c66-03c88ca7b516"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ü§ñ INITIALISATION SAM2\n",
        "# =============================================================================\n",
        "\n",
        "# Import SAM2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Configuration des chemins SAM2\n",
        "if using_colab:\n",
        "    checkpoint_path = \"../../checkpoints/sam2.1_hiera_large.pt\"\n",
        "else:\n",
        "    checkpoint_path = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "\n",
        "model_config_path = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "print(f\"ü§ñ Initialisation SAM2...\")\n",
        "print(f\"   üß† Mod√®le: {model_config_path}\")\n",
        "print(f\"   üíæ Checkpoint: {checkpoint_path}\")\n",
        "print(f\"   üñ•Ô∏è  Device: {device}\")\n",
        "\n",
        "# Construction du predictor\n",
        "predictor = build_sam2_video_predictor(\n",
        "    config_file=model_config_path,\n",
        "    ckpt_path=checkpoint_path,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialisation de l'√©tat d'inf√©rence\n",
        "print(f\"\\nüé¨ Initialisation √©tat d'inf√©rence...\")\n",
        "print(f\"   üìÅ Frames: {frames_dir}\")\n",
        "\n",
        "inference_state = predictor.init_state(\n",
        "    video_path=str(frames_dir),\n",
        "    offload_video_to_cpu=True,    # √âconomise la m√©moire GPU\n",
        "    offload_state_to_cpu=False    # Garde l'√©tat en GPU\n",
        ")\n",
        "\n",
        "# Reset de l'√©tat\n",
        "predictor.reset_state(inference_state)\n",
        "\n",
        "# V√©rification\n",
        "loaded_frames = inference_state[\"num_frames\"]\n",
        "print(f\"\\n‚úÖ SAM2 initialis√©:\")\n",
        "print(f\"   üñºÔ∏è  Frames extraites: {extracted_frames_count}\")\n",
        "print(f\"   üé¨ Frames charg√©es: {loaded_frames}\")\n",
        "print(f\"   ‚úÖ Correspondance: {'OK' if extracted_frames_count == loaded_frames else 'ERREUR'}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    print(f\"   üíæ GPU Memory: {allocated:.2f}GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twDWK7TrAdf3",
        "outputId": "08b192e9-cbd5-45f7-8042-6bd65b4da128"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üéØ AJOUT DES ANNOTATIONS INITIALES\n",
        "# =============================================================================\n",
        "\n",
        "def add_initial_annotations(predictor, inference_state, config: Dict[str, Any]):\n",
        "    \"\"\"Ajoute les annotations initiales depuis la configuration.\"\"\"\n",
        "\n",
        "    print(f\"üéØ Ajout des annotations initiales...\")\n",
        "\n",
        "    # Cr√©ation du mapping obj_id -> obj_type\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_types[obj['obj_id']] = obj['obj_type']\n",
        "\n",
        "    # Extraction automatique des annotations et frames depuis le JSON\n",
        "    all_annotations = []\n",
        "    annotation_frames = []\n",
        "\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        frame_idx = frame_data['frame']\n",
        "        annotations = frame_data['annotations']\n",
        "        annotation_frames.append(frame_idx)\n",
        "\n",
        "        print(f\"   üìç Frame {frame_idx}: {len(annotations)} annotations\")\n",
        "\n",
        "        for annotation in annotations:\n",
        "            all_annotations.append({\n",
        "                'frame': frame_idx,\n",
        "                'obj_id': annotation['obj_id'],\n",
        "                'points': annotation['points'],\n",
        "                'obj_type': obj_types.get(annotation['obj_id'], f'unknown_{annotation[\"obj_id\"]}')\n",
        "            })\n",
        "\n",
        "    if not all_annotations:\n",
        "        raise ValueError(f\"‚ùå Aucune annotation trouv√©e dans le fichier config\")\n",
        "\n",
        "    print(f\"   üìä Total: {len(all_annotations)} annotations sur {len(set(annotation_frames))} frames\")\n",
        "\n",
        "    # Ajout des annotations √† SAM2\n",
        "    added_objects = []\n",
        "\n",
        "    for annotation_data in all_annotations:\n",
        "        frame_idx = annotation_data['frame']\n",
        "        obj_id = annotation_data['obj_id']\n",
        "        obj_type = annotation_data['obj_type']\n",
        "        points_data = annotation_data['points']\n",
        "\n",
        "        # Extraction des coordonn√©es et labels\n",
        "        points = np.array([[p['x'], p['y']] for p in points_data], dtype=np.float32)\n",
        "        labels = np.array([p['label'] for p in points_data], dtype=np.int32)\n",
        "\n",
        "        print(f\"   üéØ Frame {frame_idx} - Objet {obj_id} ({obj_type}): {len(points)} points √† ({points[0][0]:.0f}, {points[0][1]:.0f})\")\n",
        "\n",
        "        # Ajout √† SAM2 avec add_new_points_or_box\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state,\n",
        "            frame_idx,\n",
        "            obj_id,\n",
        "            points,\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # √âviter les doublons dans added_objects\n",
        "        if not any(obj['obj_id'] == obj_id for obj in added_objects):\n",
        "            added_objects.append({\n",
        "                'obj_id': obj_id,\n",
        "                'obj_type': obj_type,\n",
        "                'points_count': len(points)\n",
        "            })\n",
        "\n",
        "    # V√©rification\n",
        "    sam_obj_ids = inference_state[\"obj_ids\"]\n",
        "\n",
        "    print(f\"\\nüìä R√âSUM√â ANNOTATIONS:\")\n",
        "    print(f\"   üéØ Annotations configur√©es: {len(all_annotations)}\")\n",
        "    print(f\"   üéØ Objets uniques: {len(added_objects)}\")\n",
        "    print(f\"   ‚úÖ Objets ajout√©s √† SAM2: {len(sam_obj_ids)}\")\n",
        "    print(f\"   üÜî IDs: {sorted(sam_obj_ids)}\")\n",
        "    print(f\"   üìç Frames utilis√©es: {sorted(set(annotation_frames))}\")\n",
        "\n",
        "    # R√©sum√© par type\n",
        "    type_counts = {}\n",
        "    for obj in added_objects:\n",
        "        obj_type = obj['obj_type']\n",
        "        type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "    print(f\"   üè∑Ô∏è  Types: {dict(type_counts)}\")\n",
        "\n",
        "    return added_objects, all_annotations\n",
        "\n",
        "# Ajout des annotations\n",
        "added_objects, initial_annotations_data = add_initial_annotations(predictor, inference_state, config)\n",
        "print(\"\\n‚úÖ Annotations initiales ajout√©es avec succ√®s!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH-AiTYdI57C"
      },
      "outputs": [],
      "source": [
        "# config['calibration']['camera_parameters']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Cpc-6vAdf3",
        "outputId": "72835c64-c66e-4255-81fc-b274a9d1d35b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üîÑ PROPAGATION ET G√âN√âRATION DES ANNOTATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_frame_mapping(total_frames: int, frame_interval: int) -> List[Optional[int]]:\n",
        "    \"\"\"\n",
        "    G√©n√®re le mapping entre frames originales et frames trait√©es.\n",
        "\n",
        "    Args:\n",
        "        total_frames: Nombre total de frames dans la vid√©o originale\n",
        "        frame_interval: Intervalle entre frames (ex: 10 = 1 frame sur 10)\n",
        "\n",
        "    Returns:\n",
        "        Liste o√π l'index = frame originale, valeur = frame trait√©e (ou None si pas trait√©e)\n",
        "    \"\"\"\n",
        "    frame_mapping = [None] * total_frames\n",
        "    processed_idx = 0\n",
        "\n",
        "    for original_idx in range(total_frames):\n",
        "        if original_idx % frame_interval == 0:\n",
        "            frame_mapping[original_idx] = processed_idx\n",
        "            processed_idx += 1\n",
        "\n",
        "    return frame_mapping\n",
        "\n",
        "def create_project_structure(config: Dict[str, Any], video_info: Dict[str, Any], added_objects: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Cr√©e la structure JSON du projet.\"\"\"\n",
        "\n",
        "    # Calcul du frame mapping\n",
        "    frame_mapping = generate_frame_mapping(\n",
        "        video_info['total_frames'],\n",
        "        FRAME_INTERVAL\n",
        "    )\n",
        "    processed_frame_count = sum(x is not None for x in frame_mapping)\n",
        "\n",
        "    # Structure objects avec couleurs\n",
        "    import random\n",
        "    import colorsys\n",
        "\n",
        "    config_objects_mapping = {}\n",
        "    for obj in config['objects']:\n",
        "        config_objects_mapping[obj['obj_id']] = obj\n",
        "\n",
        "    objects = {}\n",
        "    for obj_data in added_objects:\n",
        "        obj_id = str(obj_data['obj_id'])\n",
        "        obj_type = obj_data['obj_type']\n",
        "\n",
        "        # R√©cup√©rer les informations compl√®tes depuis le config\n",
        "        config_obj = config_objects_mapping.get(int(obj_id), {})\n",
        "\n",
        "        # Couleur al√©atoire reproductible\n",
        "        random.seed(int(obj_id) * 12345)\n",
        "        hue = random.random()\n",
        "        rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n",
        "        )\n",
        "\n",
        "        objects[obj_id] = {\n",
        "            \"id\": obj_id,\n",
        "            \"type\": obj_type,\n",
        "            \"team\": config_obj.get('team', None),  # ‚Üê R√©cup√©r√© depuis le config\n",
        "            \"jersey_number\": config_obj.get('jersey_number', None),\n",
        "            \"jersey_color\": config_obj.get('jersey_color', None),\n",
        "            \"role\": config_obj.get('role', None),\n",
        "            \"display_color\": hex_color\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"format_version\": \"1.0\",\n",
        "        \"video\": f\"{VIDEO_NAME}.mp4\",\n",
        "        \"metadata\": {\n",
        "            \"project_id\": str(uuid.uuid4()),\n",
        "            \"created_at\": datetime.now().isoformat() + \"Z\",\n",
        "            \"fps\": video_info['fps'],\n",
        "            \"resolution\": {\n",
        "                \"width\": video_info['width'],\n",
        "                \"height\": video_info['height'],\n",
        "                \"aspect_ratio\": round(video_info['width'] / video_info['height'], 2)\n",
        "            },\n",
        "            \"frame_interval\": FRAME_INTERVAL,\n",
        "            \"frame_count_original\": video_info['total_frames'],\n",
        "            \"frame_count_processed\": processed_frame_count,\n",
        "            \"frame_mapping\": frame_mapping,\n",
        "            \"static_video\": False\n",
        "        },\n",
        "        \"calibration\": config['calibration'],\n",
        "        \"objects\": objects,\n",
        "        \"initial_annotations\": config['initial_annotations'],  # ‚Üê Annotations initiales depuis config\n",
        "        \"annotations\": {}\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# üîß FONCTIONS UTILITAIRES POUR ANNOTATIONS COMPL√àTES\n",
        "# =============================================================================\n",
        "\n",
        "def get_object_scores(predictor, inference_state, frame_idx, obj_id):\n",
        "    \"\"\"R√©cup√®re les scores d'objet de mani√®re propre et s√ªre\"\"\"\n",
        "    try:\n",
        "        obj_idx = predictor._obj_id_to_idx(inference_state, obj_id)\n",
        "        obj_output_dict = inference_state[\"output_dict_per_obj\"][obj_idx]\n",
        "        temp_output_dict = inference_state[\"temp_output_dict_per_obj\"][obj_idx]\n",
        "\n",
        "        # Chercher dans les outputs\n",
        "        frame_output = None\n",
        "        if frame_idx in temp_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in temp_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "\n",
        "        if frame_output and \"object_score_logits\" in frame_output:\n",
        "            object_score_logits = frame_output[\"object_score_logits\"]\n",
        "            return torch.sigmoid(object_score_logits).item()\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def calculate_bbox_from_rle(rle_data: Dict[str, Any]) -> Optional[Dict[str, int]]:\n",
        "    \"\"\"Calcule la bounding box depuis un RLE base64.\"\"\"\n",
        "    from pycocotools.mask import toBbox\n",
        "\n",
        "    try:\n",
        "        rle = {\n",
        "            \"size\": rle_data[\"size\"],\n",
        "            \"counts\": base64.b64decode(rle_data[\"counts\"])\n",
        "        }\n",
        "\n",
        "        bbox = toBbox(rle)\n",
        "\n",
        "        result = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur calcul bbox: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def image_to_world(point_2d, cam_params):\n",
        "    \"\"\"\n",
        "    Projette un point 2D de l'image vers le plan du terrain (Z=0).\n",
        "    \"\"\"\n",
        "    # Create projection matrix P\n",
        "    K = np.array([\n",
        "        [cam_params[\"cam_params\"][\"x_focal_length\"], 0, cam_params[\"cam_params\"][\"principal_point\"][0]],\n",
        "        [0, cam_params[\"cam_params\"][\"y_focal_length\"], cam_params[\"cam_params\"][\"principal_point\"][1]],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    R = np.array(cam_params[\"cam_params\"][\"rotation_matrix\"])\n",
        "    t = -R @ np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    P = K @ np.hstack((R, t.reshape(-1,1)))\n",
        "\n",
        "    # Create point on image plane in homogeneous coordinates\n",
        "    point_2d_h = np.array([point_2d[0], point_2d[1], 1])\n",
        "\n",
        "    # Back-project ray from camera\n",
        "    ray = np.linalg.inv(K) @ point_2d_h\n",
        "    ray = R.T @ ray\n",
        "\n",
        "    # Find intersection with Z=0 plane\n",
        "    camera_pos = np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    t = -camera_pos[2] / ray[2]\n",
        "    world_point = camera_pos + t * ray\n",
        "\n",
        "    return world_point[:2]  # Return only X,Y coordinates since Z=0\n",
        "\n",
        "def calculate_points_output(bbox_output: dict, cam_params: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calcule les points de sortie √† partir de la bbox output.\n",
        "\n",
        "    Args:\n",
        "        bbox_output: Dict avec 'x', 'y', 'width', 'height'\n",
        "        cam_params: Param√®tres de calibration cam√©ra pour projection terrain\n",
        "\n",
        "    Returns:\n",
        "        Dict avec les points calcul√©s s√©par√©s par plan (image vs field)\n",
        "    \"\"\"\n",
        "    if not bbox_output:\n",
        "        return None\n",
        "\n",
        "    # Calculer le point CENTER_BOTTOM dans le plan image\n",
        "    center_bottom_x = bbox_output['x'] + bbox_output['width'] / 2\n",
        "    center_bottom_y = bbox_output['y'] + bbox_output['height']  # Bas de la bbox\n",
        "\n",
        "    # Structure avec s√©paration image/field\n",
        "    points_output = {\n",
        "        \"image\": {\n",
        "            \"CENTER_BOTTOM\": {\n",
        "                \"x\": float(center_bottom_x),\n",
        "                \"y\": float(center_bottom_y)\n",
        "            }\n",
        "        },\n",
        "        \"field\": {\n",
        "            \"CENTER_BOTTOM\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Projection vers le terrain si les param√®tres cam√©ra sont fournis\n",
        "    if cam_params:\n",
        "        try:\n",
        "            # Projeter le point CENTER_BOTTOM vers le terrain\n",
        "            image_point = [center_bottom_x, center_bottom_y]\n",
        "            field_point = image_to_world(image_point, cam_params)\n",
        "\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = {\n",
        "                \"x\": float(field_point[0]),\n",
        "                \"y\": float(field_point[1])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur projection terrain: {e}\")\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = None\n",
        "\n",
        "    return points_output\n",
        "\n",
        "def create_mask_annotation(obj_id: int, mask_logits, predictor=None, inference_state=None,\n",
        "                         frame_idx=None, cam_params: Dict = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Cr√©e une annotation de masque compl√®te avec points input/output, bbox et scores.\n",
        "    \"\"\"\n",
        "    # Conversion en masque binaire\n",
        "    mask = (mask_logits > 0.0).cpu().numpy()\n",
        "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "\n",
        "    # Encodage RLE\n",
        "    if not mask.flags['F_CONTIGUOUS']:\n",
        "        mask = np.asfortranarray(mask)\n",
        "\n",
        "    rle = encode_rle(mask.astype(np.uint8))\n",
        "    base64_counts = base64.b64encode(rle[\"counts\"]).decode('ascii')\n",
        "\n",
        "    # Calcul bbox et points output si masque non vide\n",
        "    bbox_output = None\n",
        "    points_output = None\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        from pycocotools.mask import toBbox\n",
        "        bbox = toBbox(rle)\n",
        "        bbox_output = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        # Calcul des points output depuis la bbox\n",
        "        points_output = calculate_points_output(bbox_output, cam_params)\n",
        "    # else:\n",
        "    #     print(f\"‚ö†Ô∏è Masque vide pour l'objet {obj_id}, bbox et points output = None\")\n",
        "\n",
        "    # R√©cup√©ration du score du masque\n",
        "    mask_score = None\n",
        "\n",
        "    if predictor and inference_state and frame_idx is not None:\n",
        "        # Score du masque\n",
        "        mask_score = get_object_scores(predictor, inference_state, frame_idx, obj_id)\n",
        "\n",
        "    # Structure d'annotation compl√®te\n",
        "    return {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"objectId\": str(obj_id),\n",
        "        \"type\": \"mask\",\n",
        "        \"mask\": {\n",
        "            \"format\": \"rle_coco_base64\",\n",
        "            \"size\": [int(rle[\"size\"][0]), int(rle[\"size\"][1])],\n",
        "            \"counts\": base64_counts\n",
        "        },\n",
        "        \"bbox\": {\n",
        "            \"output\": bbox_output\n",
        "        },\n",
        "        \"points\": {\n",
        "            \"output\": points_output\n",
        "        },\n",
        "        \"maskScore\": mask_score,\n",
        "        \"pose\": None,\n",
        "        \"warning\": False\n",
        "    }\n",
        "\n",
        "# Informations vid√©o\n",
        "cap = cv2.VideoCapture(str(video_path))\n",
        "video_info = {\n",
        "    'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "    'fps': cap.get(cv2.CAP_PROP_FPS),\n",
        "    'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "    'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "}\n",
        "cap.release()\n",
        "\n",
        "# Cr√©ation du projet\n",
        "project = create_project_structure(config, video_info, added_objects)\n",
        "\n",
        "print(f\"üîÑ D√©marrage de la propagation...\")\n",
        "print(f\"   üé¨ {extracted_frames_count} frames √† traiter\")\n",
        "print(f\"   üéØ {len(added_objects)} objets √† suivre\")\n",
        "\n",
        "# Propagation et annotation\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "\n",
        "    if str(out_frame_idx) not in project['annotations']:\n",
        "        project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "    for i, out_obj_id in enumerate(out_obj_ids):\n",
        "        annotation = create_mask_annotation(\n",
        "            obj_id=out_obj_id,\n",
        "            mask_logits=out_mask_logits[i],\n",
        "            predictor=predictor,\n",
        "            inference_state=inference_state,\n",
        "            frame_idx=out_frame_idx,\n",
        "            cam_params=config['calibration']['camera_parameters']\n",
        "        )\n",
        "        project['annotations'][str(out_frame_idx)].append(annotation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXehyd8mHIu"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHlhY4X5Adf3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üíæ SAUVEGARDE DES R√âSULTATS\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"üíæ Sauvegarde des r√©sultats...\")\n",
        "\n",
        "# Sauvegarde du JSON\n",
        "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(project, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Fichier JSON sauv√©: {output_json_path}\")\n",
        "\n",
        "# Statistiques finales\n",
        "total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "unique_frames = len(project['annotations'])\n",
        "\n",
        "\n",
        "print(f\"\\nüìä R√âSULTATS FINAUX:\")\n",
        "print(f\"   üé¨ Frames originales: {project['metadata']['frame_count_original']}\")\n",
        "print(f\"   üé¨ Frames trait√©es: {project['metadata']['frame_count_processed']}\")\n",
        "print(f\"   üìç Frames avec annotations: {unique_frames}\")\n",
        "print(f\"   üìç Annotations totales (propag√©es): {total_annotations}\")\n",
        "print(f\"   üéØ Objets suivis: {len(project['objects'])}\")\n",
        "print(f\"   ‚èØÔ∏è  Intervalle: {project['metadata']['frame_interval']}\")\n",
        "print(f\"   üìÑ Fichier de sortie: {output_json_path}\")\n",
        "print(f\"   üìÅ Dossier de sortie: {output_dir}\")\n",
        "\n",
        "# Affichage d'un √©chantillon du mapping\n",
        "sample_mapping = [(i, v) for i, v in enumerate(project['metadata']['frame_mapping'][:50]) if v is not None]\n",
        "print(f\"   üóÇÔ∏è  Mapping √©chantillon (original‚Üítrait√©): {sample_mapping[:5]}...\")\n",
        "\n",
        "print(f\"\\nüéâ Pipeline SAM2 termin√©e avec succ√®s!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "B_rW1X1nAdf4",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéâ Pipeline Termin√©e !\n",
        "\n",
        "### üìã R√©sultats produits :\n",
        "- **Frames extraites** : `videos/outputs/{VIDEO_NAME}/frames/`\n",
        "- **Projet JSON** : `videos/outputs/{VIDEO_NAME}/{VIDEO_NAME}_project.json`\n",
        "- **Masques** : Int√©gr√©s dans le JSON au format RLE base64\n",
        "\n",
        "### üìÑ Structure du JSON de sortie :\n",
        "```json\n",
        "{\n",
        "  \"metadata\": { /* informations projet */ },\n",
        "  \"calibration\": { /* param√®tres cam√©ra depuis config */ },\n",
        "  \"objects\": { /* objets avec couleurs g√©n√©r√©es */ },\n",
        "  \"initial_annotations\": { /* annotations d'entr√©e enrichies */ },\n",
        "  \"annotations\": { /* r√©sultats de segmentation SAM2 */ }\n",
        "}\n",
        "```\n",
        "\n",
        "La section **`initial_annotations`** contient maintenant :\n",
        "- üìã **Donn√©es originales** : annotations du fichier config\n",
        "- üìä **Statistiques enrichies** : nombre d'objets, points, frames\n",
        "- üìç **Informations de tra√ßabilit√©** : source, date de chargement\n",
        "- üìà **Statistiques par frame** : d√©tails par frame d'annotation\n",
        "\n",
        "### üîß Configuration utilis√©e :\n",
        "- **Vid√©o source** : `videos/{VIDEO_NAME}.mp4`\n",
        "- **Config source** : `videos/{VIDEO_NAME}_config.json`\n",
        "- **Calibration** : Charg√©e depuis le config JSON\n",
        "- **Annotations initiales** : Charg√©es depuis le config JSON (frames automatiquement d√©tect√©es)\n",
        "\n",
        "### üìà Performance :\n",
        "- **Device** : GPU/CPU automatiquement d√©tect√©\n",
        "- **Optimisations** : CUDA TF32 si disponible\n",
        "- **M√©moire** : Gestion intelligente GPU/CPU\n",
        "\n",
        "Le pipeline est maintenant **pr√™t √† √™tre utilis√© avec n'importe quelle vid√©o** ayant son fichier de configuration correspondant !\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Pour utiliser avec une autre vid√©o :\n",
        "\n",
        "1. **Placez votre vid√©o** : `videos/ma_video.mp4`\n",
        "2. **Cr√©ez le config** : `videos/ma_video_config.json` (m√™me format que `cam3_long_config.json`)\n",
        "3. **Modifiez la configuration** dans la cellule 2 :\n",
        "   ```python\n",
        "   VIDEO_NAME = \"ma_video\"          # Changez juste ce nom !\n",
        "   EXTRACT_FRAMES = True            # True=Extraire, False=Skip\n",
        "   FORCE_EXTRACTION = False         # True=Forcer, False=Skip si existent\n",
        "   ```\n",
        "4. **Ex√©cutez toutes les cellules** - c'est tout !\n",
        "\n",
        "### üé¨ Options d'extraction des frames :\n",
        "\n",
        "- **`EXTRACT_FRAMES = True`** : Active l'extraction des frames\n",
        "- **`EXTRACT_FRAMES = False`** : Skip l'extraction, utilise les frames existantes\n",
        "- **`FORCE_EXTRACTION = True`** : Force la r√©-extraction m√™me si frames existent\n",
        "- **`FORCE_EXTRACTION = False`** : Skip l'extraction si frames d√©j√† pr√©sentes\n",
        "\n",
        "#### üí° Cas d'usage typiques :\n",
        "- **Premi√®re fois** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=False`\n",
        "- **Frames d√©j√† extraites** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "- **Re-extraire avec nouvel intervalle** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=True`\n",
        "- **Utiliser frames existantes** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "\n",
        "### üìÑ Format du fichier config JSON :\n",
        "```json\n",
        "{\n",
        "  \"calibration\": { /* param√®tres de calibration cam√©ra */ },\n",
        "  \"objects\": [\n",
        "    {\"obj_id\": 1, \"obj_type\": \"player\"},\n",
        "    {\"obj_id\": 2, \"obj_type\": \"ball\"}\n",
        "  ],\n",
        "  \"initial_annotations\": [\n",
        "    {\n",
        "      \"frame\": 0,  // ‚Üê Frame automatiquement d√©tect√©e\n",
        "      \"annotations\": [\n",
        "        {\n",
        "          \"obj_id\": 1,\n",
        "          \"points\": [\n",
        "            {\"x\": 320, \"y\": 240, \"label\": 1},   // Foreground point\n",
        "            {\"x\": 400, \"y\": 300, \"label\": 0}    // Background point (optionnel)\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### ‚ú® **Nouvelles fonctionnalit√©s automatiques :**\n",
        "- üìç **Frames d√©tect√©es automatiquement** depuis le JSON (pas besoin de variable ANNOTATION_FRAME)\n",
        "- üéØ **Support multi-frames** : le notebook peut traiter plusieurs frames d'annotations initiales\n",
        "- üîÑ **Points multiples par objet** : foreground (label=1) et background (label=0)\n",
        "- üé® **√âvitement des doublons** : chaque objet n'est compt√© qu'une fois m√™me s'il a des annotations sur plusieurs frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i02azRkArbtw"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3qZlb6HaCy"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "  from google.colab import drive\n",
        "  import shutil\n",
        "\n",
        "  # Monter le Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Cr√©er directement le ZIP dans le Drive\n",
        "  shutil.make_archive(f'/content/drive/MyDrive/{VIDEO_NAME}', 'zip', '/content/videos')\n",
        "\n",
        "  print(\"Dossier vid√©o sauvegard√© en ZIP dans votre Google Drive !\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCDAZbPOMYv_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
