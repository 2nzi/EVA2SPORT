{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BTdC1gyYAdf0",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🎯 SAM2 Video Segmentation Pipeline\n",
        "\n",
        "Pipeline complète pour la segmentation vidéo avec SAM2 utilisant un fichier de configuration JSON.\n",
        "\n",
        "## 📋 Prérequis\n",
        "- Vidéo source : `videos/nom_video.mp4`\n",
        "- Fichier config : `videos/nom_video_config.json`\n",
        "- Format de config : calibration caméra + annotations initiales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A64nesoErZY",
        "outputId": "8ee71eb3-2531-4592-9a0e-2084ed53f57f"
      },
      "outputs": [],
      "source": [
        "using_colab = False\n",
        "\n",
        "def is_sam2_installed():\n",
        "    \"\"\"Vérifie si SAM 2 est déjà installé\"\"\"\n",
        "    try:\n",
        "        import sam2\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def is_checkpoint_available():\n",
        "    \"\"\"Vérifie si le checkpoint SAM 2 est disponible\"\"\"\n",
        "    import os\n",
        "    checkpoint_path = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "    return os.path.exists(checkpoint_path)\n",
        "\n",
        "# Condition combinée : Colab ET SAM 2 pas encore installé\n",
        "if using_colab and (not is_sam2_installed() or not is_checkpoint_available()):\n",
        "    print(\"🔧 Installation de SAM 2 en cours...\")\n",
        "    \n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git' #05/06/2025 main branch\n",
        "\n",
        "    !mkdir -p ../checkpoints/\n",
        "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "    # !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
        "\n",
        "    !rm -rf /content/sample_data/* # should remove all folders in your \"./content\" folder\n",
        "    import gc\n",
        "    import torch\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"✅ Installation de SAM 2 terminée\")\n",
        "    \n",
        "elif using_colab and is_sam2_installed() and is_checkpoint_available():\n",
        "    print(\"✅ SAM 2 déjà installé et checkpoint disponible - SKIP installation\")\n",
        "    \n",
        "elif not using_colab:\n",
        "    print(\"🖥️ Mode local - Installation SAM 2 skippée\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k83nNbwoAdf1",
        "outputId": "4f54ec69-dede-40eb-ff63-cac024309b91"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 📦 IMPORTS ET CONFIGURATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from pycocotools.mask import encode as encode_rle\n",
        "\n",
        "# Configuration device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"🖥️ Device utilisé: {device}\")\n",
        "\n",
        "# Optimisations CUDA\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"✅ Optimisations CUDA activées\")\n",
        "\n",
        "print(\"✅ Environnement configuré\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqk5rZs4Adf2",
        "outputId": "e69c45b1-bc44-4f79-9ad9-4707170d12b1"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ⚙️ CONFIGURATION DU PROJET\n",
        "# =============================================================================\n",
        "\n",
        "# 📋 CONFIGURATION PRINCIPALE - MODIFIEZ CES VALEURS\n",
        "VIDEO_NAME = \"SD_13_06_2025_1_PdB_S1_T959s\"  # ⚠️ Nom de la vidéo (sans extension)\n",
        "if using_colab:\n",
        "    VIDEOS_DIR = \"./videos\"   # 📁 Dossier contenant les vidéo\n",
        "else:\n",
        "    VIDEOS_DIR = \"../data/videos\"   # 📁 Dossier contenant les vidéo\n",
        "\n",
        "FRAME_INTERVAL = 3       # 🎬 Intervalle entre frames (1=toutes, 10=1 sur 10)\n",
        "\n",
        "# 🎬 OPTIONS D'EXTRACTION DES FRAMES\n",
        "EXTRACT_FRAMES = True     # ✅ True=Extraire, False=Skip extraction\n",
        "FORCE_EXTRACTION = False  # 🔄 True=Forcer même si frames existent, False=Skip si existent\n",
        "\n",
        "# 🗂️ Construction des chemins automatiques\n",
        "video_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}.mp4\"\n",
        "config_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}_config.json\"\n",
        "output_dir = Path(VIDEOS_DIR) / \"outputs\" / VIDEO_NAME\n",
        "frames_dir = output_dir / \"frames\"\n",
        "masks_dir = output_dir / \"masks\"\n",
        "output_video_path = output_dir / f\"{VIDEO_NAME}_annotated.mp4\"\n",
        "output_json_path = output_dir / f\"{VIDEO_NAME}_project.json\"\n",
        "\n",
        "# 🏗️ Création des dossiers\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "frames_dir.mkdir(exist_ok=True)\n",
        "masks_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# 🔍 Vérification des fichiers\n",
        "print(f\"📋 CONFIGURATION DU PROJET:\")\n",
        "print(f\"   🎬 Vidéo: {video_path}\")\n",
        "print(f\"   📄 Config: {config_path}\")\n",
        "print(f\"   📁 Sortie: {output_dir}\")\n",
        "print(f\"   ⏯️  Intervalle frames: {FRAME_INTERVAL}\")\n",
        "print(f\"   🎬 Extraction: {'✅ Activée' if EXTRACT_FRAMES else '❌ Désactivée'}\")\n",
        "print(f\"   🔄 Force extraction: {'✅ Oui' if FORCE_EXTRACTION else '❌ Non'}\")\n",
        "\n",
        "# Vérifications\n",
        "if not video_path.exists():\n",
        "    raise FileNotFoundError(f\"❌ Vidéo non trouvée: {video_path}\")\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(f\"❌ Fichier config non trouvé: {config_path}\")\n",
        "\n",
        "print(\"✅ Configuration validée\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klEyFoK9Adf2",
        "outputId": "e7abac42-8710-4f68-8988-51659bd44c33"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 📄 CHARGEMENT DE LA CONFIGURATION JSON\n",
        "# =============================================================================\n",
        "\n",
        "def load_config_file(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Charge et valide le fichier de configuration JSON.\"\"\"\n",
        "    print(f\"📄 Chargement de la configuration: {config_path}\")\n",
        "\n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Validation de la structure\n",
        "    required_sections = ['calibration', 'objects', 'initial_annotations']\n",
        "    for section in required_sections:\n",
        "        if section not in config:\n",
        "            raise ValueError(f\"❌ Section '{section}' manquante dans le config\")\n",
        "\n",
        "    # Statistiques\n",
        "    num_objects = len(config['objects'])\n",
        "    num_annotations = 0\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        num_annotations += len(frame_data['annotations'])\n",
        "\n",
        "    print(f\"✅ Configuration chargée:\")\n",
        "    print(f\"   📷 Calibration caméra: OK\")\n",
        "    print(f\"   🎯 Objets définis: {num_objects}\")\n",
        "    print(f\"   📍 Annotations initiales: {num_annotations}\")\n",
        "\n",
        "    # Résumé des types d'objets\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_type = obj['obj_type']\n",
        "        obj_types[obj_type] = obj_types.get(obj_type, 0) + 1\n",
        "    print(f\"   🏷️  Types: {dict(obj_types)}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# Chargement de la configuration\n",
        "config = load_config_file(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzoA46BbAdf2",
        "outputId": "01f21875-44ba-4a30-8978-23025db22c4a"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 🎬 EXTRACTION DES FRAMES\n",
        "# =============================================================================\n",
        "\n",
        "def extract_frames(video_path: Path, frames_dir: Path, frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"Extrait les frames de la vidéo selon l'intervalle spécifié.\"\"\"\n",
        "\n",
        "    print(f\"🎬 Extraction des frames...\")\n",
        "    print(f\"   📹 Source: {video_path}\")\n",
        "    print(f\"   📁 Destination: {frames_dir}\")\n",
        "    print(f\"   ⏯️  Intervalle: {frame_interval}\")\n",
        "    print(f\"   🔄 Force extraction: {'✅ Oui' if force_extraction else '❌ Non'}\")\n",
        "\n",
        "    # Vérification si extraction déjà faite\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    if existing_frames and not force_extraction:\n",
        "        print(f\"📂 {len(existing_frames)} frames déjà extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"🔄 {len(existing_frames)} frames existantes - SUPPRESSION et ré-extraction...\")\n",
        "        # Supprimer les frames existantes\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"🗑️  Frames existantes supprimées\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"❌ Impossible d'ouvrir la vidéo: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"📊 Vidéo: {total_frames} frames, {fps:.1f} FPS\")\n",
        "\n",
        "    extracted_count = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extraire seulement selon l'intervalle\n",
        "            if frame_idx % frame_interval == 0:\n",
        "                output_idx = frame_idx // frame_interval\n",
        "                filename = frames_dir / f\"{output_idx:05d}.jpg\"\n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "\n",
        "                if extracted_count % 50 == 0:\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    print(f\"📊 Progrès: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"✅ {extracted_count} frames extraites\")\n",
        "    return extracted_count\n",
        "\n",
        "# Extraction des frames\n",
        "if EXTRACT_FRAMES:\n",
        "    extracted_frames_count = extract_frames(video_path, frames_dir, FRAME_INTERVAL, FORCE_EXTRACTION)\n",
        "else:\n",
        "    # Skip extraction - compter les frames existantes\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    extracted_frames_count = len(existing_frames)\n",
        "\n",
        "    print(f\"⏭️  Extraction désactivée\")\n",
        "    if extracted_frames_count > 0:\n",
        "        print(f\"📂 Utilisation de {extracted_frames_count} frames existantes\")\n",
        "    else:\n",
        "        print(f\"⚠️  Aucune frame trouvée dans {frames_dir}\")\n",
        "        print(f\"💡 Conseil: Activez EXTRACT_FRAMES=True pour extraire les frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cziqr2A3Adf2",
        "outputId": "13efa7fc-40f4-4e0d-9c66-03c88ca7b516"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 🤖 INITIALISATION SAM2\n",
        "# =============================================================================\n",
        "\n",
        "# Import SAM2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Configuration des chemins SAM2\n",
        "if using_colab:\n",
        "    checkpoint_path = \"../../checkpoints/sam2.1_hiera_large.pt\"\n",
        "else:\n",
        "    checkpoint_path = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "\n",
        "model_config_path = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "print(f\"🤖 Initialisation SAM2...\")\n",
        "print(f\"   🧠 Modèle: {model_config_path}\")\n",
        "print(f\"   💾 Checkpoint: {checkpoint_path}\")\n",
        "print(f\"   🖥️  Device: {device}\")\n",
        "\n",
        "# Construction du predictor\n",
        "predictor = build_sam2_video_predictor(\n",
        "    config_file=model_config_path,\n",
        "    ckpt_path=checkpoint_path,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialisation de l'état d'inférence\n",
        "print(f\"\\n🎬 Initialisation état d'inférence...\")\n",
        "print(f\"   📁 Frames: {frames_dir}\")\n",
        "\n",
        "inference_state = predictor.init_state(\n",
        "    video_path=str(frames_dir),\n",
        "    offload_video_to_cpu=True,    # Économise la mémoire GPU\n",
        "    offload_state_to_cpu=False    # Garde l'état en GPU\n",
        ")\n",
        "\n",
        "# Reset de l'état\n",
        "predictor.reset_state(inference_state)\n",
        "\n",
        "# Vérification\n",
        "loaded_frames = inference_state[\"num_frames\"]\n",
        "print(f\"\\n✅ SAM2 initialisé:\")\n",
        "print(f\"   🖼️  Frames extraites: {extracted_frames_count}\")\n",
        "print(f\"   🎬 Frames chargées: {loaded_frames}\")\n",
        "print(f\"   ✅ Correspondance: {'OK' if extracted_frames_count == loaded_frames else 'ERREUR'}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    print(f\"   💾 GPU Memory: {allocated:.2f}GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twDWK7TrAdf3",
        "outputId": "08b192e9-cbd5-45f7-8042-6bd65b4da128"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 🎯 AJOUT DES ANNOTATIONS INITIALES\n",
        "# =============================================================================\n",
        "\n",
        "def add_initial_annotations(predictor, inference_state, config: Dict[str, Any]):\n",
        "    \"\"\"Ajoute les annotations initiales depuis la configuration.\"\"\"\n",
        "\n",
        "    print(f\"🎯 Ajout des annotations initiales...\")\n",
        "\n",
        "    # Création du mapping obj_id -> obj_type\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_types[obj['obj_id']] = obj['obj_type']\n",
        "\n",
        "    # Extraction automatique des annotations et frames depuis le JSON\n",
        "    all_annotations = []\n",
        "    annotation_frames = []\n",
        "\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        frame_idx = frame_data['frame']\n",
        "        annotations = frame_data['annotations']\n",
        "        annotation_frames.append(frame_idx)\n",
        "\n",
        "        print(f\"   📍 Frame {frame_idx}: {len(annotations)} annotations\")\n",
        "\n",
        "        for annotation in annotations:\n",
        "            all_annotations.append({\n",
        "                'frame': frame_idx,\n",
        "                'obj_id': annotation['obj_id'],\n",
        "                'points': annotation['points'],\n",
        "                'obj_type': obj_types.get(annotation['obj_id'], f'unknown_{annotation[\"obj_id\"]}')\n",
        "            })\n",
        "\n",
        "    if not all_annotations:\n",
        "        raise ValueError(f\"❌ Aucune annotation trouvée dans le fichier config\")\n",
        "\n",
        "    print(f\"   📊 Total: {len(all_annotations)} annotations sur {len(set(annotation_frames))} frames\")\n",
        "\n",
        "    # Ajout des annotations à SAM2\n",
        "    added_objects = []\n",
        "\n",
        "    for annotation_data in all_annotations:\n",
        "        frame_idx = annotation_data['frame']\n",
        "        obj_id = annotation_data['obj_id']\n",
        "        obj_type = annotation_data['obj_type']\n",
        "        points_data = annotation_data['points']\n",
        "\n",
        "        # Extraction des coordonnées et labels\n",
        "        points = np.array([[p['x'], p['y']] for p in points_data], dtype=np.float32)\n",
        "        labels = np.array([p['label'] for p in points_data], dtype=np.int32)\n",
        "\n",
        "        print(f\"   🎯 Frame {frame_idx} - Objet {obj_id} ({obj_type}): {len(points)} points à ({points[0][0]:.0f}, {points[0][1]:.0f})\")\n",
        "\n",
        "        # Ajout à SAM2 avec add_new_points_or_box\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state,\n",
        "            frame_idx,\n",
        "            obj_id,\n",
        "            points,\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # Éviter les doublons dans added_objects\n",
        "        if not any(obj['obj_id'] == obj_id for obj in added_objects):\n",
        "            added_objects.append({\n",
        "                'obj_id': obj_id,\n",
        "                'obj_type': obj_type,\n",
        "                'points_count': len(points)\n",
        "            })\n",
        "\n",
        "    # Vérification\n",
        "    sam_obj_ids = inference_state[\"obj_ids\"]\n",
        "\n",
        "    print(f\"\\n📊 RÉSUMÉ ANNOTATIONS:\")\n",
        "    print(f\"   🎯 Annotations configurées: {len(all_annotations)}\")\n",
        "    print(f\"   🎯 Objets uniques: {len(added_objects)}\")\n",
        "    print(f\"   ✅ Objets ajoutés à SAM2: {len(sam_obj_ids)}\")\n",
        "    print(f\"   🆔 IDs: {sorted(sam_obj_ids)}\")\n",
        "    print(f\"   📍 Frames utilisées: {sorted(set(annotation_frames))}\")\n",
        "\n",
        "    # Résumé par type\n",
        "    type_counts = {}\n",
        "    for obj in added_objects:\n",
        "        obj_type = obj['obj_type']\n",
        "        type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "    print(f\"   🏷️  Types: {dict(type_counts)}\")\n",
        "\n",
        "    return added_objects, all_annotations\n",
        "\n",
        "# Ajout des annotations\n",
        "added_objects, initial_annotations_data = add_initial_annotations(predictor, inference_state, config)\n",
        "print(\"\\n✅ Annotations initiales ajoutées avec succès!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH-AiTYdI57C"
      },
      "outputs": [],
      "source": [
        "# config['calibration']['camera_parameters']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Cpc-6vAdf3",
        "outputId": "72835c64-c66e-4255-81fc-b274a9d1d35b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 🔄 PROPAGATION ET GÉNÉRATION DES ANNOTATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_frame_mapping(total_frames: int, frame_interval: int) -> List[Optional[int]]:\n",
        "    \"\"\"\n",
        "    Génère le mapping entre frames originales et frames traitées.\n",
        "\n",
        "    Args:\n",
        "        total_frames: Nombre total de frames dans la vidéo originale\n",
        "        frame_interval: Intervalle entre frames (ex: 10 = 1 frame sur 10)\n",
        "\n",
        "    Returns:\n",
        "        Liste où l'index = frame originale, valeur = frame traitée (ou None si pas traitée)\n",
        "    \"\"\"\n",
        "    frame_mapping = [None] * total_frames\n",
        "    processed_idx = 0\n",
        "\n",
        "    for original_idx in range(total_frames):\n",
        "        if original_idx % frame_interval == 0:\n",
        "            frame_mapping[original_idx] = processed_idx\n",
        "            processed_idx += 1\n",
        "\n",
        "    return frame_mapping\n",
        "\n",
        "def create_project_structure(config: Dict[str, Any], video_info: Dict[str, Any], added_objects: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Crée la structure JSON du projet.\"\"\"\n",
        "\n",
        "    # Calcul du frame mapping\n",
        "    frame_mapping = generate_frame_mapping(\n",
        "        video_info['total_frames'],\n",
        "        FRAME_INTERVAL\n",
        "    )\n",
        "    processed_frame_count = sum(x is not None for x in frame_mapping)\n",
        "\n",
        "    # Structure objects avec couleurs\n",
        "    import random\n",
        "    import colorsys\n",
        "\n",
        "    config_objects_mapping = {}\n",
        "    for obj in config['objects']:\n",
        "        config_objects_mapping[obj['obj_id']] = obj\n",
        "\n",
        "    objects = {}\n",
        "    for obj_data in added_objects:\n",
        "        obj_id = str(obj_data['obj_id'])\n",
        "        obj_type = obj_data['obj_type']\n",
        "\n",
        "        # Récupérer les informations complètes depuis le config\n",
        "        config_obj = config_objects_mapping.get(int(obj_id), {})\n",
        "\n",
        "        # Couleur aléatoire reproductible\n",
        "        random.seed(int(obj_id) * 12345)\n",
        "        hue = random.random()\n",
        "        rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n",
        "        )\n",
        "\n",
        "        objects[obj_id] = {\n",
        "            \"id\": obj_id,\n",
        "            \"type\": obj_type,\n",
        "            \"team\": config_obj.get('team', None),  # ← Récupéré depuis le config\n",
        "            \"jersey_number\": config_obj.get('jersey_number', None),\n",
        "            \"jersey_color\": config_obj.get('jersey_color', None),\n",
        "            \"role\": config_obj.get('role', None),\n",
        "            \"display_color\": hex_color\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"format_version\": \"1.0\",\n",
        "        \"video\": f\"{VIDEO_NAME}.mp4\",\n",
        "        \"metadata\": {\n",
        "            \"project_id\": str(uuid.uuid4()),\n",
        "            \"created_at\": datetime.now().isoformat() + \"Z\",\n",
        "            \"fps\": video_info['fps'],\n",
        "            \"resolution\": {\n",
        "                \"width\": video_info['width'],\n",
        "                \"height\": video_info['height'],\n",
        "                \"aspect_ratio\": round(video_info['width'] / video_info['height'], 2)\n",
        "            },\n",
        "            \"frame_interval\": FRAME_INTERVAL,\n",
        "            \"frame_count_original\": video_info['total_frames'],\n",
        "            \"frame_count_processed\": processed_frame_count,\n",
        "            \"frame_mapping\": frame_mapping,\n",
        "            \"static_video\": False\n",
        "        },\n",
        "        \"calibration\": config['calibration'],\n",
        "        \"objects\": objects,\n",
        "        \"initial_annotations\": config['initial_annotations'],  # ← Annotations initiales depuis config\n",
        "        \"annotations\": {}\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# 🔧 FONCTIONS UTILITAIRES POUR ANNOTATIONS COMPLÈTES\n",
        "# =============================================================================\n",
        "\n",
        "def get_object_scores(predictor, inference_state, frame_idx, obj_id):\n",
        "    \"\"\"Récupère les scores d'objet de manière propre et sûre\"\"\"\n",
        "    try:\n",
        "        obj_idx = predictor._obj_id_to_idx(inference_state, obj_id)\n",
        "        obj_output_dict = inference_state[\"output_dict_per_obj\"][obj_idx]\n",
        "        temp_output_dict = inference_state[\"temp_output_dict_per_obj\"][obj_idx]\n",
        "\n",
        "        # Chercher dans les outputs\n",
        "        frame_output = None\n",
        "        if frame_idx in temp_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in temp_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "\n",
        "        if frame_output and \"object_score_logits\" in frame_output:\n",
        "            object_score_logits = frame_output[\"object_score_logits\"]\n",
        "            return torch.sigmoid(object_score_logits).item()\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def calculate_bbox_from_rle(rle_data: Dict[str, Any]) -> Optional[Dict[str, int]]:\n",
        "    \"\"\"Calcule la bounding box depuis un RLE base64.\"\"\"\n",
        "    from pycocotools.mask import toBbox\n",
        "\n",
        "    try:\n",
        "        rle = {\n",
        "            \"size\": rle_data[\"size\"],\n",
        "            \"counts\": base64.b64decode(rle_data[\"counts\"])\n",
        "        }\n",
        "\n",
        "        bbox = toBbox(rle)\n",
        "\n",
        "        result = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erreur calcul bbox: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def image_to_world(point_2d, cam_params):\n",
        "    \"\"\"\n",
        "    Projette un point 2D de l'image vers le plan du terrain (Z=0).\n",
        "    \"\"\"\n",
        "    # Create projection matrix P\n",
        "    K = np.array([\n",
        "        [cam_params[\"cam_params\"][\"x_focal_length\"], 0, cam_params[\"cam_params\"][\"principal_point\"][0]],\n",
        "        [0, cam_params[\"cam_params\"][\"y_focal_length\"], cam_params[\"cam_params\"][\"principal_point\"][1]],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    R = np.array(cam_params[\"cam_params\"][\"rotation_matrix\"])\n",
        "    t = -R @ np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    P = K @ np.hstack((R, t.reshape(-1,1)))\n",
        "\n",
        "    # Create point on image plane in homogeneous coordinates\n",
        "    point_2d_h = np.array([point_2d[0], point_2d[1], 1])\n",
        "\n",
        "    # Back-project ray from camera\n",
        "    ray = np.linalg.inv(K) @ point_2d_h\n",
        "    ray = R.T @ ray\n",
        "\n",
        "    # Find intersection with Z=0 plane\n",
        "    camera_pos = np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    t = -camera_pos[2] / ray[2]\n",
        "    world_point = camera_pos + t * ray\n",
        "\n",
        "    return world_point[:2]  # Return only X,Y coordinates since Z=0\n",
        "\n",
        "def calculate_points_output(bbox_output: dict, cam_params: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calcule les points de sortie à partir de la bbox output.\n",
        "\n",
        "    Args:\n",
        "        bbox_output: Dict avec 'x', 'y', 'width', 'height'\n",
        "        cam_params: Paramètres de calibration caméra pour projection terrain\n",
        "\n",
        "    Returns:\n",
        "        Dict avec les points calculés séparés par plan (image vs field)\n",
        "    \"\"\"\n",
        "    if not bbox_output:\n",
        "        return None\n",
        "\n",
        "    # Calculer le point CENTER_BOTTOM dans le plan image\n",
        "    center_bottom_x = bbox_output['x'] + bbox_output['width'] / 2\n",
        "    center_bottom_y = bbox_output['y'] + bbox_output['height']  # Bas de la bbox\n",
        "\n",
        "    # Structure avec séparation image/field\n",
        "    points_output = {\n",
        "        \"image\": {\n",
        "            \"CENTER_BOTTOM\": {\n",
        "                \"x\": float(center_bottom_x),\n",
        "                \"y\": float(center_bottom_y)\n",
        "            }\n",
        "        },\n",
        "        \"field\": {\n",
        "            \"CENTER_BOTTOM\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Projection vers le terrain si les paramètres caméra sont fournis\n",
        "    if cam_params:\n",
        "        try:\n",
        "            # Projeter le point CENTER_BOTTOM vers le terrain\n",
        "            image_point = [center_bottom_x, center_bottom_y]\n",
        "            field_point = image_to_world(image_point, cam_params)\n",
        "\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = {\n",
        "                \"x\": float(field_point[0]),\n",
        "                \"y\": float(field_point[1])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur projection terrain: {e}\")\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = None\n",
        "\n",
        "    return points_output\n",
        "\n",
        "def create_mask_annotation(obj_id: int, mask_logits, predictor=None, inference_state=None,\n",
        "                         frame_idx=None, cam_params: Dict = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Crée une annotation de masque complète avec points input/output, bbox et scores.\n",
        "    \"\"\"\n",
        "    # Conversion en masque binaire\n",
        "    mask = (mask_logits > 0.0).cpu().numpy()\n",
        "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "\n",
        "    # Encodage RLE\n",
        "    if not mask.flags['F_CONTIGUOUS']:\n",
        "        mask = np.asfortranarray(mask)\n",
        "\n",
        "    rle = encode_rle(mask.astype(np.uint8))\n",
        "    base64_counts = base64.b64encode(rle[\"counts\"]).decode('ascii')\n",
        "\n",
        "    # Calcul bbox et points output si masque non vide\n",
        "    bbox_output = None\n",
        "    points_output = None\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        from pycocotools.mask import toBbox\n",
        "        bbox = toBbox(rle)\n",
        "        bbox_output = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        # Calcul des points output depuis la bbox\n",
        "        points_output = calculate_points_output(bbox_output, cam_params)\n",
        "    # else:\n",
        "    #     print(f\"⚠️ Masque vide pour l'objet {obj_id}, bbox et points output = None\")\n",
        "\n",
        "    # Récupération du score du masque\n",
        "    mask_score = None\n",
        "\n",
        "    if predictor and inference_state and frame_idx is not None:\n",
        "        # Score du masque\n",
        "        mask_score = get_object_scores(predictor, inference_state, frame_idx, obj_id)\n",
        "\n",
        "    # Structure d'annotation complète\n",
        "    return {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"objectId\": str(obj_id),\n",
        "        \"type\": \"mask\",\n",
        "        \"mask\": {\n",
        "            \"format\": \"rle_coco_base64\",\n",
        "            \"size\": [int(rle[\"size\"][0]), int(rle[\"size\"][1])],\n",
        "            \"counts\": base64_counts\n",
        "        },\n",
        "        \"bbox\": {\n",
        "            \"output\": bbox_output\n",
        "        },\n",
        "        \"points\": {\n",
        "            \"output\": points_output\n",
        "        },\n",
        "        \"maskScore\": mask_score,\n",
        "        \"pose\": None,\n",
        "        \"warning\": False\n",
        "    }\n",
        "\n",
        "# Informations vidéo\n",
        "cap = cv2.VideoCapture(str(video_path))\n",
        "video_info = {\n",
        "    'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "    'fps': cap.get(cv2.CAP_PROP_FPS),\n",
        "    'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "    'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "}\n",
        "cap.release()\n",
        "\n",
        "# Création du projet\n",
        "project = create_project_structure(config, video_info, added_objects)\n",
        "\n",
        "print(f\"🔄 Démarrage de la propagation...\")\n",
        "print(f\"   🎬 {extracted_frames_count} frames à traiter\")\n",
        "print(f\"   🎯 {len(added_objects)} objets à suivre\")\n",
        "\n",
        "# Propagation et annotation\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "\n",
        "    if str(out_frame_idx) not in project['annotations']:\n",
        "        project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "    for i, out_obj_id in enumerate(out_obj_ids):\n",
        "        annotation = create_mask_annotation(\n",
        "            obj_id=out_obj_id,\n",
        "            mask_logits=out_mask_logits[i],\n",
        "            predictor=predictor,\n",
        "            inference_state=inference_state,\n",
        "            frame_idx=out_frame_idx,\n",
        "            cam_params=config['calibration']['camera_parameters']\n",
        "        )\n",
        "        project['annotations'][str(out_frame_idx)].append(annotation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXehyd8mHIu"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHlhY4X5Adf3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 💾 SAUVEGARDE DES RÉSULTATS\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"💾 Sauvegarde des résultats...\")\n",
        "\n",
        "# Sauvegarde du JSON\n",
        "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(project, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Fichier JSON sauvé: {output_json_path}\")\n",
        "\n",
        "# Statistiques finales\n",
        "total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "unique_frames = len(project['annotations'])\n",
        "\n",
        "\n",
        "print(f\"\\n📊 RÉSULTATS FINAUX:\")\n",
        "print(f\"   🎬 Frames originales: {project['metadata']['frame_count_original']}\")\n",
        "print(f\"   🎬 Frames traitées: {project['metadata']['frame_count_processed']}\")\n",
        "print(f\"   📍 Frames avec annotations: {unique_frames}\")\n",
        "print(f\"   📍 Annotations totales (propagées): {total_annotations}\")\n",
        "print(f\"   🎯 Objets suivis: {len(project['objects'])}\")\n",
        "print(f\"   ⏯️  Intervalle: {project['metadata']['frame_interval']}\")\n",
        "print(f\"   📄 Fichier de sortie: {output_json_path}\")\n",
        "print(f\"   📁 Dossier de sortie: {output_dir}\")\n",
        "\n",
        "# Affichage d'un échantillon du mapping\n",
        "sample_mapping = [(i, v) for i, v in enumerate(project['metadata']['frame_mapping'][:50]) if v is not None]\n",
        "print(f\"   🗂️  Mapping échantillon (original→traité): {sample_mapping[:5]}...\")\n",
        "\n",
        "print(f\"\\n🎉 Pipeline SAM2 terminée avec succès!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "B_rW1X1nAdf4",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎉 Pipeline Terminée !\n",
        "\n",
        "### 📋 Résultats produits :\n",
        "- **Frames extraites** : `videos/outputs/{VIDEO_NAME}/frames/`\n",
        "- **Projet JSON** : `videos/outputs/{VIDEO_NAME}/{VIDEO_NAME}_project.json`\n",
        "- **Masques** : Intégrés dans le JSON au format RLE base64\n",
        "\n",
        "### 📄 Structure du JSON de sortie :\n",
        "```json\n",
        "{\n",
        "  \"metadata\": { /* informations projet */ },\n",
        "  \"calibration\": { /* paramètres caméra depuis config */ },\n",
        "  \"objects\": { /* objets avec couleurs générées */ },\n",
        "  \"initial_annotations\": { /* annotations d'entrée enrichies */ },\n",
        "  \"annotations\": { /* résultats de segmentation SAM2 */ }\n",
        "}\n",
        "```\n",
        "\n",
        "La section **`initial_annotations`** contient maintenant :\n",
        "- 📋 **Données originales** : annotations du fichier config\n",
        "- 📊 **Statistiques enrichies** : nombre d'objets, points, frames\n",
        "- 📍 **Informations de traçabilité** : source, date de chargement\n",
        "- 📈 **Statistiques par frame** : détails par frame d'annotation\n",
        "\n",
        "### 🔧 Configuration utilisée :\n",
        "- **Vidéo source** : `videos/{VIDEO_NAME}.mp4`\n",
        "- **Config source** : `videos/{VIDEO_NAME}_config.json`\n",
        "- **Calibration** : Chargée depuis le config JSON\n",
        "- **Annotations initiales** : Chargées depuis le config JSON (frames automatiquement détectées)\n",
        "\n",
        "### 📈 Performance :\n",
        "- **Device** : GPU/CPU automatiquement détecté\n",
        "- **Optimisations** : CUDA TF32 si disponible\n",
        "- **Mémoire** : Gestion intelligente GPU/CPU\n",
        "\n",
        "Le pipeline est maintenant **prêt à être utilisé avec n'importe quelle vidéo** ayant son fichier de configuration correspondant !\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Pour utiliser avec une autre vidéo :\n",
        "\n",
        "1. **Placez votre vidéo** : `videos/ma_video.mp4`\n",
        "2. **Créez le config** : `videos/ma_video_config.json` (même format que `cam3_long_config.json`)\n",
        "3. **Modifiez la configuration** dans la cellule 2 :\n",
        "   ```python\n",
        "   VIDEO_NAME = \"ma_video\"          # Changez juste ce nom !\n",
        "   EXTRACT_FRAMES = True            # True=Extraire, False=Skip\n",
        "   FORCE_EXTRACTION = False         # True=Forcer, False=Skip si existent\n",
        "   ```\n",
        "4. **Exécutez toutes les cellules** - c'est tout !\n",
        "\n",
        "### 🎬 Options d'extraction des frames :\n",
        "\n",
        "- **`EXTRACT_FRAMES = True`** : Active l'extraction des frames\n",
        "- **`EXTRACT_FRAMES = False`** : Skip l'extraction, utilise les frames existantes\n",
        "- **`FORCE_EXTRACTION = True`** : Force la ré-extraction même si frames existent\n",
        "- **`FORCE_EXTRACTION = False`** : Skip l'extraction si frames déjà présentes\n",
        "\n",
        "#### 💡 Cas d'usage typiques :\n",
        "- **Première fois** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=False`\n",
        "- **Frames déjà extraites** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "- **Re-extraire avec nouvel intervalle** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=True`\n",
        "- **Utiliser frames existantes** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "\n",
        "### 📄 Format du fichier config JSON :\n",
        "```json\n",
        "{\n",
        "  \"calibration\": { /* paramètres de calibration caméra */ },\n",
        "  \"objects\": [\n",
        "    {\"obj_id\": 1, \"obj_type\": \"player\"},\n",
        "    {\"obj_id\": 2, \"obj_type\": \"ball\"}\n",
        "  ],\n",
        "  \"initial_annotations\": [\n",
        "    {\n",
        "      \"frame\": 0,  // ← Frame automatiquement détectée\n",
        "      \"annotations\": [\n",
        "        {\n",
        "          \"obj_id\": 1,\n",
        "          \"points\": [\n",
        "            {\"x\": 320, \"y\": 240, \"label\": 1},   // Foreground point\n",
        "            {\"x\": 400, \"y\": 300, \"label\": 0}    // Background point (optionnel)\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### ✨ **Nouvelles fonctionnalités automatiques :**\n",
        "- 📍 **Frames détectées automatiquement** depuis le JSON (pas besoin de variable ANNOTATION_FRAME)\n",
        "- 🎯 **Support multi-frames** : le notebook peut traiter plusieurs frames d'annotations initiales\n",
        "- 🔄 **Points multiples par objet** : foreground (label=1) et background (label=0)\n",
        "- 🎨 **Évitement des doublons** : chaque objet n'est compté qu'une fois même s'il a des annotations sur plusieurs frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i02azRkArbtw"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3qZlb6HaCy"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "  from google.colab import drive\n",
        "  import shutil\n",
        "\n",
        "  # Monter le Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Créer directement le ZIP dans le Drive\n",
        "  shutil.make_archive(f'/content/drive/MyDrive/{VIDEO_NAME}', 'zip', '/content/videos')\n",
        "\n",
        "  print(\"Dossier vidéo sauvegardé en ZIP dans votre Google Drive !\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCDAZbPOMYv_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
