{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "BTdC1gyYAdf0",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ¯ SAM2 Video Segmentation Pipeline\n",
        "\n",
        "Pipeline complÃ¨te pour la segmentation vidÃ©o avec SAM2 utilisant un fichier de configuration JSON.\n",
        "\n",
        "## ğŸ“‹ PrÃ©requis\n",
        "- VidÃ©o source : `videos/nom_video.mp4`\n",
        "- Fichier config : `videos/nom_video_config.json`\n",
        "- Format de config : calibration camÃ©ra + annotations initiales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A64nesoErZY",
        "outputId": "8ee71eb3-2531-4592-9a0e-2084ed53f57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cpu\n",
            "Torchvision version: 0.20.1+cpu\n",
            "CUDA is available: False\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k83nNbwoAdf1",
        "outputId": "4f54ec69-dede-40eb-ff63-cac024309b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¥ï¸ Device utilisÃ©: cpu\n",
            "âœ… Environnement configurÃ©\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ“¦ IMPORTS ET CONFIGURATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from pycocotools.mask import encode as encode_rle\n",
        "\n",
        "# Configuration device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"ğŸ–¥ï¸ Device utilisÃ©: {device}\")\n",
        "\n",
        "# Optimisations CUDA\n",
        "if device.type == \"cuda\":\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        print(\"âœ… Optimisations CUDA activÃ©es\")\n",
        "\n",
        "print(\"âœ… Environnement configurÃ©\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqk5rZs4Adf2",
        "outputId": "e69c45b1-bc44-4f79-9ad9-4707170d12b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ CONFIGURATION DU PROJET:\n",
            "   ğŸ¬ VidÃ©o: ..\\data\\videos\\SD_13_06_2025_1_PdB_S1_T959s.mp4\n",
            "   ğŸ“„ Config: ..\\data\\videos\\SD_13_06_2025_1_PdB_S1_T959s_config.json\n",
            "   ğŸ“ Sortie: ..\\data\\videos\\outputs\\SD_13_06_2025_1_PdB_S1_T959s\n",
            "   â¯ï¸  Intervalle frames: 3\n",
            "   ğŸ¬ Extraction: âœ… ActivÃ©e\n",
            "   ğŸ”„ Force extraction: âŒ Non\n",
            "âœ… Configuration validÃ©e\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# âš™ï¸ CONFIGURATION DU PROJET\n",
        "# =============================================================================\n",
        "\n",
        "# ğŸ“‹ CONFIGURATION PRINCIPALE - MODIFIEZ CES VALEURS\n",
        "VIDEO_NAME = \"SD_13_06_2025_1_PdB_S1_T959s\"  # âš ï¸ Nom de la vidÃ©o (sans extension)\n",
        "VIDEOS_DIR = \"../data/videos\"   # ğŸ“ Dossier contenant les vidÃ©o\n",
        "FRAME_INTERVAL = 3       # ğŸ¬ Intervalle entre frames (1=toutes, 10=1 sur 10)\n",
        "\n",
        "# ğŸ¬ OPTIONS D'EXTRACTION DES FRAMES\n",
        "EXTRACT_FRAMES = True     # âœ… True=Extraire, False=Skip extraction\n",
        "FORCE_EXTRACTION = False  # ğŸ”„ True=Forcer mÃªme si frames existent, False=Skip si existent\n",
        "\n",
        "# ğŸ—‚ï¸ Construction des chemins automatiques\n",
        "video_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}.mp4\"\n",
        "config_path = Path(VIDEOS_DIR) / f\"{VIDEO_NAME}_config.json\"\n",
        "output_dir = Path(VIDEOS_DIR) / \"outputs\" / VIDEO_NAME\n",
        "frames_dir = output_dir / \"frames\"\n",
        "masks_dir = output_dir / \"masks\"\n",
        "output_video_path = output_dir / f\"{VIDEO_NAME}_annotated.mp4\"\n",
        "output_json_path = output_dir / f\"{VIDEO_NAME}_project.json\"\n",
        "\n",
        "# ğŸ—ï¸ CrÃ©ation des dossiers\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "frames_dir.mkdir(exist_ok=True)\n",
        "masks_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# ğŸ” VÃ©rification des fichiers\n",
        "print(f\"ğŸ“‹ CONFIGURATION DU PROJET:\")\n",
        "print(f\"   ğŸ¬ VidÃ©o: {video_path}\")\n",
        "print(f\"   ğŸ“„ Config: {config_path}\")\n",
        "print(f\"   ğŸ“ Sortie: {output_dir}\")\n",
        "print(f\"   â¯ï¸  Intervalle frames: {FRAME_INTERVAL}\")\n",
        "print(f\"   ğŸ¬ Extraction: {'âœ… ActivÃ©e' if EXTRACT_FRAMES else 'âŒ DÃ©sactivÃ©e'}\")\n",
        "print(f\"   ğŸ”„ Force extraction: {'âœ… Oui' if FORCE_EXTRACTION else 'âŒ Non'}\")\n",
        "\n",
        "# VÃ©rifications\n",
        "if not video_path.exists():\n",
        "    raise FileNotFoundError(f\"âŒ VidÃ©o non trouvÃ©e: {video_path}\")\n",
        "if not config_path.exists():\n",
        "    raise FileNotFoundError(f\"âŒ Fichier config non trouvÃ©: {config_path}\")\n",
        "\n",
        "print(\"âœ… Configuration validÃ©e\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klEyFoK9Adf2",
        "outputId": "e7abac42-8710-4f68-8988-51659bd44c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Chargement de la configuration: ..\\data\\videos\\SD_13_06_2025_1_PdB_S1_T959s_config.json\n",
            "âœ… Configuration chargÃ©e:\n",
            "   ğŸ“· Calibration camÃ©ra: OK\n",
            "   ğŸ¯ Objets dÃ©finis: 11\n",
            "   ğŸ“ Annotations initiales: 11\n",
            "   ğŸ·ï¸  Types: {'ball': 1, 'player': 10}\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ“„ CHARGEMENT DE LA CONFIGURATION JSON\n",
        "# =============================================================================\n",
        "\n",
        "def load_config_file(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Charge et valide le fichier de configuration JSON.\"\"\"\n",
        "    print(f\"ğŸ“„ Chargement de la configuration: {config_path}\")\n",
        "\n",
        "    with open(config_path, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Validation de la structure\n",
        "    required_sections = ['calibration', 'objects', 'initial_annotations']\n",
        "    for section in required_sections:\n",
        "        if section not in config:\n",
        "            raise ValueError(f\"âŒ Section '{section}' manquante dans le config\")\n",
        "\n",
        "    # Statistiques\n",
        "    num_objects = len(config['objects'])\n",
        "    num_annotations = 0\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        num_annotations += len(frame_data['annotations'])\n",
        "\n",
        "    print(f\"âœ… Configuration chargÃ©e:\")\n",
        "    print(f\"   ğŸ“· Calibration camÃ©ra: OK\")\n",
        "    print(f\"   ğŸ¯ Objets dÃ©finis: {num_objects}\")\n",
        "    print(f\"   ğŸ“ Annotations initiales: {num_annotations}\")\n",
        "\n",
        "    # RÃ©sumÃ© des types d'objets\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_type = obj['obj_type']\n",
        "        obj_types[obj_type] = obj_types.get(obj_type, 0) + 1\n",
        "    print(f\"   ğŸ·ï¸  Types: {dict(obj_types)}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# Chargement de la configuration\n",
        "config = load_config_file(config_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu1xKxBRjzd1",
        "outputId": "2f843b32-37b6-4848-a365-2382c292f50c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'calibration': {'status': 'success',\n",
              "  'camera_parameters': {'mode': 'full',\n",
              "   'use_ransac': 5,\n",
              "   'rep_err': 5.677561117314194,\n",
              "   'cam_params': {'pan_degrees': 42.19814348024774,\n",
              "    'tilt_degrees': 63.36640628466144,\n",
              "    'roll_degrees': 1.0557592747483122,\n",
              "    'x_focal_length': 1567.6990141649394,\n",
              "    'y_focal_length': 1567.6990141649394,\n",
              "    'principal_point': [959.5, 539.5],\n",
              "    'position_meters': [-62.3440986793476,\n",
              "     32.78263355828649,\n",
              "     -14.987520424641314],\n",
              "    'rotation_matrix': [[0.735152508193547,\n",
              "      0.677701643528148,\n",
              "      0.016470338672408935],\n",
              "     [-0.3147092720910286, 0.3196673869020791, 0.8937398031928149],\n",
              "     [0.6004239033854928, -0.6622184262842915, 0.4482832721983776]],\n",
              "    'radial_distortion': [0, 0, 0, 0, 0, 0],\n",
              "    'tangential_distortion': [0, 0],\n",
              "    'thin_prism_distortion': [0, 0, 0, 0]},\n",
              "   'calib_plane': 0},\n",
              "  'input_lines': {'Side line top': [{'x': 2.640748255461165,\n",
              "     'y': 222.55601554081358},\n",
              "    {'x': 971.9611366049758, 'y': 48.76291209253772},\n",
              "    {'x': 1257.7863793234224, 'y': -0.892260321255388}],\n",
              "   'Middle line': [{'x': 971.9611366049758, 'y': 48.76291209253772},\n",
              "    {'x': 1347.883466702063, 'y': 113.93532588564116},\n",
              "    {'x': 1754.873757964199, 'y': 185.31463623046875},\n",
              "    {'x': 1910.2135637894417, 'y': 213.24567071322736}],\n",
              "   'Circle central': [{'x': 1347.883466702063, 'y': 113.93532588564116},\n",
              "    {'x': 1490.7960880612864, 'y': 98.41808450633083},\n",
              "    {'x': 1633.7087094205096, 'y': 98.41808450633083},\n",
              "    {'x': 1720.6990006826456, 'y': 126.34911898908945},\n",
              "    {'x': 1754.873757964199, 'y': 185.31463623046875},\n",
              "    {'x': 1636.8155055370146, 'y': 207.03877416150323},\n",
              "    {'x': 1472.1553113622572, 'y': 194.62498105805497},\n",
              "    {'x': 1388.2718162166263, 'y': 172.90084312702047},\n",
              "    {'x': 1313.7087094205099, 'y': 135.65946381667564},\n",
              "    {'x': 1347.883466702063, 'y': 113.93532588564116}],\n",
              "   'Big rect. left bottom': [{'x': 888.0776414593447, 'y': 998.4180845063308},\n",
              "    {'x': 1515.6504569933252, 'y': 604.280153471848}],\n",
              "   'Big rect. left main': [{'x': 1515.6504569933252, 'y': 604.280153471848},\n",
              "    {'x': 1055.8446317506068, 'y': 418.0732569201239},\n",
              "    {'x': 698.5630783525486, 'y': 297.0387741615032},\n",
              "    {'x': 496.621330779733, 'y': 213.24567071322736}],\n",
              "   'Circle left': [{'x': 1055.8446317506068, 'y': 418.0732569201239},\n",
              "    {'x': 1018.5630783525486, 'y': 340.4870500235722},\n",
              "    {'x': 900.504825925364, 'y': 297.0387741615032},\n",
              "    {'x': 766.9125929156553, 'y': 287.728429333917},\n",
              "    {'x': 698.5630783525486, 'y': 297.0387741615032}],\n",
              "   'Big rect. left top': [{'x': 496.621330779733, 'y': 213.24567071322736},\n",
              "    {'x': 21.28152495449029, 'y': 321.86636036839974}],\n",
              "   'Small rect. left top': [{'x': 136.23298126516988, 'y': 424.28015347184805},\n",
              "    {'x': 338.1747288379854, 'y': 362.2111879546067}],\n",
              "   'Small rect. left main': [{'x': 338.1747288379854, 'y': 362.2111879546067},\n",
              "    {'x': 723.4174472845874, 'y': 588.7629120925377}],\n",
              "   'Small rect. left bottom': [{'x': 723.4174472845874,\n",
              "     'y': 588.7629120925377},\n",
              "    {'x': 487.3009424302184, 'y': 691.1767051959861}],\n",
              "   'Side line left': [{'x': 996.8155055370146, 'y': 1072.9008431270204},\n",
              "    {'x': 888.0776414593447, 'y': 998.4180845063308},\n",
              "    {'x': 487.3009424302184, 'y': 691.1767051959861},\n",
              "    {'x': 136.23298126516988, 'y': 424.28015347184805},\n",
              "    {'x': 21.28152495449029, 'y': 321.86636036839974}]},\n",
              "  'detections': None,\n",
              "  'message': 'Calibration rÃ©ussie'},\n",
              " 'objects': [{'obj_id': 1, 'obj_type': 'ball', 'team': None},\n",
              "  {'obj_id': 2, 'obj_type': 'player', 'team': 1},\n",
              "  {'obj_id': 3, 'obj_type': 'player', 'team': 1},\n",
              "  {'obj_id': 4, 'obj_type': 'player', 'team': 2},\n",
              "  {'obj_id': 5, 'obj_type': 'player', 'team': 2},\n",
              "  {'obj_id': 6, 'obj_type': 'player', 'team': 1},\n",
              "  {'obj_id': 7, 'obj_type': 'player', 'team': 2},\n",
              "  {'obj_id': 8, 'obj_type': 'player', 'team': 1},\n",
              "  {'obj_id': 9, 'obj_type': 'player', 'team': 2},\n",
              "  {'obj_id': 10, 'obj_type': 'player', 'team': 2},\n",
              "  {'obj_id': 11, 'obj_type': 'player', 'team': 2}],\n",
              " 'initial_annotations': [{'frame': 0,\n",
              "   'annotations': [{'obj_id': 1, 'points': [{'x': 540, 'y': 328, 'label': 1}]},\n",
              "    {'obj_id': 2, 'points': [{'x': 344, 'y': 447, 'label': 1}]},\n",
              "    {'obj_id': 3, 'points': [{'x': 360, 'y': 360, 'label': 1}]},\n",
              "    {'obj_id': 4, 'points': [{'x': 620, 'y': 386, 'label': 1}]},\n",
              "    {'obj_id': 5, 'points': [{'x': 376, 'y': 331, 'label': 1}]},\n",
              "    {'obj_id': 6, 'points': [{'x': 588, 'y': 305, 'label': 1}]},\n",
              "    {'obj_id': 7, 'points': [{'x': 678, 'y': 286, 'label': 1}]},\n",
              "    {'obj_id': 8, 'points': [{'x': 964, 'y': 331, 'label': 1}]},\n",
              "    {'obj_id': 9, 'points': [{'x': 996, 'y': 312, 'label': 1}]},\n",
              "    {'obj_id': 10, 'points': [{'x': 1115, 'y': 402, 'label': 1}]},\n",
              "    {'obj_id': 11, 'points': [{'x': 1193, 'y': 212, 'label': 1}]}]}]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzoA46BbAdf2",
        "outputId": "01f21875-44ba-4a30-8978-23025db22c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¬ Extraction des frames...\n",
            "   ğŸ“¹ Source: ..\\data\\videos\\SD_13_06_2025_1_PdB_S1_T959s.mp4\n",
            "   ğŸ“ Destination: ..\\data\\videos\\outputs\\SD_13_06_2025_1_PdB_S1_T959s\\frames\n",
            "   â¯ï¸  Intervalle: 3\n",
            "   ğŸ”„ Force extraction: âŒ Non\n",
            "ğŸ“‚ 208 frames dÃ©jÃ  extraites - SKIP\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¬ EXTRACTION DES FRAMES\n",
        "# =============================================================================\n",
        "\n",
        "def extract_frames(video_path: Path, frames_dir: Path, frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"Extrait les frames de la vidÃ©o selon l'intervalle spÃ©cifiÃ©.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ¬ Extraction des frames...\")\n",
        "    print(f\"   ğŸ“¹ Source: {video_path}\")\n",
        "    print(f\"   ğŸ“ Destination: {frames_dir}\")\n",
        "    print(f\"   â¯ï¸  Intervalle: {frame_interval}\")\n",
        "    print(f\"   ğŸ”„ Force extraction: {'âœ… Oui' if force_extraction else 'âŒ Non'}\")\n",
        "\n",
        "    # VÃ©rification si extraction dÃ©jÃ  faite\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    if existing_frames and not force_extraction:\n",
        "        print(f\"ğŸ“‚ {len(existing_frames)} frames dÃ©jÃ  extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"ğŸ”„ {len(existing_frames)} frames existantes - SUPPRESSION et rÃ©-extraction...\")\n",
        "        # Supprimer les frames existantes\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"ğŸ—‘ï¸  Frames existantes supprimÃ©es\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"âŒ Impossible d'ouvrir la vidÃ©o: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"ğŸ“Š VidÃ©o: {total_frames} frames, {fps:.1f} FPS\")\n",
        "\n",
        "    extracted_count = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extraire seulement selon l'intervalle\n",
        "            if frame_idx % frame_interval == 0:\n",
        "                output_idx = frame_idx // frame_interval\n",
        "                filename = frames_dir / f\"{output_idx:05d}.jpg\"\n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "\n",
        "                if extracted_count % 50 == 0:\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    print(f\"ğŸ“Š ProgrÃ¨s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"âœ… {extracted_count} frames extraites\")\n",
        "    return extracted_count\n",
        "\n",
        "# Extraction des frames\n",
        "if EXTRACT_FRAMES:\n",
        "    extracted_frames_count = extract_frames(video_path, frames_dir, FRAME_INTERVAL, FORCE_EXTRACTION)\n",
        "else:\n",
        "    # Skip extraction - compter les frames existantes\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    extracted_frames_count = len(existing_frames)\n",
        "\n",
        "    print(f\"â­ï¸  Extraction dÃ©sactivÃ©e\")\n",
        "    if extracted_frames_count > 0:\n",
        "        print(f\"ğŸ“‚ Utilisation de {extracted_frames_count} frames existantes\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Aucune frame trouvÃ©e dans {frames_dir}\")\n",
        "        print(f\"ğŸ’¡ Conseil: Activez EXTRACT_FRAMES=True pour extraire les frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cziqr2A3Adf2",
        "outputId": "13efa7fc-40f4-4e0d-9c66-03c88ca7b516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Initialisation SAM2...\n",
            "   ğŸ§  ModÃ¨le: configs/sam2.1/sam2.1_hiera_l.yaml\n",
            "   ğŸ’¾ Checkpoint: ../checkpoints/sam2.1_hiera_large.pt\n",
            "   ğŸ–¥ï¸  Device: cpu\n",
            "\n",
            "ğŸ¬ Initialisation Ã©tat d'infÃ©rence...\n",
            "   ğŸ“ Frames: ..\\data\\videos\\outputs\\SD_13_06_2025_1_PdB_S1_T959s\\frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:17<00:00, 11.57it/s]\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¤– INITIALISATION SAM2\n",
        "# =============================================================================\n",
        "\n",
        "# Import SAM2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# Configuration des chemins SAM2\n",
        "checkpoint_path = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_config_path = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "print(f\"ğŸ¤– Initialisation SAM2...\")\n",
        "print(f\"   ğŸ§  ModÃ¨le: {model_config_path}\")\n",
        "print(f\"   ğŸ’¾ Checkpoint: {checkpoint_path}\")\n",
        "print(f\"   ğŸ–¥ï¸  Device: {device}\")\n",
        "\n",
        "# Construction du predictor\n",
        "predictor = build_sam2_video_predictor(\n",
        "    config_file=model_config_path,\n",
        "    ckpt_path=checkpoint_path,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialisation de l'Ã©tat d'infÃ©rence\n",
        "print(f\"\\nğŸ¬ Initialisation Ã©tat d'infÃ©rence...\")\n",
        "print(f\"   ğŸ“ Frames: {frames_dir}\")\n",
        "\n",
        "inference_state = predictor.init_state(\n",
        "    video_path=str(frames_dir),\n",
        "    offload_video_to_cpu=True,    # Ã‰conomise la mÃ©moire GPU\n",
        "    offload_state_to_cpu=False    # Garde l'Ã©tat en GPU\n",
        ")\n",
        "\n",
        "# Reset de l'Ã©tat\n",
        "predictor.reset_state(inference_state)\n",
        "\n",
        "# VÃ©rification\n",
        "loaded_frames = inference_state[\"num_frames\"]\n",
        "print(f\"\\nâœ… SAM2 initialisÃ©:\")\n",
        "print(f\"   ğŸ–¼ï¸  Frames extraites: {extracted_frames_count}\")\n",
        "print(f\"   ğŸ¬ Frames chargÃ©es: {loaded_frames}\")\n",
        "print(f\"   âœ… Correspondance: {'OK' if extracted_frames_count == loaded_frames else 'ERREUR'}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    print(f\"   ğŸ’¾ GPU Memory: {allocated:.2f}GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twDWK7TrAdf3",
        "outputId": "08b192e9-cbd5-45f7-8042-6bd65b4da128"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¯ AJOUT DES ANNOTATIONS INITIALES\n",
        "# =============================================================================\n",
        "\n",
        "def add_initial_annotations(predictor, inference_state, config: Dict[str, Any]):\n",
        "    \"\"\"Ajoute les annotations initiales depuis la configuration.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ¯ Ajout des annotations initiales...\")\n",
        "\n",
        "    # CrÃ©ation du mapping obj_id -> obj_type\n",
        "    obj_types = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_types[obj['obj_id']] = obj['obj_type']\n",
        "\n",
        "    # Extraction automatique des annotations et frames depuis le JSON\n",
        "    all_annotations = []\n",
        "    annotation_frames = []\n",
        "\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        frame_idx = frame_data['frame']\n",
        "        annotations = frame_data['annotations']\n",
        "        annotation_frames.append(frame_idx)\n",
        "\n",
        "        print(f\"   ğŸ“ Frame {frame_idx}: {len(annotations)} annotations\")\n",
        "\n",
        "        for annotation in annotations:\n",
        "            all_annotations.append({\n",
        "                'frame': frame_idx,\n",
        "                'obj_id': annotation['obj_id'],\n",
        "                'points': annotation['points'],\n",
        "                'obj_type': obj_types.get(annotation['obj_id'], f'unknown_{annotation[\"obj_id\"]}')\n",
        "            })\n",
        "\n",
        "    if not all_annotations:\n",
        "        raise ValueError(f\"âŒ Aucune annotation trouvÃ©e dans le fichier config\")\n",
        "\n",
        "    print(f\"   ğŸ“Š Total: {len(all_annotations)} annotations sur {len(set(annotation_frames))} frames\")\n",
        "\n",
        "    # Ajout des annotations Ã  SAM2\n",
        "    added_objects = []\n",
        "\n",
        "    for annotation_data in all_annotations:\n",
        "        frame_idx = annotation_data['frame']\n",
        "        obj_id = annotation_data['obj_id']\n",
        "        obj_type = annotation_data['obj_type']\n",
        "        points_data = annotation_data['points']\n",
        "\n",
        "        # Extraction des coordonnÃ©es et labels\n",
        "        points = np.array([[p['x'], p['y']] for p in points_data], dtype=np.float32)\n",
        "        labels = np.array([p['label'] for p in points_data], dtype=np.int32)\n",
        "\n",
        "        print(f\"   ğŸ¯ Frame {frame_idx} - Objet {obj_id} ({obj_type}): {len(points)} points Ã  ({points[0][0]:.0f}, {points[0][1]:.0f})\")\n",
        "\n",
        "        # Ajout Ã  SAM2 avec add_new_points_or_box\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state,\n",
        "            frame_idx,\n",
        "            obj_id,\n",
        "            points,\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # Ã‰viter les doublons dans added_objects\n",
        "        if not any(obj['obj_id'] == obj_id for obj in added_objects):\n",
        "            added_objects.append({\n",
        "                'obj_id': obj_id,\n",
        "                'obj_type': obj_type,\n",
        "                'points_count': len(points)\n",
        "            })\n",
        "\n",
        "    # VÃ©rification\n",
        "    sam_obj_ids = inference_state[\"obj_ids\"]\n",
        "\n",
        "    print(f\"\\nğŸ“Š RÃ‰SUMÃ‰ ANNOTATIONS:\")\n",
        "    print(f\"   ğŸ¯ Annotations configurÃ©es: {len(all_annotations)}\")\n",
        "    print(f\"   ğŸ¯ Objets uniques: {len(added_objects)}\")\n",
        "    print(f\"   âœ… Objets ajoutÃ©s Ã  SAM2: {len(sam_obj_ids)}\")\n",
        "    print(f\"   ğŸ†” IDs: {sorted(sam_obj_ids)}\")\n",
        "    print(f\"   ğŸ“ Frames utilisÃ©es: {sorted(set(annotation_frames))}\")\n",
        "\n",
        "    # RÃ©sumÃ© par type\n",
        "    type_counts = {}\n",
        "    for obj in added_objects:\n",
        "        obj_type = obj['obj_type']\n",
        "        type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "    print(f\"   ğŸ·ï¸  Types: {dict(type_counts)}\")\n",
        "\n",
        "    return added_objects, all_annotations\n",
        "\n",
        "# Ajout des annotations\n",
        "added_objects, initial_annotations_data = add_initial_annotations(predictor, inference_state, config)\n",
        "print(\"\\nâœ… Annotations initiales ajoutÃ©es avec succÃ¨s!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DH-AiTYdI57C"
      },
      "outputs": [],
      "source": [
        "# config['calibration']['camera_parameters']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Cpc-6vAdf3",
        "outputId": "72835c64-c66e-4255-81fc-b274a9d1d35b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ğŸ”„ PROPAGATION ET GÃ‰NÃ‰RATION DES ANNOTATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_frame_mapping(total_frames: int, frame_interval: int) -> List[Optional[int]]:\n",
        "    \"\"\"\n",
        "    GÃ©nÃ¨re le mapping entre frames originales et frames traitÃ©es.\n",
        "\n",
        "    Args:\n",
        "        total_frames: Nombre total de frames dans la vidÃ©o originale\n",
        "        frame_interval: Intervalle entre frames (ex: 10 = 1 frame sur 10)\n",
        "\n",
        "    Returns:\n",
        "        Liste oÃ¹ l'index = frame originale, valeur = frame traitÃ©e (ou None si pas traitÃ©e)\n",
        "    \"\"\"\n",
        "    frame_mapping = [None] * total_frames\n",
        "    processed_idx = 0\n",
        "\n",
        "    for original_idx in range(total_frames):\n",
        "        if original_idx % frame_interval == 0:\n",
        "            frame_mapping[original_idx] = processed_idx\n",
        "            processed_idx += 1\n",
        "\n",
        "    return frame_mapping\n",
        "\n",
        "def create_project_structure(config: Dict[str, Any], video_info: Dict[str, Any], added_objects: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"CrÃ©e la structure JSON du projet.\"\"\"\n",
        "\n",
        "    # Calcul du frame mapping\n",
        "    frame_mapping = generate_frame_mapping(\n",
        "        video_info['total_frames'],\n",
        "        FRAME_INTERVAL\n",
        "    )\n",
        "    processed_frame_count = sum(x is not None for x in frame_mapping)\n",
        "\n",
        "    # Structure objects avec couleurs\n",
        "    import random\n",
        "    import colorsys\n",
        "\n",
        "    config_objects_mapping = {}\n",
        "    for obj in config['objects']:\n",
        "        config_objects_mapping[obj['obj_id']] = obj\n",
        "\n",
        "    objects = {}\n",
        "    for obj_data in added_objects:\n",
        "        obj_id = str(obj_data['obj_id'])\n",
        "        obj_type = obj_data['obj_type']\n",
        "\n",
        "        # RÃ©cupÃ©rer les informations complÃ¨tes depuis le config\n",
        "        config_obj = config_objects_mapping.get(int(obj_id), {})\n",
        "\n",
        "        # Couleur alÃ©atoire reproductible\n",
        "        random.seed(int(obj_id) * 12345)\n",
        "        hue = random.random()\n",
        "        rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n",
        "        )\n",
        "\n",
        "        objects[obj_id] = {\n",
        "            \"id\": obj_id,\n",
        "            \"type\": obj_type,\n",
        "            \"team\": config_obj.get('team', None),  # â† RÃ©cupÃ©rÃ© depuis le config\n",
        "            \"jersey_number\": config_obj.get('jersey_number', None),\n",
        "            \"jersey_color\": config_obj.get('jersey_color', None),\n",
        "            \"role\": config_obj.get('role', None),\n",
        "            \"display_color\": hex_color\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"format_version\": \"1.0\",\n",
        "        \"video\": f\"{VIDEO_NAME}.mp4\",\n",
        "        \"metadata\": {\n",
        "            \"project_id\": str(uuid.uuid4()),\n",
        "            \"created_at\": datetime.now().isoformat() + \"Z\",\n",
        "            \"fps\": video_info['fps'],\n",
        "            \"resolution\": {\n",
        "                \"width\": video_info['width'],\n",
        "                \"height\": video_info['height'],\n",
        "                \"aspect_ratio\": round(video_info['width'] / video_info['height'], 2)\n",
        "            },\n",
        "            \"frame_interval\": FRAME_INTERVAL,\n",
        "            \"frame_count_original\": video_info['total_frames'],\n",
        "            \"frame_count_processed\": processed_frame_count,\n",
        "            \"frame_mapping\": frame_mapping,\n",
        "            \"static_video\": False\n",
        "        },\n",
        "        \"calibration\": config['calibration'],\n",
        "        \"objects\": objects,\n",
        "        \"initial_annotations\": config['initial_annotations'],  # â† Annotations initiales depuis config\n",
        "        \"annotations\": {}\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ”§ FONCTIONS UTILITAIRES POUR ANNOTATIONS COMPLÃˆTES\n",
        "# =============================================================================\n",
        "\n",
        "def get_object_scores(predictor, inference_state, frame_idx, obj_id):\n",
        "    \"\"\"RÃ©cupÃ¨re les scores d'objet de maniÃ¨re propre et sÃ»re\"\"\"\n",
        "    try:\n",
        "        obj_idx = predictor._obj_id_to_idx(inference_state, obj_id)\n",
        "        obj_output_dict = inference_state[\"output_dict_per_obj\"][obj_idx]\n",
        "        temp_output_dict = inference_state[\"temp_output_dict_per_obj\"][obj_idx]\n",
        "\n",
        "        # Chercher dans les outputs\n",
        "        frame_output = None\n",
        "        if frame_idx in temp_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in temp_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "\n",
        "        if frame_output and \"object_score_logits\" in frame_output:\n",
        "            object_score_logits = frame_output[\"object_score_logits\"]\n",
        "            return torch.sigmoid(object_score_logits).item()\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def calculate_bbox_from_rle(rle_data: Dict[str, Any]) -> Optional[Dict[str, int]]:\n",
        "    \"\"\"Calcule la bounding box depuis un RLE base64.\"\"\"\n",
        "    from pycocotools.mask import toBbox\n",
        "\n",
        "    try:\n",
        "        rle = {\n",
        "            \"size\": rle_data[\"size\"],\n",
        "            \"counts\": base64.b64decode(rle_data[\"counts\"])\n",
        "        }\n",
        "\n",
        "        bbox = toBbox(rle)\n",
        "\n",
        "        result = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erreur calcul bbox: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def image_to_world(point_2d, cam_params):\n",
        "    \"\"\"\n",
        "    Projette un point 2D de l'image vers le plan du terrain (Z=0).\n",
        "    \"\"\"\n",
        "    # Create projection matrix P\n",
        "    K = np.array([\n",
        "        [cam_params[\"cam_params\"][\"x_focal_length\"], 0, cam_params[\"cam_params\"][\"principal_point\"][0]],\n",
        "        [0, cam_params[\"cam_params\"][\"y_focal_length\"], cam_params[\"cam_params\"][\"principal_point\"][1]],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    R = np.array(cam_params[\"cam_params\"][\"rotation_matrix\"])\n",
        "    t = -R @ np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    P = K @ np.hstack((R, t.reshape(-1,1)))\n",
        "\n",
        "    # Create point on image plane in homogeneous coordinates\n",
        "    point_2d_h = np.array([point_2d[0], point_2d[1], 1])\n",
        "\n",
        "    # Back-project ray from camera\n",
        "    ray = np.linalg.inv(K) @ point_2d_h\n",
        "    ray = R.T @ ray\n",
        "\n",
        "    # Find intersection with Z=0 plane\n",
        "    camera_pos = np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    t = -camera_pos[2] / ray[2]\n",
        "    world_point = camera_pos + t * ray\n",
        "\n",
        "    return world_point[:2]  # Return only X,Y coordinates since Z=0\n",
        "\n",
        "def calculate_points_output(bbox_output: dict, cam_params: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calcule les points de sortie Ã  partir de la bbox output.\n",
        "\n",
        "    Args:\n",
        "        bbox_output: Dict avec 'x', 'y', 'width', 'height'\n",
        "        cam_params: ParamÃ¨tres de calibration camÃ©ra pour projection terrain\n",
        "\n",
        "    Returns:\n",
        "        Dict avec les points calculÃ©s sÃ©parÃ©s par plan (image vs field)\n",
        "    \"\"\"\n",
        "    if not bbox_output:\n",
        "        return None\n",
        "\n",
        "    # Calculer le point CENTER_BOTTOM dans le plan image\n",
        "    center_bottom_x = bbox_output['x'] + bbox_output['width'] / 2\n",
        "    center_bottom_y = bbox_output['y'] + bbox_output['height']  # Bas de la bbox\n",
        "\n",
        "    # Structure avec sÃ©paration image/field\n",
        "    points_output = {\n",
        "        \"image\": {\n",
        "            \"CENTER_BOTTOM\": {\n",
        "                \"x\": float(center_bottom_x),\n",
        "                \"y\": float(center_bottom_y)\n",
        "            }\n",
        "        },\n",
        "        \"field\": {\n",
        "            \"CENTER_BOTTOM\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Projection vers le terrain si les paramÃ¨tres camÃ©ra sont fournis\n",
        "    if cam_params:\n",
        "        try:\n",
        "            # Projeter le point CENTER_BOTTOM vers le terrain\n",
        "            image_point = [center_bottom_x, center_bottom_y]\n",
        "            field_point = image_to_world(image_point, cam_params)\n",
        "\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = {\n",
        "                \"x\": float(field_point[0]),\n",
        "                \"y\": float(field_point[1])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erreur projection terrain: {e}\")\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = None\n",
        "\n",
        "    return points_output\n",
        "\n",
        "def create_mask_annotation(obj_id: int, mask_logits, predictor=None, inference_state=None,\n",
        "                         frame_idx=None, cam_params: Dict = None) -> Dict:\n",
        "    \"\"\"\n",
        "    CrÃ©e une annotation de masque complÃ¨te avec points input/output, bbox et scores.\n",
        "    \"\"\"\n",
        "    # Conversion en masque binaire\n",
        "    mask = (mask_logits > 0.0).cpu().numpy()\n",
        "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "\n",
        "    # Encodage RLE\n",
        "    if not mask.flags['F_CONTIGUOUS']:\n",
        "        mask = np.asfortranarray(mask)\n",
        "\n",
        "    rle = encode_rle(mask.astype(np.uint8))\n",
        "    base64_counts = base64.b64encode(rle[\"counts\"]).decode('ascii')\n",
        "\n",
        "    # Calcul bbox et points output si masque non vide\n",
        "    bbox_output = None\n",
        "    points_output = None\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        from pycocotools.mask import toBbox\n",
        "        bbox = toBbox(rle)\n",
        "        bbox_output = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        # Calcul des points output depuis la bbox\n",
        "        points_output = calculate_points_output(bbox_output, cam_params)\n",
        "    # else:\n",
        "    #     print(f\"âš ï¸ Masque vide pour l'objet {obj_id}, bbox et points output = None\")\n",
        "\n",
        "    # RÃ©cupÃ©ration du score du masque\n",
        "    mask_score = None\n",
        "\n",
        "    if predictor and inference_state and frame_idx is not None:\n",
        "        # Score du masque\n",
        "        mask_score = get_object_scores(predictor, inference_state, frame_idx, obj_id)\n",
        "\n",
        "    # Structure d'annotation complÃ¨te\n",
        "    return {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"objectId\": str(obj_id),\n",
        "        \"type\": \"mask\",\n",
        "        \"mask\": {\n",
        "            \"format\": \"rle_coco_base64\",\n",
        "            \"size\": [int(rle[\"size\"][0]), int(rle[\"size\"][1])],\n",
        "            \"counts\": base64_counts\n",
        "        },\n",
        "        \"bbox\": {\n",
        "            \"output\": bbox_output\n",
        "        },\n",
        "        \"points\": {\n",
        "            \"output\": points_output\n",
        "        },\n",
        "        \"maskScore\": mask_score,\n",
        "        \"pose\": None,\n",
        "        \"warning\": False\n",
        "    }\n",
        "\n",
        "# Informations vidÃ©o\n",
        "cap = cv2.VideoCapture(str(video_path))\n",
        "video_info = {\n",
        "    'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "    'fps': cap.get(cv2.CAP_PROP_FPS),\n",
        "    'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "    'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "}\n",
        "cap.release()\n",
        "\n",
        "# CrÃ©ation du projet\n",
        "project = create_project_structure(config, video_info, added_objects)\n",
        "\n",
        "print(f\"ğŸ”„ DÃ©marrage de la propagation...\")\n",
        "print(f\"   ğŸ¬ {extracted_frames_count} frames Ã  traiter\")\n",
        "print(f\"   ğŸ¯ {len(added_objects)} objets Ã  suivre\")\n",
        "\n",
        "# Propagation et annotation\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "\n",
        "    if str(out_frame_idx) not in project['annotations']:\n",
        "        project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "    for i, out_obj_id in enumerate(out_obj_ids):\n",
        "        annotation = create_mask_annotation(\n",
        "            obj_id=out_obj_id,\n",
        "            mask_logits=out_mask_logits[i],\n",
        "            predictor=predictor,\n",
        "            inference_state=inference_state,\n",
        "            frame_idx=out_frame_idx,\n",
        "            cam_params=config['calibration']['camera_parameters']\n",
        "        )\n",
        "        project['annotations'][str(out_frame_idx)].append(annotation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXehyd8mHIu"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHlhY4X5Adf3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ğŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"ğŸ’¾ Sauvegarde des rÃ©sultats...\")\n",
        "\n",
        "# Sauvegarde du JSON\n",
        "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(project, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Fichier JSON sauvÃ©: {output_json_path}\")\n",
        "\n",
        "# Statistiques finales\n",
        "total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "unique_frames = len(project['annotations'])\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“Š RÃ‰SULTATS FINAUX:\")\n",
        "print(f\"   ğŸ¬ Frames originales: {project['metadata']['frame_count_original']}\")\n",
        "print(f\"   ğŸ¬ Frames traitÃ©es: {project['metadata']['frame_count_processed']}\")\n",
        "print(f\"   ğŸ“ Frames avec annotations: {unique_frames}\")\n",
        "print(f\"   ğŸ“ Annotations totales (propagÃ©es): {total_annotations}\")\n",
        "print(f\"   ğŸ¯ Objets suivis: {len(project['objects'])}\")\n",
        "print(f\"   â¯ï¸  Intervalle: {project['metadata']['frame_interval']}\")\n",
        "print(f\"   ğŸ“„ Fichier de sortie: {output_json_path}\")\n",
        "print(f\"   ğŸ“ Dossier de sortie: {output_dir}\")\n",
        "\n",
        "# Affichage d'un Ã©chantillon du mapping\n",
        "sample_mapping = [(i, v) for i, v in enumerate(project['metadata']['frame_mapping'][:50]) if v is not None]\n",
        "print(f\"   ğŸ—‚ï¸  Mapping Ã©chantillon (originalâ†’traitÃ©): {sample_mapping[:5]}...\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Pipeline SAM2 terminÃ©e avec succÃ¨s!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "B_rW1X1nAdf4",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ‰ Pipeline TerminÃ©e !\n",
        "\n",
        "### ğŸ“‹ RÃ©sultats produits :\n",
        "- **Frames extraites** : `videos/outputs/{VIDEO_NAME}/frames/`\n",
        "- **Projet JSON** : `videos/outputs/{VIDEO_NAME}/{VIDEO_NAME}_project.json`\n",
        "- **Masques** : IntÃ©grÃ©s dans le JSON au format RLE base64\n",
        "\n",
        "### ğŸ“„ Structure du JSON de sortie :\n",
        "```json\n",
        "{\n",
        "  \"metadata\": { /* informations projet */ },\n",
        "  \"calibration\": { /* paramÃ¨tres camÃ©ra depuis config */ },\n",
        "  \"objects\": { /* objets avec couleurs gÃ©nÃ©rÃ©es */ },\n",
        "  \"initial_annotations\": { /* annotations d'entrÃ©e enrichies */ },\n",
        "  \"annotations\": { /* rÃ©sultats de segmentation SAM2 */ }\n",
        "}\n",
        "```\n",
        "\n",
        "La section **`initial_annotations`** contient maintenant :\n",
        "- ğŸ“‹ **DonnÃ©es originales** : annotations du fichier config\n",
        "- ğŸ“Š **Statistiques enrichies** : nombre d'objets, points, frames\n",
        "- ğŸ“ **Informations de traÃ§abilitÃ©** : source, date de chargement\n",
        "- ğŸ“ˆ **Statistiques par frame** : dÃ©tails par frame d'annotation\n",
        "\n",
        "### ğŸ”§ Configuration utilisÃ©e :\n",
        "- **VidÃ©o source** : `videos/{VIDEO_NAME}.mp4`\n",
        "- **Config source** : `videos/{VIDEO_NAME}_config.json`\n",
        "- **Calibration** : ChargÃ©e depuis le config JSON\n",
        "- **Annotations initiales** : ChargÃ©es depuis le config JSON (frames automatiquement dÃ©tectÃ©es)\n",
        "\n",
        "### ğŸ“ˆ Performance :\n",
        "- **Device** : GPU/CPU automatiquement dÃ©tectÃ©\n",
        "- **Optimisations** : CUDA TF32 si disponible\n",
        "- **MÃ©moire** : Gestion intelligente GPU/CPU\n",
        "\n",
        "Le pipeline est maintenant **prÃªt Ã  Ãªtre utilisÃ© avec n'importe quelle vidÃ©o** ayant son fichier de configuration correspondant !\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ Pour utiliser avec une autre vidÃ©o :\n",
        "\n",
        "1. **Placez votre vidÃ©o** : `videos/ma_video.mp4`\n",
        "2. **CrÃ©ez le config** : `videos/ma_video_config.json` (mÃªme format que `cam3_long_config.json`)\n",
        "3. **Modifiez la configuration** dans la cellule 2 :\n",
        "   ```python\n",
        "   VIDEO_NAME = \"ma_video\"          # Changez juste ce nom !\n",
        "   EXTRACT_FRAMES = True            # True=Extraire, False=Skip\n",
        "   FORCE_EXTRACTION = False         # True=Forcer, False=Skip si existent\n",
        "   ```\n",
        "4. **ExÃ©cutez toutes les cellules** - c'est tout !\n",
        "\n",
        "### ğŸ¬ Options d'extraction des frames :\n",
        "\n",
        "- **`EXTRACT_FRAMES = True`** : Active l'extraction des frames\n",
        "- **`EXTRACT_FRAMES = False`** : Skip l'extraction, utilise les frames existantes\n",
        "- **`FORCE_EXTRACTION = True`** : Force la rÃ©-extraction mÃªme si frames existent\n",
        "- **`FORCE_EXTRACTION = False`** : Skip l'extraction si frames dÃ©jÃ  prÃ©sentes\n",
        "\n",
        "#### ğŸ’¡ Cas d'usage typiques :\n",
        "- **PremiÃ¨re fois** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=False`\n",
        "- **Frames dÃ©jÃ  extraites** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "- **Re-extraire avec nouvel intervalle** : `EXTRACT_FRAMES=True, FORCE_EXTRACTION=True`\n",
        "- **Utiliser frames existantes** : `EXTRACT_FRAMES=False, FORCE_EXTRACTION=False`\n",
        "\n",
        "### ğŸ“„ Format du fichier config JSON :\n",
        "```json\n",
        "{\n",
        "  \"calibration\": { /* paramÃ¨tres de calibration camÃ©ra */ },\n",
        "  \"objects\": [\n",
        "    {\"obj_id\": 1, \"obj_type\": \"player\"},\n",
        "    {\"obj_id\": 2, \"obj_type\": \"ball\"}\n",
        "  ],\n",
        "  \"initial_annotations\": [\n",
        "    {\n",
        "      \"frame\": 0,  // â† Frame automatiquement dÃ©tectÃ©e\n",
        "      \"annotations\": [\n",
        "        {\n",
        "          \"obj_id\": 1,\n",
        "          \"points\": [\n",
        "            {\"x\": 320, \"y\": 240, \"label\": 1},   // Foreground point\n",
        "            {\"x\": 400, \"y\": 300, \"label\": 0}    // Background point (optionnel)\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### âœ¨ **Nouvelles fonctionnalitÃ©s automatiques :**\n",
        "- ğŸ“ **Frames dÃ©tectÃ©es automatiquement** depuis le JSON (pas besoin de variable ANNOTATION_FRAME)\n",
        "- ğŸ¯ **Support multi-frames** : le notebook peut traiter plusieurs frames d'annotations initiales\n",
        "- ğŸ”„ **Points multiples par objet** : foreground (label=1) et background (label=0)\n",
        "- ğŸ¨ **Ã‰vitement des doublons** : chaque objet n'est comptÃ© qu'une fois mÃªme s'il a des annotations sur plusieurs frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i02azRkArbtw"
      },
      "outputs": [],
      "source": [
        "project['objects']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P3qZlb6HaCy"
      },
      "outputs": [],
      "source": [
        "using_colab=True\n",
        "if using_colab:\n",
        "  from google.colab import drive\n",
        "  import shutil\n",
        "\n",
        "  # Monter le Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # CrÃ©er directement le ZIP dans le Drive\n",
        "  shutil.make_archive(f'/content/drive/MyDrive/{VIDEO_NAME}', 'zip', '/content/videos')\n",
        "\n",
        "  print(\"Dossier vidÃ©o sauvegardÃ© en ZIP dans votre Google Drive !\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCDAZbPOMYv_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
