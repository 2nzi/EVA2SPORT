{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è Cr√©ation de la structure de dossiers...\n",
            "   ‚úÖ Dossiers cr√©√©s dans: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "üìã CONFIGURATION CENTRALIS√âE:\n",
            "   üåç Environnement: üñ•Ô∏è Local\n",
            "   üé¨ Vid√©o: SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "   ‚èØÔ∏è  Intervalle frames: 3\n",
            "   üé¨ Extraction: ‚úÖ Activ√©e\n",
            "   üîÑ Force extraction: ‚úÖ Oui\n",
            "   üéØ Segmentation: ‚úÖ Activ√©e\n",
            "   üé• FPS vid√©o: 25.0\n",
            "   üïê Offset avant: 2.0s (50 frames)\n",
            "   üïê Offset apr√®s: 2.0s (50 frames)\n",
            "   ü§ñ Mod√®le SAM2: sam2.1_hiera_l\n",
            "   üìÅ Dossier vid√©os: ..\\data\\videos\n",
            "   üìÑ Fichier config: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1_config.json\n",
            "   üìÅ Sortie: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "   üíæ Checkpoint: ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "\n",
            "üñ•Ô∏è Mode Local d√©tect√©:\n",
            "‚úÖ Tous les fichiers sont valides\n",
            "\n",
            "‚úÖ Configuration centralis√©e initialis√©e!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ‚öôÔ∏è CONFIGURATION CENTRALIS√âE\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# üåç D√âTECTION AUTOMATIQUE DE L'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def detect_environment():\n",
        "    \"\"\"D√©tecte automatiquement si on est sur Colab ou en local\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# üìã CONFIGURATION PRINCIPALE\n",
        "# =============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration centralis√©e du projet\"\"\"\n",
        "    \n",
        "    # üåç Environnement\n",
        "    USING_COLAB = detect_environment()\n",
        "    \n",
        "    # üé¨ VID√âO ET PROJET  \n",
        "    VIDEO_NAME = \"SD_13_06_2025_cam1_PdB_S1_T959s_1\"  # ‚ö†Ô∏è MODIFIEZ ICI\n",
        "    FRAME_INTERVAL = 3                            # ‚ö†Ô∏è MODIFIEZ ICI\n",
        "    \n",
        "    # üé¨ OPTIONS D'EXTRACTION\n",
        "    EXTRACT_FRAMES = True                         # ‚ö†Ô∏è MODIFIEZ ICI\n",
        "    FORCE_EXTRACTION = True                      # ‚ö†Ô∏è MODIFIEZ ICI\n",
        "    \n",
        "    # üéØ SEGMENTATION VID√âO (NOUVEAU)\n",
        "    SEGMENT_MODE = True                          # ‚ö†Ô∏è MODIFIEZ ICI - Active le mode segmentation\n",
        "    \n",
        "    # üïê OFFSETS EN SECONDES (RECOMMAND√â)\n",
        "    SEGMENT_OFFSET_BEFORE_SECONDS = 2.0         # ‚ö†Ô∏è MODIFIEZ ICI - Secondes AVANT la frame de r√©f√©rence\n",
        "    SEGMENT_OFFSET_AFTER_SECONDS = 2.0          # ‚ö†Ô∏è MODIFIEZ ICI - Secondes APR√àS la frame de r√©f√©rence\n",
        "    \n",
        "    # üé¨ OFFSETS EN FRAMES (OPTIONNEL - sera calcul√© automatiquement si secondes d√©finies)\n",
        "    SEGMENT_OFFSET_BEFORE = None                 # ‚ö†Ô∏è MODIFIEZ ICI - Frames AVANT (ou None pour auto-calcul)\n",
        "    SEGMENT_OFFSET_AFTER = None                  # ‚ö†Ô∏è MODIFIEZ ICI - Frames APR√àS (ou None pour auto-calcul)\n",
        "    \n",
        "    # ü§ñ SAM2 CONFIGURATION\n",
        "    SAM2_MODEL = \"sam2.1_hiera_l\"                # ou \"sam2.1_hiera_s\" pour small\n",
        "    SAM2_CHECKPOINT = \"sam2.1_hiera_large.pt\"    # ou \"sam2.1_hiera_small.pt\"\n",
        "    \n",
        "    # üóÇÔ∏è CHEMINS AUTOMATIQUES\n",
        "    @property\n",
        "    def videos_dir(self):\n",
        "        return Path(\"./videos\") if self.USING_COLAB else Path(\"../data/videos\")\n",
        "    \n",
        "    @property \n",
        "    def checkpoint_path(self):\n",
        "        base = \"../../checkpoints\" if self.USING_COLAB else \"../checkpoints\"\n",
        "        return Path(base) / self.SAM2_CHECKPOINT\n",
        "    \n",
        "    @property\n",
        "    def model_config_path(self):\n",
        "        return f\"configs/sam2.1/{self.SAM2_MODEL}.yaml\"\n",
        "    \n",
        "    # üé¨ CHEMINS VID√âO\n",
        "    @property\n",
        "    def video_path(self):\n",
        "        return self.videos_dir / f\"{self.VIDEO_NAME}.mp4\"\n",
        "    \n",
        "    @property\n",
        "    def config_path(self):\n",
        "        return self.videos_dir / f\"{self.VIDEO_NAME}_config.json\"\n",
        "    \n",
        "    @property\n",
        "    def output_dir(self):\n",
        "        return self.videos_dir / \"outputs\" / self.VIDEO_NAME\n",
        "    \n",
        "    @property\n",
        "    def frames_dir(self):\n",
        "        return self.output_dir / \"frames\"\n",
        "    \n",
        "    @property\n",
        "    def masks_dir(self):\n",
        "        return self.output_dir / \"masks\"\n",
        "    \n",
        "    @property\n",
        "    def output_video_path(self):\n",
        "        return self.output_dir / f\"{self.VIDEO_NAME}_annotated.mp4\"\n",
        "    \n",
        "    @property\n",
        "    def output_json_path(self):\n",
        "        return self.output_dir / f\"{self.VIDEO_NAME}_project.json\"\n",
        "    \n",
        "    def setup_directories(self):\n",
        "        \"\"\"Cr√©e tous les dossiers n√©cessaires (sans v√©rifier les fichiers)\"\"\"\n",
        "        print(\"üèóÔ∏è Cr√©ation de la structure de dossiers...\")\n",
        "        self.videos_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.frames_dir.mkdir(exist_ok=True)\n",
        "        self.masks_dir.mkdir(exist_ok=True)\n",
        "        print(f\"   ‚úÖ Dossiers cr√©√©s dans: {self.output_dir}\")\n",
        "    \n",
        "    def check_files_exist(self):\n",
        "        \"\"\"V√©rifie si les fichiers requis existent (sans lever d'erreur)\"\"\"\n",
        "        video_exists = self.video_path.exists()\n",
        "        config_exists = self.config_path.exists()\n",
        "        \n",
        "        print(f\"üìÑ V√©rification des fichiers:\")\n",
        "        print(f\"   üé¨ Vid√©o: {'‚úÖ' if video_exists else '‚ùå'} {self.video_path}\")\n",
        "        print(f\"   üìÑ Config: {'‚úÖ' if config_exists else '‚ùå'} {self.config_path}\")\n",
        "        \n",
        "        return video_exists and config_exists\n",
        "    \n",
        "    def wait_for_files(self, max_wait_minutes=10, check_interval=10):\n",
        "        \"\"\"Attend que les fichiers soient disponibles (utile pour Colab)\"\"\"\n",
        "        if not self.USING_COLAB:\n",
        "            # En local, validation imm√©diate\n",
        "            if not self.check_files_exist():\n",
        "                missing = []\n",
        "                if not self.video_path.exists():\n",
        "                    missing.append(f\"vid√©o: {self.video_path}\")\n",
        "                if not self.config_path.exists():\n",
        "                    missing.append(f\"config: {self.config_path}\")\n",
        "                raise FileNotFoundError(f\"‚ùå Fichiers manquants: {', '.join(missing)}\")\n",
        "            return True\n",
        "        \n",
        "        # Sur Colab, attente avec timeout\n",
        "        print(f\"‚è≥ Attente des fichiers (max {max_wait_minutes}min)...\")\n",
        "        start_time = time.time()\n",
        "        max_wait_seconds = max_wait_minutes * 60\n",
        "        \n",
        "        while time.time() - start_time < max_wait_seconds:\n",
        "            if self.check_files_exist():\n",
        "                print(\"‚úÖ Tous les fichiers sont disponibles!\")\n",
        "                return True\n",
        "            \n",
        "            elapsed = int(time.time() - start_time)\n",
        "            remaining = max_wait_seconds - elapsed\n",
        "            print(f\"   ‚è≥ Attente... ({elapsed}s √©coul√©es, {remaining}s restantes)\")\n",
        "            time.sleep(check_interval)\n",
        "        \n",
        "        print(f\"‚ö†Ô∏è Timeout atteint ({max_wait_minutes}min)\")\n",
        "        return False\n",
        "    \n",
        "    def validate_files_now(self):\n",
        "        \"\"\"Validation imm√©diate avec erreur si fichiers manquants\"\"\"\n",
        "        if not self.video_path.exists():\n",
        "            raise FileNotFoundError(f\"‚ùå Vid√©o non trouv√©e: {self.video_path}\")\n",
        "        if not self.config_path.exists():\n",
        "            raise FileNotFoundError(f\"‚ùå Fichier config non trouv√©: {self.config_path}\")\n",
        "        print(\"‚úÖ Tous les fichiers sont valides\")\n",
        "    \n",
        "    def get_video_fps(self):\n",
        "        \"\"\"R√©cup√®re le FPS de la vid√©o\"\"\"\n",
        "        if not self.video_path.exists():\n",
        "            print(f\"‚ö†Ô∏è  Vid√©o non trouv√©e pour r√©cup√©rer le FPS: {self.video_path}\")\n",
        "            return 25.0  # FPS par d√©faut\n",
        "        \n",
        "        import cv2\n",
        "        cap = cv2.VideoCapture(str(self.video_path))\n",
        "        if not cap.isOpened():\n",
        "            print(f\"‚ö†Ô∏è  Impossible d'ouvrir la vid√©o pour r√©cup√©rer le FPS\")\n",
        "            return 25.0  # FPS par d√©faut\n",
        "        \n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        cap.release()\n",
        "        return fps if fps > 0 else 25.0  # FPS par d√©faut si invalide\n",
        "    \n",
        "    def seconds_to_frames(self, seconds: float, fps: float = None) -> int:\n",
        "        \"\"\"Convertit des secondes en nombre de frames ENTI√àRES\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        # ‚úÖ CORRECTION: s'assurer que le r√©sultat est un entier (frame enti√®re)\n",
        "        return int(round(seconds * fps))\n",
        "    \n",
        "    def frames_to_seconds(self, frames: int, fps: float = None) -> float:\n",
        "        \"\"\"Convertit des frames en secondes\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        return frames / fps\n",
        "    \n",
        "    def get_segment_offsets_frames(self):\n",
        "        \"\"\"Calcule les offsets en frames ENTI√àRES, priorit√© aux secondes si d√©finies\"\"\"\n",
        "        fps = self.get_video_fps()\n",
        "        \n",
        "        # Priorit√© aux offsets en secondes\n",
        "        if hasattr(self, 'SEGMENT_OFFSET_BEFORE_SECONDS') and self.SEGMENT_OFFSET_BEFORE_SECONDS is not None:\n",
        "            offset_before_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_BEFORE_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_BEFORE') and self.SEGMENT_OFFSET_BEFORE is not None:\n",
        "            # ‚úÖ CORRECTION: s'assurer que m√™me les frames d√©finies manuellement sont des entiers\n",
        "            offset_before_frames = int(self.SEGMENT_OFFSET_BEFORE)\n",
        "        else:\n",
        "            offset_before_frames = self.seconds_to_frames(2.0, fps)  # D√©faut: 2 secondes\n",
        "        \n",
        "        if hasattr(self, 'SEGMENT_OFFSET_AFTER_SECONDS') and self.SEGMENT_OFFSET_AFTER_SECONDS is not None:\n",
        "            offset_after_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_AFTER_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_AFTER') and self.SEGMENT_OFFSET_AFTER is not None:\n",
        "            # ‚úÖ CORRECTION: s'assurer que m√™me les frames d√©finies manuellement sont des entiers\n",
        "            offset_after_frames = int(self.SEGMENT_OFFSET_AFTER)\n",
        "        else:\n",
        "            offset_after_frames = self.seconds_to_frames(2.0, fps)  # D√©faut: 2 secondes\n",
        "        \n",
        "        return offset_before_frames, offset_after_frames\n",
        "    \n",
        "    def get_video_fps(self):\n",
        "        \"\"\"R√©cup√®re le FPS de la vid√©o\"\"\"\n",
        "        if not self.video_path.exists():\n",
        "            print(f\"‚ö†Ô∏è  Vid√©o non trouv√©e pour r√©cup√©rer le FPS: {self.video_path}\")\n",
        "            return 25.0  # FPS par d√©faut\n",
        "        \n",
        "        import cv2\n",
        "        cap = cv2.VideoCapture(str(self.video_path))\n",
        "        if not cap.isOpened():\n",
        "            print(f\"‚ö†Ô∏è  Impossible d'ouvrir la vid√©o pour r√©cup√©rer le FPS\")\n",
        "            return 25.0  # FPS par d√©faut\n",
        "        \n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        cap.release()\n",
        "        return fps if fps > 0 else 25.0  # FPS par d√©faut si invalide\n",
        "    \n",
        "    def seconds_to_frames(self, seconds: float, fps: float = None) -> int:\n",
        "        \"\"\"Convertit des secondes en nombre de frames\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        return int(round(seconds * fps))\n",
        "    \n",
        "    def frames_to_seconds(self, frames: int, fps: float = None) -> float:\n",
        "        \"\"\"Convertit des frames en secondes\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        return frames / fps\n",
        "    \n",
        "    def get_segment_offsets_frames(self):\n",
        "        \"\"\"Calcule les offsets en frames, priorit√© aux secondes si d√©finies\"\"\"\n",
        "        fps = self.get_video_fps()\n",
        "        \n",
        "        # Priorit√© aux offsets en secondes\n",
        "        if hasattr(self, 'SEGMENT_OFFSET_BEFORE_SECONDS') and self.SEGMENT_OFFSET_BEFORE_SECONDS is not None:\n",
        "            offset_before_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_BEFORE_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_BEFORE') and self.SEGMENT_OFFSET_BEFORE is not None:\n",
        "            offset_before_frames = self.SEGMENT_OFFSET_BEFORE\n",
        "        else:\n",
        "            offset_before_frames = self.seconds_to_frames(2.0, fps)  # D√©faut: 2 secondes\n",
        "        \n",
        "        if hasattr(self, 'SEGMENT_OFFSET_AFTER_SECONDS') and self.SEGMENT_OFFSET_AFTER_SECONDS is not None:\n",
        "            offset_after_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_AFTER_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_AFTER') and self.SEGMENT_OFFSET_AFTER is not None:\n",
        "            offset_after_frames = self.SEGMENT_OFFSET_AFTER\n",
        "        else:\n",
        "            offset_after_frames = self.seconds_to_frames(2.0, fps)  # D√©faut: 2 secondes\n",
        "        \n",
        "        return offset_before_frames, offset_after_frames\n",
        "    \n",
        "    def display_config(self):\n",
        "        \"\"\"Affiche la configuration actuelle\"\"\"\n",
        "        print(f\"üìã CONFIGURATION CENTRALIS√âE:\")\n",
        "        print(f\"   üåç Environnement: {'üî¨ Colab' if self.USING_COLAB else 'üñ•Ô∏è Local'}\")\n",
        "        print(f\"   üé¨ Vid√©o: {self.VIDEO_NAME}\")\n",
        "        print(f\"   ‚èØÔ∏è  Intervalle frames: {self.FRAME_INTERVAL}\")\n",
        "        print(f\"   üé¨ Extraction: {'‚úÖ Activ√©e' if self.EXTRACT_FRAMES else '‚ùå D√©sactiv√©e'}\")\n",
        "        print(f\"   üîÑ Force extraction: {'‚úÖ Oui' if self.FORCE_EXTRACTION else '‚ùå Non'}\")\n",
        "        print(f\"   üéØ Segmentation: {'‚úÖ Activ√©e' if getattr(self, 'SEGMENT_MODE', False) else '‚ùå D√©sactiv√©e'}\")\n",
        "        \n",
        "        if getattr(self, 'SEGMENT_MODE', False):\n",
        "            # R√©cup√©rer les informations vid√©o\n",
        "            fps = self.get_video_fps() if self.video_path.exists() else 25.0\n",
        "            offset_before_frames, offset_after_frames = self.get_segment_offsets_frames()\n",
        "            \n",
        "            # Afficher les offsets en secondes et frames\n",
        "            offset_before_seconds = self.frames_to_seconds(offset_before_frames, fps)\n",
        "            offset_after_seconds = self.frames_to_seconds(offset_after_frames, fps)\n",
        "            \n",
        "            print(f\"   üé• FPS vid√©o: {fps:.1f}\")\n",
        "            print(f\"   üïê Offset avant: {offset_before_seconds:.1f}s ({offset_before_frames} frames)\")\n",
        "            print(f\"   üïê Offset apr√®s: {offset_after_seconds:.1f}s ({offset_after_frames} frames)\")\n",
        "            \n",
        "        print(f\"   ü§ñ Mod√®le SAM2: {self.SAM2_MODEL}\")\n",
        "        print(f\"   üìÅ Dossier vid√©os: {self.videos_dir}\")\n",
        "        print(f\"   üìÑ Fichier config: {self.config_path}\")\n",
        "        print(f\"   üìÅ Sortie: {self.output_dir}\")\n",
        "        print(f\"   üíæ Checkpoint: {self.checkpoint_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ INITIALISATION\n",
        "# =============================================================================\n",
        "\n",
        "# Cr√©ation de l'instance de configuration\n",
        "cfg = Config()\n",
        "\n",
        "# Setup automatique des dossiers (toujours safe)\n",
        "cfg.setup_directories()\n",
        "cfg.display_config()\n",
        "\n",
        "# V√©rification des fichiers selon l'environnement\n",
        "if cfg.USING_COLAB:\n",
        "    print(f\"\\nüî¨ Mode Colab d√©tect√©:\")\n",
        "    print(f\"   üí° Les dossiers sont cr√©√©s, vous pouvez maintenant uploader vos fichiers\")\n",
        "    print(f\"   üì§ Uploadez dans: {cfg.videos_dir}\")\n",
        "    print(f\"   üìÑ Fichiers attendus:\")\n",
        "    print(f\"      ‚Ä¢ {cfg.video_path.name}\")\n",
        "    print(f\"      ‚Ä¢ {cfg.config_path.name}\")\n",
        "    \n",
        "    # V√©rification simple sans erreur\n",
        "    cfg.check_files_exist()\n",
        "    print(f\"   üí° Utilisez cfg.wait_for_files() quand les uploads sont termin√©s\")\n",
        "else:\n",
        "    print(f\"\\nüñ•Ô∏è Mode Local d√©tect√©:\")\n",
        "    # Validation imm√©diate en local\n",
        "    cfg.validate_files_now()\n",
        "\n",
        "print(\"\\n‚úÖ Configuration centralis√©e initialis√©e!\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ Configuration de la Segmentation Vid√©o\n",
        "\n",
        "## Mode Segmentation\n",
        "\n",
        "Pour activer le mode segmentation et traiter seulement une partie de votre vid√©o, modifiez la configuration dans la cellule pr√©c√©dente :\n",
        "\n",
        "```python\n",
        "# üéØ SEGMENTATION VID√âO (MODIFIEZ CES VALEURS)\n",
        "SEGMENT_MODE = True                           # ‚úÖ Activez ici !\n",
        "\n",
        "# üïê OFFSETS EN SECONDES (RECOMMAND√â - PLUS SIMPLE)\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 5.0          # 5 secondes AVANT la frame de r√©f√©rence  \n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 10.0          # 10 secondes APR√àS la frame de r√©f√©rence\n",
        "\n",
        "# üé¨ OFFSETS EN FRAMES (OPTIONNEL - CALCUL√â AUTOMATIQUEMENT)\n",
        "SEGMENT_OFFSET_BEFORE = None                 # Calcul√© depuis les secondes\n",
        "SEGMENT_OFFSET_AFTER = None                  # Calcul√© depuis les secondes\n",
        "```\n",
        "\n",
        "## üÜï Nouveaut√© : Offsets en Secondes\n",
        "\n",
        "Le syst√®me **privil√©gie maintenant les offsets en secondes** car c'est plus intuitif et universel !\n",
        "\n",
        "### ‚úÖ **Avec les secondes** (recommand√©)\n",
        "```python\n",
        "# Simple et universel\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 3.0          # 3 secondes avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 5.0           # 5 secondes apr√®s\n",
        "\n",
        "# ‚úÖ Conversion automatique selon le FPS de votre vid√©o :\n",
        "# Vid√©o 25 FPS ‚Üí 3s = 75 frames, 5s = 125 frames\n",
        "# Vid√©o 30 FPS ‚Üí 3s = 90 frames, 5s = 150 frames\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è **Avec les frames** (optionnel)\n",
        "```python\n",
        "# Plus technique, d√©pend du FPS\n",
        "SEGMENT_OFFSET_BEFORE = 75                   # 75 frames avant\n",
        "SEGMENT_OFFSET_AFTER = 125                   # 125 frames apr√®s\n",
        "```\n",
        "\n",
        "### Comment √ßa fonctionne :\n",
        "\n",
        "1. **Frame de r√©f√©rence** : La frame avec votre annotation initiale (d√©finie dans le fichier config JSON)\n",
        "2. **OFFSET_BEFORE_SECONDS** : Dur√©e en secondes √† inclure AVANT la frame de r√©f√©rence\n",
        "3. **OFFSET_AFTER_SECONDS** : Dur√©e en secondes √† inclure APR√àS la frame de r√©f√©rence\n",
        "4. **Conversion automatique** : Le syst√®me d√©tecte le FPS et convertit automatiquement\n",
        "\n",
        "### Exemple concret :\n",
        "- Frame de r√©f√©rence : 1000 (√† 40s dans une vid√©o 25 FPS)\n",
        "- OFFSET_BEFORE_SECONDS : 6.0 ‚Üí 6s √ó 25 FPS = 150 frames ‚Üí Segment commence √† 850 (34s)\n",
        "- OFFSET_AFTER_SECONDS : 12.0 ‚Üí 12s √ó 25 FPS = 300 frames ‚Üí Segment finit √† 1300 (52s)\n",
        "- **R√©sultat** : Segment de 18 secondes (450 frames) au lieu de toute la vid√©o\n",
        "\n",
        "## üéØ Cas d'usage pratiques\n",
        "\n",
        "### üèÉ‚Äç‚ôÇÔ∏è **Actions rapides** (3-6 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 1.5          # 1.5s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 1.5           # 1.5s apr√®s\n",
        "```\n",
        "\n",
        "### ‚öΩ **Actions moyennes** (10-20 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 5.0          # 5s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 5.0           # 5s apr√®s\n",
        "```\n",
        "\n",
        "### üéØ **S√©quences longues** (30-60 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 15.0         # 15s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 15.0          # 15s apr√®s\n",
        "```\n",
        "\n",
        "### üî¨ **Tests et debug** (courts segments)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 2.0          # 2s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 2.0           # 2s apr√®s\n",
        "```\n",
        "\n",
        "### Avantages du mode segmentation :\n",
        "- ‚ö° **Plus rapide** : Traite seulement la partie pertinente de la vid√©o\n",
        "- üíæ **√âconomise l'espace** : Moins de frames extraites et stock√©es  \n",
        "- üéØ **Plus pr√©cis** : Focus sur la zone d'int√©r√™t autour de l'annotation\n",
        "- üïê **Intuitif** : Configuration en secondes plut√¥t qu'en frames techniques\n",
        "\n",
        "---\n",
        "\n",
        "**üí° Astuce :** Modifiez la configuration ci-dessus, puis relancez la cellule de configuration pour voir les nouvelles informations d'offset s'afficher !\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ V√©rification et setup SAM2...\n",
            "   üì¶ SAM2 install√©: ‚úÖ\n",
            "   üíæ Checkpoint disponible: ‚úÖ ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "üñ•Ô∏è Mode local d√©tect√©\n",
            "‚úÖ SAM2 pr√™t en mode local\n",
            "‚úÖ Setup SAM2 termin√©\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üîß INSTALLATION ET SETUP SAM2 (AUTO-INSTALL COLAB)\n",
        "# =============================================================================\n",
        "\n",
        "def is_sam2_installed():\n",
        "    \"\"\"V√©rifie si SAM 2 est d√©j√† install√©\"\"\"\n",
        "    try:\n",
        "        import sam2\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def install_sam2_colab(cfg):\n",
        "    \"\"\"Installation compl√®te de SAM2 sur Colab avec chemins de la config\"\"\"\n",
        "    print(\"üîß Installation de SAM 2 en cours...\")\n",
        "    \n",
        "    # Packages de base\n",
        "    print(\"   üì¶ Installation des d√©pendances...\")\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
        "    \n",
        "    # Cr√©ation du dossier checkpoints (utilise la config)\n",
        "    checkpoint_dir = cfg.checkpoint_path.parent\n",
        "    print(f\"   üìÅ Cr√©ation du dossier checkpoints: {checkpoint_dir}\")\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # T√©l√©chargement du mod√®le configur√©\n",
        "    checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything_2/092824\"\n",
        "    model_file = cfg.SAM2_CHECKPOINT\n",
        "    \n",
        "    print(f\"   ‚¨áÔ∏è T√©l√©chargement du mod√®le: {model_file}\")\n",
        "    !wget -P {checkpoint_dir} -q {checkpoint_url}/{model_file}\n",
        "    \n",
        "    # Optionnel: t√©l√©charger l'autre mod√®le si besoin\n",
        "    if \"large\" in model_file:\n",
        "        other_model = \"sam2.1_hiera_small.pt\"\n",
        "        print(f\"   üì¶ Mod√®le small √©galement disponible: {other_model}\")\n",
        "        # !wget -P {checkpoint_dir} -q {checkpoint_url}/{other_model}\n",
        "    else:\n",
        "        other_model = \"sam2.1_hiera_large.pt\"\n",
        "        print(f\"   üì¶ Mod√®le large √©galement disponible: {other_model}\")\n",
        "        # !wget -P {checkpoint_dir} -q {checkpoint_url}/{other_model}\n",
        "    \n",
        "    # Nettoyage Colab\n",
        "    if cfg.USING_COLAB:\n",
        "        print(\"   üßπ Nettoyage des fichiers temporaires Colab...\")\n",
        "        !rm -rf /content/sample_data/* 2>/dev/null || true\n",
        "    \n",
        "    # Nettoyage m√©moire\n",
        "    import gc\n",
        "    import torch\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"   üî• M√©moire GPU nettoy√©e\")\n",
        "    \n",
        "    print(\"‚úÖ Installation de SAM 2 termin√©e\")\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ AUTO-SETUP SAM2 INTELLIGENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ü§ñ V√©rification et setup SAM2...\")\n",
        "\n",
        "# V√©rifications\n",
        "sam2_installed = is_sam2_installed()\n",
        "checkpoint_available = cfg.checkpoint_path.exists()\n",
        "\n",
        "print(f\"   üì¶ SAM2 install√©: {'‚úÖ' if sam2_installed else '‚ùå'}\")\n",
        "print(f\"   üíæ Checkpoint disponible: {'‚úÖ' if checkpoint_available else '‚ùå'} {cfg.checkpoint_path}\")\n",
        "\n",
        "# Logique d'installation\n",
        "if cfg.USING_COLAB and (not sam2_installed or not checkpoint_available):\n",
        "    print(f\"üî¨ Colab d√©tect√© - Installation automatique...\")\n",
        "    install_sam2_colab(cfg)\n",
        "    \n",
        "elif cfg.USING_COLAB and sam2_installed and checkpoint_available:\n",
        "    print(\"‚úÖ SAM 2 d√©j√† install√© sur Colab - SKIP installation\")\n",
        "    \n",
        "elif not cfg.USING_COLAB:\n",
        "    print(\"üñ•Ô∏è Mode local d√©tect√©\")\n",
        "    if not sam2_installed:\n",
        "        print(\"   ‚ö†Ô∏è SAM2 non install√©. Installez avec: pip install git+https://github.com/facebookresearch/sam2.git\")\n",
        "    if not checkpoint_available:\n",
        "        print(f\"   ‚ö†Ô∏è Checkpoint manquant: {cfg.checkpoint_path}\")\n",
        "        print(\"   üí° T√©l√©chargez depuis: https://github.com/facebookresearch/sam2#download-checkpoints\")\n",
        "    \n",
        "    if sam2_installed and checkpoint_available:\n",
        "        print(\"‚úÖ SAM2 pr√™t en mode local\")\n",
        "\n",
        "print(\"‚úÖ Setup SAM2 termin√©\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k83nNbwoAdf1",
        "outputId": "4f54ec69-dede-40eb-ff63-cac024309b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Configuration de l'environnement PyTorch...\n",
            "   üì¶ PyTorch version: 2.5.1+cu121\n",
            "   üì¶ Torchvision version: 0.20.1+cu121\n",
            "   üî• CUDA disponible: NVIDIA GeForce GTX 1650 with Max-Q Design (4.0GB)\n",
            "   ‚ö° Optimisations activ√©es: Autocast bfloat16, cuDNN benchmark\n",
            "‚úÖ Device ajout√© √† la configuration: cfg.device = cuda\n",
            "\n",
            "‚úÖ Environnement PyTorch configur√© et optimis√©!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üì¶ IMPORTS ET CONFIGURATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "# ==================== IMPORTS SYST√àME ====================\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import uuid\n",
        "import base64\n",
        "\n",
        "# ==================== IMPORTS SCIENTIFIQUES ====================\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==================== IMPORTS DEEP LEARNING ====================\n",
        "import torch\n",
        "import torchvision\n",
        "from pycocotools.mask import encode as encode_rle\n",
        "\n",
        "# =============================================================================\n",
        "# üñ•Ô∏è CONFIGURATION AUTOMATIQUE DU DEVICE ET OPTIMISATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def setup_torch_environment(verbose=True):\n",
        "    \"\"\"Configure automatiquement l'environnement PyTorch optimal\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"üîß Configuration de l'environnement PyTorch...\")\n",
        "        print(f\"   üì¶ PyTorch version: {torch.__version__}\")\n",
        "        print(f\"   üì¶ Torchvision version: {torchvision.__version__}\")\n",
        "    \n",
        "    # === S√âLECTION DU DEVICE ===\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        if verbose:\n",
        "            print(f\"   üî• CUDA disponible: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        if verbose:\n",
        "            print(f\"   üçé MPS (Apple Silicon) disponible\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        if verbose:\n",
        "            print(f\"   üíª CPU seulement\")\n",
        "    \n",
        "    # === OPTIMISATIONS CUDA ===\n",
        "    optimizations_applied = []\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        # Autocast pour √©conomiser la m√©moire\n",
        "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "        optimizations_applied.append(\"Autocast bfloat16\")\n",
        "        \n",
        "        # Optimisations TensorFloat-32 (si GPU r√©cent)\n",
        "        gpu_compute_capability = torch.cuda.get_device_properties(0).major\n",
        "        if gpu_compute_capability >= 8:  # Ampere et plus r√©cent\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "            optimizations_applied.append(\"TensorFloat-32\")\n",
        "        \n",
        "        # Optimisations m√©moire additionnelles\n",
        "        torch.backends.cudnn.benchmark = True  # Optimise pour tailles fixes\n",
        "        optimizations_applied.append(\"cuDNN benchmark\")\n",
        "    \n",
        "    if verbose and optimizations_applied:\n",
        "        print(f\"   ‚ö° Optimisations activ√©es: {', '.join(optimizations_applied)}\")\n",
        "    \n",
        "    return device, optimizations_applied\n",
        "\n",
        "def display_system_info(device):\n",
        "    \"\"\"Affiche les informations syst√®me d√©taill√©es\"\"\"\n",
        "    print(f\"\\nüìä INFORMATIONS SYST√àME:\")\n",
        "    print(f\"   üñ•Ô∏è  Device principal: {device}\")\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        print(f\"   üî• GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "        print(f\"   üßÆ Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
        "    \n",
        "    # CPU Info\n",
        "    import platform\n",
        "    print(f\"   üíª CPU: {platform.processor()}\")\n",
        "    print(f\"   üß† Threads: {torch.get_num_threads()}\")\n",
        "    \n",
        "    # Versions importantes\n",
        "    print(f\"   üêç Python: {platform.python_version()}\")\n",
        "    print(f\"   üì¶ NumPy: {np.__version__}\")\n",
        "    print(f\"   üé• OpenCV: {cv2.__version__}\")\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ INITIALISATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration automatique\n",
        "device, optimizations = setup_torch_environment(verbose=True)\n",
        "\n",
        "# Ajout du device √† notre configuration centralis√©e\n",
        "if 'cfg' in globals():\n",
        "    cfg.device = device\n",
        "    cfg.torch_optimizations = optimizations\n",
        "    print(f\"‚úÖ Device ajout√© √† la configuration: cfg.device = {device}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Configuration cfg non trouv√©e - device disponible en tant que variable globale\")\n",
        "\n",
        "# Affichage des informations syst√®me (optionnel, d√©commentez si besoin)\n",
        "# display_system_info(device)\n",
        "\n",
        "# Nettoyage initial de la m√©moire\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n‚úÖ Environnement PyTorch configur√© et optimis√©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è Mode local: chargement imm√©diat\n",
            "üìÑ Chargement de la configuration projet: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1_config.json\n",
            "‚úÖ Configuration projet charg√©e et valid√©e:\n",
            "   üì∑ Calibration cam√©ra: ‚úÖ OK\n",
            "   üéØ Objets d√©finis: 13\n",
            "   üìç Annotations initiales: 13 sur 1 frames\n",
            "   üè∑Ô∏è  Types d'objets:\n",
            "      ‚Ä¢ player: 12 ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
            "      ‚Ä¢ ball: 1 ([13])\n",
            "\n",
            "‚úÖ Configuration projet pr√™te √† l'usage!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üìÑ CHARGEMENT ET VALIDATION DE LA CONFIGURATION JSON\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_validate_project_config(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Charge et valide le fichier de configuration JSON du projet.\"\"\"\n",
        "    print(f\"üìÑ Chargement de la configuration projet: {config_path}\")\n",
        "\n",
        "    # V√©rification existence du fichier\n",
        "    if not config_path.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Fichier config non trouv√©: {config_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            config = json.load(f)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"‚ùå Erreur JSON dans {config_path}: {e}\")\n",
        "\n",
        "    # === VALIDATION DE LA STRUCTURE ===\n",
        "    required_sections = ['calibration', 'objects', 'initial_annotations']\n",
        "    missing_sections = [section for section in required_sections if section not in config]\n",
        "    \n",
        "    if missing_sections:\n",
        "        raise ValueError(f\"‚ùå Sections manquantes dans le config: {missing_sections}\")\n",
        "\n",
        "    # === VALIDATION DES DONN√âES ===\n",
        "    \n",
        "    # Validation calibration\n",
        "    if 'camera_parameters' not in config['calibration']:\n",
        "        raise ValueError(\"‚ùå 'camera_parameters' manquant dans la calibration\")\n",
        "    \n",
        "    # Validation objets\n",
        "    if not config['objects']:\n",
        "        raise ValueError(\"‚ùå Aucun objet d√©fini dans la configuration\")\n",
        "    \n",
        "    # Validation annotations initiales\n",
        "    if not config['initial_annotations']:\n",
        "        raise ValueError(\"‚ùå Aucune annotation initiale d√©finie\")\n",
        "\n",
        "    # === STATISTIQUES ET R√âSUM√â ===\n",
        "    num_objects = len(config['objects'])\n",
        "    \n",
        "    # Comptage des annotations\n",
        "    total_annotations = 0\n",
        "    annotation_frames = set()\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        frame_idx = frame_data.get('frame')\n",
        "        annotations = frame_data.get('annotations', [])\n",
        "        total_annotations += len(annotations)\n",
        "        annotation_frames.add(frame_idx)\n",
        "\n",
        "    # Types d'objets\n",
        "    obj_types = {}\n",
        "    obj_by_type = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_type = obj.get('obj_type', 'unknown')\n",
        "        obj_types[obj_type] = obj_types.get(obj_type, 0) + 1\n",
        "        if obj_type not in obj_by_type:\n",
        "            obj_by_type[obj_type] = []\n",
        "        obj_by_type[obj_type].append(obj.get('obj_id', 'no_id'))\n",
        "\n",
        "    print(f\"‚úÖ Configuration projet charg√©e et valid√©e:\")\n",
        "    print(f\"   üì∑ Calibration cam√©ra: ‚úÖ OK\")\n",
        "    print(f\"   üéØ Objets d√©finis: {num_objects}\")\n",
        "    print(f\"   üìç Annotations initiales: {total_annotations} sur {len(annotation_frames)} frames\")\n",
        "    print(f\"   üè∑Ô∏è  Types d'objets:\")\n",
        "    for obj_type, count in obj_types.items():\n",
        "        ids = obj_by_type[obj_type]\n",
        "        print(f\"      ‚Ä¢ {obj_type}: {count} ({ids})\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ CHARGEMENT DE LA CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Chargement avec gestion intelligente Colab/Local\n",
        "if cfg.USING_COLAB:\n",
        "    print(\"üî¨ Mode Colab d√©tect√© pour le chargement config\")\n",
        "    if cfg.config_path.exists():\n",
        "        project_config = load_and_validate_project_config(cfg.config_path)\n",
        "    else:\n",
        "        print(f\"‚è≥ Fichier config pas encore upload√©, utilisez:\")\n",
        "        print(f\"   project_config = wait_and_load_config(cfg)\")\n",
        "        project_config = None\n",
        "else:\n",
        "    print(\"üñ•Ô∏è Mode local: chargement imm√©diat\")\n",
        "    project_config = load_and_validate_project_config(cfg.config_path)\n",
        "\n",
        "if project_config:\n",
        "    print(\"\\n‚úÖ Configuration projet pr√™te √† l'usage!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fonctions de segmentation vid√©o charg√©es!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üéØ FONCTIONS DE SEGMENTATION VID√âO\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_segment_bounds(reference_frame: int, offset_before: int, offset_after: int,\n",
        "                           total_frames: int, frame_interval: int = 1) -> tuple:\n",
        "    \"\"\"\n",
        "    Calcule les bornes du segment vid√©o √† traiter.\n",
        "    \n",
        "    Args:\n",
        "        reference_frame: Frame de r√©f√©rence (celle avec l'annotation initiale)\n",
        "        offset_before: Nombre de frames √† prendre AVANT la frame de r√©f√©rence\n",
        "        offset_after: Nombre de frames √† prendre APR√àS la frame de r√©f√©rence\n",
        "        total_frames: Nombre total de frames dans la vid√©o\n",
        "        frame_interval: Intervalle entre les frames trait√©es\n",
        "    \n",
        "    Returns:\n",
        "        (start_frame, end_frame, processed_start_idx, processed_end_idx)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calcul des bornes en frames originales\n",
        "    start_frame = max(0, reference_frame - offset_before)\n",
        "    end_frame = min(total_frames - 1, reference_frame + offset_after)\n",
        "    \n",
        "    # Conversion en indices de frames trait√©es\n",
        "    processed_start_idx = start_frame // frame_interval\n",
        "    processed_end_idx = end_frame // frame_interval\n",
        "    \n",
        "    print(f\"üéØ CALCUL DES BORNES DE SEGMENTATION:\")\n",
        "    print(f\"   üìç Frame de r√©f√©rence: {reference_frame}\")\n",
        "    print(f\"   üìâ Offset avant: {offset_before} frames\")\n",
        "    print(f\"   üìà Offset apr√®s: {offset_after} frames\")\n",
        "    print(f\"   üé¨ Segment original: frames {start_frame} √† {end_frame}\")\n",
        "    print(f\"   üé¨ Segment trait√©: indices {processed_start_idx} √† {processed_end_idx}\")\n",
        "    print(f\"   üìä Nombre de frames √† traiter: {processed_end_idx - processed_start_idx + 1}\")\n",
        "    \n",
        "    return start_frame, end_frame, processed_start_idx, processed_end_idx\n",
        "\n",
        "def extract_segment_frames(video_path, frames_dir, start_frame: int, end_frame: int,\n",
        "                         frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"\n",
        "    Extrait seulement les frames du segment sp√©cifi√© avec nommage s√©quentiel.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Chemin vers la vid√©o\n",
        "        frames_dir: Dossier de destination\n",
        "        start_frame: Frame de d√©but\n",
        "        end_frame: Frame de fin\n",
        "        frame_interval: Intervalle entre frames\n",
        "        force_extraction: Force la r√©-extraction\n",
        "    \n",
        "    Returns:\n",
        "        Nombre de frames extraites\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üé¨ EXTRACTION DU SEGMENT:\")\n",
        "    print(f\"   üìπ Vid√©o: {video_path}\")\n",
        "    print(f\"   üìÅ Destination: {frames_dir}\")\n",
        "    print(f\"   üéØ Segment: frames {start_frame} √† {end_frame}\")\n",
        "    print(f\"   ‚èØÔ∏è  Intervalle: {frame_interval}\")    \n",
        "    \n",
        "    # ‚úÖ CORRECTION: Calculer les frames attendues avec nommage s√©quentiel\n",
        "    expected_frames = []\n",
        "    sequential_idx = 0  # ‚Üê Compteur s√©quentiel pour le nommage\n",
        "    \n",
        "    for frame_idx in range(start_frame, end_frame + 1, frame_interval):\n",
        "        expected_frames.append((frame_idx, sequential_idx))  # (frame_originale, index_s√©quentiel)\n",
        "        sequential_idx += 1\n",
        "    \n",
        "    # V√©rifier si extraction d√©j√† faite avec le nouveau nommage\n",
        "    existing_frames = []\n",
        "    for frame_idx, seq_idx in expected_frames:\n",
        "        filename = frames_dir / f\"{seq_idx:05d}.jpg\"  # ‚Üê Utiliser l'index s√©quentiel\n",
        "        if filename.exists():\n",
        "            existing_frames.append(filename)\n",
        "    \n",
        "    if len(existing_frames) == len(expected_frames) and not force_extraction:\n",
        "        print(f\"üìÇ {len(existing_frames)} frames du segment d√©j√† extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"üîÑ {len(existing_frames)} frames existantes - SUPPRESSION et r√©-extraction...\")\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"üóëÔ∏è  Frames existantes supprim√©es\")\n",
        "    \n",
        "    # Extraction\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
        "    \n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    \n",
        "    print(f\"üìä Vid√©o: {total_frames} frames total, {fps:.1f} FPS\")\n",
        "    print(f\"üìä Nommage: s√©quentiel de 00000.jpg √† {len(expected_frames)-1:05d}.jpg\")\n",
        "    \n",
        "    extracted_count = 0\n",
        "    \n",
        "    try:\n",
        "        # Positionner √† la frame de d√©but\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        \n",
        "        # ‚úÖ CORRECTION: Utiliser le mapping (frame_originale, index_s√©quentiel)\n",
        "        for frame_idx in range(start_frame, end_frame + 1):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            # Extraire selon l'intervalle\n",
        "            if (frame_idx - start_frame) % frame_interval == 0:\n",
        "                # Trouver l'index s√©quentiel pour cette frame\n",
        "                sequential_idx = (frame_idx - start_frame) // frame_interval\n",
        "                filename = frames_dir / f\"{sequential_idx:05d}.jpg\"  # ‚Üê Nommage s√©quentiel\n",
        "                \n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "                \n",
        "                if extracted_count % 10 == 0:\n",
        "                    progress = ((frame_idx - start_frame) / (end_frame - start_frame)) * 100\n",
        "                    print(f\"üìä Progr√®s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "    \n",
        "    finally:\n",
        "        cap.release()\n",
        "    \n",
        "    print(f\"‚úÖ {extracted_count} frames du segment extraites\")\n",
        "    print(f\"üìã Nommage: 00000.jpg √† {extracted_count-1:05d}.jpg\")\n",
        "    return extracted_count\n",
        "\n",
        "print(\"‚úÖ Fonctions de segmentation vid√©o charg√©es!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé¨ D√©marrage de l'extraction des frames...\n",
            "üñ•Ô∏è Mode Local: Extraction directe\n",
            "üéØ MODE SEGMENTATION ACTIV√â\n",
            "üéØ CALCUL DES BORNES DE SEGMENTATION:\n",
            "   üìç Frame de r√©f√©rence: 280\n",
            "   üìâ Offset avant: 50 frames\n",
            "   üìà Offset apr√®s: 50 frames\n",
            "   üé¨ Segment original: frames 230 √† 330\n",
            "   üé¨ Segment trait√©: indices 76 √† 110\n",
            "   üìä Nombre de frames √† traiter: 35\n",
            "üé¨ EXTRACTION DU SEGMENT:\n",
            "   üìπ Vid√©o: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1.mp4\n",
            "   üìÅ Destination: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   üéØ Segment: frames 230 √† 330\n",
            "   ‚èØÔ∏è  Intervalle: 3\n",
            "üîÑ 34 frames existantes - SUPPRESSION et r√©-extraction...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóëÔ∏è  Frames existantes supprim√©es\n",
            "üìä Vid√©o: 624 frames total, 25.0 FPS\n",
            "üìä Nommage: s√©quentiel de 00000.jpg √† 00033.jpg\n",
            "üìä Progr√®s: 10 frames extraites (27.0%)\n",
            "üìä Progr√®s: 20 frames extraites (57.0%)\n",
            "üìä Progr√®s: 30 frames extraites (87.0%)\n",
            "‚úÖ 34 frames du segment extraites\n",
            "üìã Nommage: 00000.jpg √† 00033.jpg\n",
            "\n",
            "üéØ SEGMENTATION TERMIN√âE:\n",
            "   üìä Frames du segment: 101\n",
            "   üé¨ Plage: 230 √† 330\n",
            "   üìç Frame de r√©f√©rence: 280\n",
            "   üìä Frames extraites: 34\n",
            "\n",
            "üìä R√âSUM√â EXTRACTION:\n",
            "   üé¨ Frames extraites/compt√©es: 34\n",
            "   üìÅ Dossier: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   ‚úÖ cfg.extracted_frames_count = 34\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üé¨ EXTRACTION DES FRAMES\n",
        "# =============================================================================\n",
        "\n",
        "def extract_frames(video_path: Path, frames_dir: Path, frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"Extrait les frames de la vid√©o selon l'intervalle sp√©cifi√©.\"\"\"\n",
        "\n",
        "    print(f\"üé¨ Extraction des frames...\")\n",
        "    print(f\"   üìπ Source: {video_path}\")\n",
        "    print(f\"   üìÅ Destination: {frames_dir}\")\n",
        "    print(f\"   ‚èØÔ∏è  Intervalle: {frame_interval}\")\n",
        "    print(f\"   üîÑ Force extraction: {'‚úÖ Oui' if force_extraction else '‚ùå Non'}\")\n",
        "\n",
        "    # V√©rification existence du fichier vid√©o\n",
        "    if not video_path.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Vid√©o non trouv√©e: {video_path}\")\n",
        "\n",
        "    # V√©rification si extraction d√©j√† faite\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    if existing_frames and not force_extraction:\n",
        "        print(f\"üìÇ {len(existing_frames)} frames d√©j√† extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"üîÑ {len(existing_frames)} frames existantes - SUPPRESSION et r√©-extraction...\")\n",
        "        # Supprimer les frames existantes\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"üóëÔ∏è  Frames existantes supprim√©es\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"üìä Vid√©o: {total_frames} frames, {fps:.1f} FPS\")\n",
        "    print(f\"üìä Frames √† extraire: ~{total_frames // frame_interval}\")\n",
        "\n",
        "    extracted_count = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extraire seulement selon l'intervalle\n",
        "            if frame_idx % frame_interval == 0:\n",
        "                output_idx = frame_idx // frame_interval\n",
        "                filename = frames_dir / f\"{output_idx:05d}.jpg\"\n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "\n",
        "                if extracted_count % 50 == 0:\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    print(f\"üìä Progr√®s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"‚úÖ {extracted_count} frames extraites\")\n",
        "    return extracted_count\n",
        "\n",
        "def count_existing_frames(frames_dir: Path) -> int:\n",
        "    \"\"\"Compte les frames existantes dans le dossier\"\"\"\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    return len(existing_frames)\n",
        "\n",
        "def extract_frames_with_config(cfg) -> int:\n",
        "    \"\"\"Extrait les frames en utilisant la configuration centralis√©e avec support segmentation\"\"\"\n",
        "    \n",
        "    # V√©rification des fichiers (gestion Colab/Local)\n",
        "    if cfg.USING_COLAB and not cfg.video_path.exists():\n",
        "        print(\"üî¨ Mode Colab: Vid√©o pas encore upload√©e\")\n",
        "        print(f\"   üí° Uploadez {cfg.video_path.name} dans {cfg.videos_dir}\")\n",
        "        print(f\"   üí° Puis relancez cette cellule ou utilisez cfg.wait_for_files()\")\n",
        "        return 0\n",
        "    \n",
        "    # V√©rification si extraction activ√©e\n",
        "    if not cfg.EXTRACT_FRAMES:\n",
        "        existing_count = count_existing_frames(cfg.frames_dir)\n",
        "        print(f\"‚è≠Ô∏è  Extraction d√©sactiv√©e (cfg.EXTRACT_FRAMES = False)\")\n",
        "        if existing_count > 0:\n",
        "            print(f\"üìÇ Utilisation de {existing_count} frames existantes\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Aucune frame trouv√©e dans {cfg.frames_dir}\")\n",
        "            print(f\"üí° Conseil: Activez cfg.EXTRACT_FRAMES = True pour extraire\")\n",
        "        return existing_count\n",
        "    \n",
        "    # D√©cider du mode d'extraction : segmentation ou complet\n",
        "    if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "        print(\"üéØ MODE SEGMENTATION ACTIV√â\")\n",
        "        \n",
        "        # V√©rifier si project_config est disponible\n",
        "        if 'project_config' not in globals() or project_config is None:\n",
        "            print(\"‚ùå project_config non disponible pour la segmentation\")\n",
        "            print(\"üí° Chargez d'abord la configuration projet ou utilisez le mode complet\")\n",
        "            print(\"üîÑ Utilisation du mode complet par d√©faut...\")\n",
        "            return extract_frames(\n",
        "                video_path=cfg.video_path,\n",
        "                frames_dir=cfg.frames_dir,\n",
        "                frame_interval=cfg.FRAME_INTERVAL,\n",
        "                force_extraction=cfg.FORCE_EXTRACTION\n",
        "            )\n",
        "        \n",
        "        # R√©cup√©rer la frame de r√©f√©rence\n",
        "        reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Obtenir les informations vid√©o\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        \n",
        "        # ‚úÖ AM√âLIORATION: Utiliser les offsets calcul√©s depuis les secondes\n",
        "        offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "        \n",
        "        # Calculer les bornes du segment\n",
        "        start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "            reference_frame=reference_frame,\n",
        "            offset_before=offset_before_frames,\n",
        "            offset_after=offset_after_frames,\n",
        "            total_frames=total_frames,\n",
        "            frame_interval=cfg.FRAME_INTERVAL\n",
        "        )\n",
        "        \n",
        "        # Extraire les frames du segment\n",
        "        extracted_count = extract_segment_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            start_frame=start_frame,\n",
        "            end_frame=end_frame,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nüéØ SEGMENTATION TERMIN√âE:\")\n",
        "        print(f\"   üìä Frames du segment: {end_frame - start_frame + 1}\")\n",
        "        print(f\"   üé¨ Plage: {start_frame} √† {end_frame}\")\n",
        "        print(f\"   üìç Frame de r√©f√©rence: {reference_frame}\")\n",
        "        print(f\"   üìä Frames extraites: {extracted_count}\")\n",
        "        \n",
        "        return extracted_count\n",
        "        \n",
        "    else:\n",
        "        print(\"üé¨ MODE COMPLET ACTIV√â (toute la vid√©o)\")\n",
        "        return extract_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ EXTRACTION AVEC CONFIGURATION CENTRALIS√âE\n",
        "# =============================================================================\n",
        "\n",
        "# Extraction intelligente selon l'environnement\n",
        "print(\"üé¨ D√©marrage de l'extraction des frames...\")\n",
        "\n",
        "# V√©rification pr√©requis selon l'environnement\n",
        "if cfg.USING_COLAB:\n",
        "    print(\"üî¨ Mode Colab: V√©rification des fichiers upload√©s...\")\n",
        "    if not cfg.video_path.exists():\n",
        "        print(f\"‚ö†Ô∏è  Vid√©o pas encore upload√©e: {cfg.video_path.name}\")\n",
        "        print(f\"   üì§ Uploadez dans: {cfg.videos_dir}\")\n",
        "        print(f\"   üîÑ Puis relancez cette cellule\")\n",
        "        extracted_frames_count = 0\n",
        "    else:\n",
        "        extracted_frames_count = extract_frames_with_config(cfg)\n",
        "else:\n",
        "    print(\"üñ•Ô∏è Mode Local: Extraction directe\")\n",
        "    extracted_frames_count = extract_frames_with_config(cfg)\n",
        "\n",
        "# Ajout du r√©sultat √† la configuration pour usage ult√©rieur\n",
        "cfg.extracted_frames_count = extracted_frames_count\n",
        "\n",
        "print(f\"\\nüìä R√âSUM√â EXTRACTION:\")\n",
        "print(f\"   üé¨ Frames extraites/compt√©es: {extracted_frames_count}\")\n",
        "print(f\"   üìÅ Dossier: {cfg.frames_dir}\")\n",
        "print(f\"   ‚úÖ cfg.extracted_frames_count = {extracted_frames_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Initialisation SAM2...\n",
            "   üß† Mod√®le: configs/sam2.1/sam2.1_hiera_l.yaml\n",
            "   üíæ Checkpoint: ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "   üñ•Ô∏è  Device: cuda\n",
            "\n",
            "üé¨ Initialisation √©tat d'inf√©rence...\n",
            "   üìÅ Frames: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:04<00:00,  8.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ SAM2 initialis√©:\n",
            "   üñºÔ∏è  Frames extraites: 34\n",
            "   üé¨ Frames charg√©es: 34\n",
            "   ‚úÖ Correspondance: OK\n",
            "   üíæ GPU Memory: 1.45GB\n",
            "\n",
            "‚úÖ SAM2 pr√™t pour l'ajout d'annotations!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ü§ñ INITIALISATION SAM2\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_sam2_predictor(cfg, verbose=True):\n",
        "    \"\"\"Initialise le predictor SAM2 avec la configuration centralis√©e\"\"\"\n",
        "    \n",
        "    # Import SAM2\n",
        "    from sam2.build_sam import build_sam2_video_predictor\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"ü§ñ Initialisation SAM2...\")\n",
        "        print(f\"   üß† Mod√®le: {cfg.model_config_path}\")\n",
        "        print(f\"   üíæ Checkpoint: {cfg.checkpoint_path}\")\n",
        "        print(f\"   üñ•Ô∏è  Device: {cfg.device}\")\n",
        "    \n",
        "    # V√©rification du checkpoint\n",
        "    if not cfg.checkpoint_path.exists():\n",
        "        raise FileNotFoundError(f\"‚ùå Checkpoint SAM2 non trouv√©: {cfg.checkpoint_path}\")\n",
        "    \n",
        "    # Construction du predictor\n",
        "    predictor = build_sam2_video_predictor(\n",
        "        config_file=cfg.model_config_path,\n",
        "        ckpt_path=str(cfg.checkpoint_path),\n",
        "        device=cfg.device\n",
        "    )\n",
        "    \n",
        "    return predictor\n",
        "\n",
        "def initialize_inference_state(predictor, cfg, verbose=True):\n",
        "    \"\"\"Initialise l'√©tat d'inf√©rence avec v√©rifications\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nüé¨ Initialisation √©tat d'inf√©rence...\")\n",
        "        print(f\"   üìÅ Frames: {cfg.frames_dir}\")\n",
        "    \n",
        "    # V√©rification des frames\n",
        "    if cfg.extracted_frames_count == 0:\n",
        "        raise ValueError(f\"‚ùå Aucune frame extraite. Extrayez d'abord les frames.\")\n",
        "    \n",
        "    # Initialisation de l'√©tat d'inf√©rence\n",
        "    inference_state = predictor.init_state(\n",
        "        video_path=str(cfg.frames_dir),\n",
        "        offload_video_to_cpu=True,    # √âconomise la m√©moire GPU\n",
        "        offload_state_to_cpu=False    # Garde l'√©tat en GPU\n",
        "    )\n",
        "    \n",
        "    # Reset de l'√©tat\n",
        "    predictor.reset_state(inference_state)\n",
        "    \n",
        "    # V√©rification\n",
        "    loaded_frames = inference_state[\"num_frames\"]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n‚úÖ SAM2 initialis√©:\")\n",
        "        print(f\"   üñºÔ∏è  Frames extraites: {cfg.extracted_frames_count}\")\n",
        "        print(f\"   üé¨ Frames charg√©es: {loaded_frames}\")\n",
        "        print(f\"   ‚úÖ Correspondance: {'OK' if cfg.extracted_frames_count == loaded_frames else 'ERREUR'}\")\n",
        "        \n",
        "        if cfg.device.type == \"cuda\":\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            print(f\"   üíæ GPU Memory: {allocated:.2f}GB\")\n",
        "    \n",
        "    if cfg.extracted_frames_count != loaded_frames:\n",
        "        print(f\"‚ö†Ô∏è Incoh√©rence frames : {cfg.extracted_frames_count} extraites vs {loaded_frames} charg√©es\")\n",
        "    \n",
        "    return inference_state\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ INITIALISATION AVEC CONFIGURATION CENTRALIS√âE\n",
        "# =============================================================================\n",
        "\n",
        "# V√©rification pr√©requis\n",
        "if not hasattr(cfg, 'extracted_frames_count') or cfg.extracted_frames_count == 0:\n",
        "    print(\"‚ùå Frames non extraites. Ex√©cutez d'abord la cellule d'extraction des frames.\")\n",
        "    print(\"üí° Ou d√©finissez cfg.extracted_frames_count manuellement si frames d√©j√† pr√©sentes\")\n",
        "else:\n",
        "    # Initialisation SAM2\n",
        "    predictor = initialize_sam2_predictor(cfg)\n",
        "    inference_state = initialize_inference_state(predictor, cfg)\n",
        "    \n",
        "    # Ajout √† la configuration pour usage ult√©rieur\n",
        "    cfg.predictor = predictor\n",
        "    cfg.inference_state = inference_state\n",
        "    \n",
        "    print(\"\\n‚úÖ SAM2 pr√™t pour l'ajout d'annotations!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for -: 'int' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Calculer les bornes du segment\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m start_frame, end_frame, processed_start_idx, processed_end_idx \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_segment_bounds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEGMENT_OFFSET_BEFORE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEGMENT_OFFSET_AFTER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFRAME_INTERVAL\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m segment_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m: start_frame,\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_frame\u001b[39m\u001b[38;5;124m'\u001b[39m: end_frame,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_start_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: processed_start_idx,\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_end_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: processed_end_idx\n\u001b[0;32m    161\u001b[0m }\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéØ Informations segmentation pr√©par√©es pour l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124majout d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mcalculate_segment_bounds\u001b[1;34m(reference_frame, offset_before, offset_after, total_frames, frame_interval)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mCalcule les bornes du segment vid√©o √† traiter.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    (start_frame, end_frame, processed_start_idx, processed_end_idx)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calcul des bornes en frames originales\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m start_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[43mreference_frame\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moffset_before\u001b[49m)\n\u001b[0;32m     23\u001b[0m end_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(total_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, reference_frame \u001b[38;5;241m+\u001b[39m offset_after)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Conversion en indices de frames trait√©es\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üéØ AJOUT DES ANNOTATIONS INITIALES\n",
        "# =============================================================================\n",
        "def add_initial_annotations(predictor, inference_state, project_config: Dict[str, Any], frame_interval: int = 1, segment_info: Dict = None):\n",
        "    \"\"\"Ajoute les annotations initiales depuis la configuration projet avec conversion des frames et support segmentation.\"\"\"\n",
        "\n",
        "    print(f\"üéØ Ajout des annotations initiales...\")\n",
        "\n",
        "    # Cr√©ation du mapping obj_id -> obj_type\n",
        "    obj_types = {}\n",
        "    for obj in project_config['objects']:\n",
        "        obj_types[obj['obj_id']] = obj['obj_type']\n",
        "\n",
        "    # Extraction automatique des annotations et frames depuis le JSON\n",
        "    all_annotations = []\n",
        "    annotation_frames = []\n",
        "\n",
        "    for frame_data in project_config['initial_annotations']:\n",
        "        frame_idx_original = frame_data['frame']  # ‚Üê Frame originale du JSON\n",
        "        \n",
        "        # ‚úÖ CORRECTION: Conversion avec prise en compte de la segmentation\n",
        "        frame_idx_processed = frame_idx_original // frame_interval\n",
        "        \n",
        "        # üéØ NOUVEAU: Ajustement pour le mode segmentation\n",
        "        if segment_info:\n",
        "            # Calculer l'offset par rapport au d√©but du segment\n",
        "            segment_start_processed = segment_info['start_frame'] // frame_interval\n",
        "            frame_idx_segment = frame_idx_processed - segment_start_processed\n",
        "            \n",
        "            print(f\"   üéØ MODE SEGMENTATION:\")\n",
        "            print(f\"      üìç Frame originale: {frame_idx_original}\")\n",
        "            print(f\"      üìç Frame trait√©e: {frame_idx_processed}\")\n",
        "            print(f\"      üìç Segment commence √†: {segment_start_processed} (trait√©)\")\n",
        "            print(f\"      üìç Index dans le segment: {frame_idx_segment}\")\n",
        "            \n",
        "            # V√©rifier que la frame est dans le segment\n",
        "            segment_end_processed = segment_info['end_frame'] // frame_interval\n",
        "            if frame_idx_processed < segment_start_processed or frame_idx_processed > segment_end_processed:\n",
        "                raise ValueError(f\"‚ùå Frame d'annotation {frame_idx_processed} en dehors du segment [{segment_start_processed}, {segment_end_processed}]\")\n",
        "            \n",
        "            # Utiliser l'index dans le segment\n",
        "            frame_idx_for_sam = frame_idx_segment\n",
        "        else:\n",
        "            frame_idx_for_sam = frame_idx_processed\n",
        "        \n",
        "        annotations = frame_data['annotations']\n",
        "        annotation_frames.append(frame_idx_for_sam)\n",
        "\n",
        "        print(f\"   üìç Frame {frame_idx_original} (original) ‚Üí {frame_idx_for_sam} (pour SAM2): {len(annotations)} annotations\")\n",
        "\n",
        "        for annotation in annotations:\n",
        "            all_annotations.append({\n",
        "                'frame_original': frame_idx_original,\n",
        "                'frame_processed': frame_idx_processed,\n",
        "                'frame_for_sam': frame_idx_for_sam,  # ‚Üê Nouvel index pour SAM2\n",
        "                'obj_id': annotation['obj_id'],\n",
        "                'points': annotation['points'],\n",
        "                'obj_type': obj_types.get(annotation['obj_id'], f'unknown_{annotation[\"obj_id\"]}')\n",
        "            })\n",
        "\n",
        "    if not all_annotations:\n",
        "        raise ValueError(f\"‚ùå Aucune annotation trouv√©e dans le fichier config\")\n",
        "\n",
        "    print(f\"   üìä Total: {len(all_annotations)} annotations sur {len(set(annotation_frames))} frames\")\n",
        "\n",
        "    # Ajout des annotations √† SAM2 avec les frames converties\n",
        "    added_objects = []\n",
        "\n",
        "    for annotation_data in all_annotations:\n",
        "        frame_idx_for_sam = annotation_data['frame_for_sam']  # ‚Üê Utiliser l'index ajust√©\n",
        "        frame_idx_original = annotation_data['frame_original']\n",
        "        obj_id = annotation_data['obj_id']\n",
        "        obj_type = annotation_data['obj_type']\n",
        "        points_data = annotation_data['points']\n",
        "\n",
        "        # Extraction des coordonn√©es et labels\n",
        "        points = np.array([[p['x'], p['y']] for p in points_data], dtype=np.float32)\n",
        "        labels = np.array([p['label'] for p in points_data], dtype=np.int32)\n",
        "\n",
        "        print(f\"   üéØ Frame {frame_idx_original}‚Üí{frame_idx_for_sam} - Objet {obj_id} ({obj_type}): {len(points)} points √† ({points[0][0]:.0f}, {points[0][1]:.0f})\")\n",
        "\n",
        "        # ‚úÖ CORRECTION: Utiliser frame_idx_for_sam (ajust√© pour le segment)\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state,\n",
        "            frame_idx_for_sam,  # ‚Üê Frame ajust√©e pour SAM2\n",
        "            obj_id,\n",
        "            points,\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # √âviter les doublons dans added_objects\n",
        "        if not any(obj['obj_id'] == obj_id for obj in added_objects):\n",
        "            added_objects.append({\n",
        "                'obj_id': obj_id,\n",
        "                'obj_type': obj_type,\n",
        "                'points_count': len(points)\n",
        "            })\n",
        "\n",
        "    # V√©rification\n",
        "    sam_obj_ids = inference_state[\"obj_ids\"]\n",
        "\n",
        "    print(f\"\\nüìä R√âSUM√â ANNOTATIONS:\")\n",
        "    print(f\"   üéØ Annotations configur√©es: {len(all_annotations)}\")\n",
        "    print(f\"   üéØ Objets uniques: {len(added_objects)}\")\n",
        "    print(f\"   ‚úÖ Objets ajout√©s √† SAM2: {len(sam_obj_ids)}\")\n",
        "    print(f\"   üÜî IDs: {sorted(sam_obj_ids)}\")\n",
        "    \n",
        "    if segment_info:\n",
        "        print(f\"   üéØ Mode segmentation: Frame d'annotation ajust√©e √† l'index {annotation_data['frame_for_sam']} dans le segment\")\n",
        "    else:\n",
        "        print(f\"   üé¨ Mode complet: Frames utilis√©es (original‚Üítrait√©e): {[(a['frame_original'], a['frame_processed']) for a in all_annotations[:3]]}...\")\n",
        "\n",
        "    # R√©sum√© par type\n",
        "    type_counts = {}\n",
        "    for obj in added_objects:\n",
        "        obj_type = obj['obj_type']\n",
        "        type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "    print(f\"   üè∑Ô∏è  Types: {dict(type_counts)}\")\n",
        "\n",
        "    return added_objects, all_annotations\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ AJOUT AVEC CONFIGURATION CENTRALIS√âE CORRIG√âE\n",
        "# =============================================================================\n",
        "\n",
        "# V√©rifications pr√©requis\n",
        "if not hasattr(cfg, 'predictor') or not hasattr(cfg, 'inference_state'):\n",
        "    print(\"‚ùå SAM2 non initialis√©. Ex√©cutez d'abord la cellule d'initialisation SAM2.\")\n",
        "elif project_config is None:\n",
        "    print(\"‚ùå Configuration projet non charg√©e.\")\n",
        "    if cfg.USING_COLAB:\n",
        "        print(\"üí° Sur Colab, utilisez: project_config = wait_and_load_config(cfg)\")\n",
        "    else:\n",
        "        print(\"üí° La configuration devrait √™tre charg√©e automatiquement\")\n",
        "else:\n",
        "    # üéØ NOUVEAU: Pr√©parer les informations de segmentation si applicable\n",
        "    segment_info = None\n",
        "    if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "        # R√©cup√©rer les informations du segment depuis la configuration\n",
        "        reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Obtenir les informations vid√©o\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "\n",
        "# ‚úÖ CORRECTION: Utiliser les offsets convertis en frames enti√®res\n",
        "offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "\n",
        "# Calculer les bornes du segment\n",
        "start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "    reference_frame=reference_frame,\n",
        "    offset_before=offset_before_frames,\n",
        "    offset_after=offset_after_frames,\n",
        "    total_frames=total_frames,\n",
        "    frame_interval=cfg.FRAME_INTERVAL\n",
        ")\n",
        "        \n",
        "        segment_info = {\n",
        "            'start_frame': start_frame,\n",
        "            'end_frame': end_frame,\n",
        "            'processed_start_idx': processed_start_idx,\n",
        "            'processed_end_idx': processed_end_idx\n",
        "        }\n",
        "        \n",
        "        print(\"üéØ Informations segmentation pr√©par√©es pour l'ajout d'annotations\")\n",
        "    \n",
        "    # ‚úÖ CORRECTION: Passer les informations de segmentation\n",
        "    added_objects, initial_annotations_data = add_initial_annotations(\n",
        "        cfg.predictor, \n",
        "        cfg.inference_state, \n",
        "        project_config,\n",
        "        cfg.FRAME_INTERVAL,\n",
        "        segment_info  # ‚Üê Nouvelles informations de segmentation\n",
        "    )\n",
        "    \n",
        "    # Ajout √† la configuration pour usage ult√©rieur\n",
        "    cfg.added_objects = added_objects\n",
        "    cfg.initial_annotations_data = initial_annotations_data\n",
        "    \n",
        "    print(\"\\n‚úÖ Annotations initiales ajout√©es avec succ√®s!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üîß FONCTIONS UTILITAIRES POUR ANNOTATIONS COMPL√àTES\n",
        "# =============================================================================\n",
        "\n",
        "def get_object_scores(predictor, inference_state, frame_idx, obj_id):\n",
        "    \"\"\"R√©cup√®re les scores d'objet de mani√®re propre et s√ªre\"\"\"\n",
        "    try:\n",
        "        obj_idx = predictor._obj_id_to_idx(inference_state, obj_id)\n",
        "        obj_output_dict = inference_state[\"output_dict_per_obj\"][obj_idx]\n",
        "        temp_output_dict = inference_state[\"temp_output_dict_per_obj\"][obj_idx]\n",
        "\n",
        "        # Chercher dans les outputs\n",
        "        frame_output = None\n",
        "        if frame_idx in temp_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in temp_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "\n",
        "        if frame_output and \"object_score_logits\" in frame_output:\n",
        "            object_score_logits = frame_output[\"object_score_logits\"]\n",
        "            return torch.sigmoid(object_score_logits).item()\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def calculate_bbox_from_rle(rle_data: Dict[str, Any]) -> Optional[Dict[str, int]]:\n",
        "    \"\"\"Calcule la bounding box depuis un RLE base64.\"\"\"\n",
        "    from pycocotools.mask import toBbox\n",
        "\n",
        "    try:\n",
        "        rle = {\n",
        "            \"size\": rle_data[\"size\"],\n",
        "            \"counts\": base64.b64decode(rle_data[\"counts\"])\n",
        "        }\n",
        "\n",
        "        bbox = toBbox(rle)\n",
        "\n",
        "        result = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur calcul bbox: {e}\")\n",
        "        return None\n",
        "\n",
        "def image_to_world(point_2d, cam_params):\n",
        "    \"\"\"\n",
        "    Projette un point 2D de l'image vers le plan du terrain (Z=0).\n",
        "    \"\"\"\n",
        "    # Create projection matrix P\n",
        "    K = np.array([\n",
        "        [cam_params[\"cam_params\"][\"x_focal_length\"], 0, cam_params[\"cam_params\"][\"principal_point\"][0]],\n",
        "        [0, cam_params[\"cam_params\"][\"y_focal_length\"], cam_params[\"cam_params\"][\"principal_point\"][1]],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    R = np.array(cam_params[\"cam_params\"][\"rotation_matrix\"])\n",
        "    t = -R @ np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    P = K @ np.hstack((R, t.reshape(-1,1)))\n",
        "\n",
        "    # Create point on image plane in homogeneous coordinates\n",
        "    point_2d_h = np.array([point_2d[0], point_2d[1], 1])\n",
        "\n",
        "    # Back-project ray from camera\n",
        "    ray = np.linalg.inv(K) @ point_2d_h\n",
        "    ray = R.T @ ray\n",
        "\n",
        "    # Find intersection with Z=0 plane\n",
        "    camera_pos = np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    t = -camera_pos[2] / ray[2]\n",
        "    world_point = camera_pos + t * ray\n",
        "\n",
        "    return world_point[:2]  # Return only X,Y coordinates since Z=0\n",
        "\n",
        "def calculate_points_output(bbox_output: dict, cam_params: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calcule les points de sortie √† partir de la bbox output.\n",
        "\n",
        "    Args:\n",
        "        bbox_output: Dict avec 'x', 'y', 'width', 'height'\n",
        "        cam_params: Param√®tres de calibration cam√©ra pour projection terrain\n",
        "\n",
        "    Returns:\n",
        "        Dict avec les points calcul√©s s√©par√©s par plan (image vs field)\n",
        "    \"\"\"\n",
        "    if not bbox_output:\n",
        "        return None\n",
        "\n",
        "    # Calculer le point CENTER_BOTTOM dans le plan image\n",
        "    center_bottom_x = bbox_output['x'] + bbox_output['width'] / 2\n",
        "    center_bottom_y = bbox_output['y'] + bbox_output['height']  # Bas de la bbox\n",
        "\n",
        "    # Structure avec s√©paration image/field\n",
        "    points_output = {\n",
        "        \"image\": {\n",
        "            \"CENTER_BOTTOM\": {\n",
        "                \"x\": float(center_bottom_x),\n",
        "                \"y\": float(center_bottom_y)\n",
        "            }\n",
        "        },\n",
        "        \"field\": {\n",
        "            \"CENTER_BOTTOM\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Projection vers le terrain si les param√®tres cam√©ra sont fournis\n",
        "    if cam_params:\n",
        "        try:\n",
        "            # Projeter le point CENTER_BOTTOM vers le terrain\n",
        "            image_point = [center_bottom_x, center_bottom_y]\n",
        "            field_point = image_to_world(image_point, cam_params)\n",
        "\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = {\n",
        "                \"x\": float(field_point[0]),\n",
        "                \"y\": float(field_point[1])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur projection terrain: {e}\")\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = None\n",
        "\n",
        "    return points_output\n",
        "\n",
        "def create_mask_annotation(obj_id: int, mask_logits, predictor=None, inference_state=None,\n",
        "                         frame_idx=None, cam_params: Dict = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Cr√©e une annotation de masque compl√®te avec points input/output, bbox et scores.\n",
        "    \"\"\"\n",
        "    # Conversion en masque binaire\n",
        "    mask = (mask_logits > 0.0).cpu().numpy()\n",
        "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "\n",
        "    # Encodage RLE\n",
        "    if not mask.flags['F_CONTIGUOUS']:\n",
        "        mask = np.asfortranarray(mask)\n",
        "\n",
        "    rle = encode_rle(mask.astype(np.uint8))\n",
        "    base64_counts = base64.b64encode(rle[\"counts\"]).decode('ascii')\n",
        "\n",
        "    # Calcul bbox et points output si masque non vide\n",
        "    bbox_output = None\n",
        "    points_output = None\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        from pycocotools.mask import toBbox\n",
        "        bbox = toBbox(rle)\n",
        "        bbox_output = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        # Calcul des points output depuis la bbox\n",
        "        points_output = calculate_points_output(bbox_output, cam_params)\n",
        "\n",
        "    # R√©cup√©ration du score du masque\n",
        "    mask_score = None\n",
        "\n",
        "    if predictor and inference_state and frame_idx is not None:\n",
        "        # Score du masque\n",
        "        mask_score = get_object_scores(predictor, inference_state, frame_idx, obj_id)\n",
        "\n",
        "    # Structure d'annotation compl√®te\n",
        "    return {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"objectId\": str(obj_id),\n",
        "        \"type\": \"mask\",\n",
        "        \"mask\": {\n",
        "            \"format\": \"rle_coco_base64\",\n",
        "            \"size\": [int(rle[\"size\"][0]), int(rle[\"size\"][1])],\n",
        "            \"counts\": base64_counts\n",
        "        },\n",
        "        \"bbox\": {\n",
        "            \"output\": bbox_output\n",
        "        },\n",
        "        \"points\": {\n",
        "            \"output\": points_output\n",
        "        },\n",
        "        \"maskScore\": mask_score,\n",
        "        \"pose\": None,\n",
        "        \"warning\": False\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Fonctions utilitaires pour annotations charg√©es!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üîÑ PROPAGATION ET G√âN√âRATION DES ANNOTATIONS\n",
        "# =============================================================================\n",
        "def generate_frame_mapping_with_anchor(total_frames: int, frame_interval: int, anchor_frame: int) -> List[Optional[int]]:\n",
        "    \"\"\"\n",
        "    G√©n√®re le mapping en s'assurant que anchor_frame est incluse.\n",
        "    \n",
        "    Args:\n",
        "        total_frames: Nombre total de frames dans la vid√©o originale\n",
        "        frame_interval: Intervalle entre frames (ex: 10 = 1 frame sur 10)\n",
        "        anchor_frame: Frame qui DOIT √™tre incluse (frame d'annotation initiale)\n",
        "    \n",
        "    Returns:\n",
        "        Liste o√π l'index = frame originale, valeur = frame trait√©e (ou None si pas trait√©e)\n",
        "    \"\"\"\n",
        "    frame_mapping = [None] * total_frames\n",
        "    processed_frames = set()\n",
        "    processed_idx = 0\n",
        "    \n",
        "    # 1. S'assurer que anchor_frame est incluse\n",
        "    if 0 <= anchor_frame < total_frames:\n",
        "        frame_mapping[anchor_frame] = processed_idx\n",
        "        processed_frames.add(anchor_frame)\n",
        "        processed_idx += 1\n",
        "    \n",
        "    # 2. Ajouter les frames selon l'intervalle, en √©vitant les doublons\n",
        "    for original_idx in range(0, total_frames, frame_interval):\n",
        "        if original_idx not in processed_frames:\n",
        "            frame_mapping[original_idx] = processed_idx\n",
        "            processed_frames.add(original_idx)\n",
        "            processed_idx += 1\n",
        "    \n",
        "    # 3. R√©organiser les indices pour que anchor_frame ait l'index correspondant √† sa position chronologique\n",
        "    # Tri des frames trait√©es par ordre chronologique\n",
        "    sorted_frames = sorted(processed_frames)\n",
        "    final_mapping = [None] * total_frames\n",
        "    \n",
        "    for new_idx, original_frame in enumerate(sorted_frames):\n",
        "        final_mapping[original_frame] = new_idx\n",
        "    \n",
        "    return final_mapping, sorted_frames\n",
        "\n",
        "def get_initial_annotation_frame(project_config: Dict[str, Any], frame_interval: int = 1) -> int:\n",
        "    \"\"\"R√©cup√®re la frame d'annotation initiale depuis la config et la convertit selon l'intervalle\"\"\"\n",
        "    if not project_config.get('initial_annotations'):\n",
        "        return 0\n",
        "    \n",
        "    # Prendre la premi√®re frame d'annotation comme anchor (frame originale)\n",
        "    first_annotation = project_config['initial_annotations'][0]\n",
        "    original_frame = first_annotation.get('frame', 0)\n",
        "    \n",
        "    # ‚úÖ CONVERSION: Frame originale ‚Üí Frame trait√©e selon l'intervalle\n",
        "    processed_frame = original_frame // frame_interval\n",
        "    \n",
        "    print(f\"   üîÑ Conversion anchor frame: {original_frame} (original) ‚Üí {processed_frame} (trait√©e avec intervalle {frame_interval})\")\n",
        "    \n",
        "    return processed_frame\n",
        "\n",
        "def get_video_info(video_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"R√©cup√®re les informations de la vid√©o\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
        "    \n",
        "    video_info = {\n",
        "        'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        'fps': cap.get(cv2.CAP_PROP_FPS),\n",
        "        'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    }\n",
        "    cap.release()\n",
        "    return video_info\n",
        "def create_project_structure_with_config(cfg, project_config: Dict[str, Any], added_objects: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Cr√©e la structure JSON du projet avec la configuration centralis√©e.\"\"\"\n",
        "    \n",
        "    # Informations vid√©o\n",
        "    video_info = get_video_info(cfg.video_path)\n",
        "    \n",
        "    # ‚úÖ CORRECTION: Passer l'intervalle pour la conversion\n",
        "    anchor_frame = get_initial_annotation_frame(project_config, cfg.FRAME_INTERVAL)\n",
        "    \n",
        "    # Calcul du mapping avec la frame d√©j√† convertie\n",
        "    frame_mapping, processed_frames = generate_frame_mapping_with_anchor(\n",
        "        video_info['total_frames'], \n",
        "        cfg.FRAME_INTERVAL,\n",
        "        anchor_frame * cfg.FRAME_INTERVAL  # ‚Üê Reconvertir pour le mapping original\n",
        "    )\n",
        "    processed_frame_count = len(processed_frames)\n",
        "\n",
        "    # Structure objects avec couleurs\n",
        "    import random\n",
        "    import colorsys\n",
        "\n",
        "    config_objects_mapping = {}\n",
        "    for obj in project_config['objects']:\n",
        "        config_objects_mapping[obj['obj_id']] = obj\n",
        "\n",
        "    objects = {}\n",
        "    for obj_data in added_objects:\n",
        "        obj_id = str(obj_data['obj_id'])\n",
        "        obj_type = obj_data['obj_type']\n",
        "\n",
        "        # R√©cup√©rer les informations compl√®tes depuis le config\n",
        "        config_obj = config_objects_mapping.get(int(obj_id), {})\n",
        "\n",
        "        # Couleur al√©atoire reproductible\n",
        "        random.seed(int(obj_id) * 12345)\n",
        "        hue = random.random()\n",
        "        rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n",
        "        )\n",
        "\n",
        "        objects[obj_id] = {\n",
        "            \"id\": obj_id,\n",
        "            \"type\": obj_type,\n",
        "            \"team\": config_obj.get('team', None),\n",
        "            \"jersey_number\": config_obj.get('jersey_number', None),\n",
        "            \"jersey_color\": config_obj.get('jersey_color', None),\n",
        "            \"role\": config_obj.get('role', None),\n",
        "            \"display_color\": hex_color\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"format_version\": \"1.0\",\n",
        "        \"video\": f\"{cfg.VIDEO_NAME}.mp4\",\n",
        "        \"metadata\": {\n",
        "            \"project_id\": str(uuid.uuid4()),\n",
        "            \"created_at\": datetime.now().isoformat() + \"Z\",\n",
        "            \"fps\": video_info['fps'],\n",
        "            \"resolution\": {\n",
        "                \"width\": video_info['width'],\n",
        "                \"height\": video_info['height'],\n",
        "                \"aspect_ratio\": round(video_info['width'] / video_info['height'], 2)\n",
        "            },\n",
        "            \"frame_interval\": cfg.FRAME_INTERVAL,\n",
        "            \"frame_count_original\": video_info['total_frames'],\n",
        "            \"frame_count_processed\": processed_frame_count,\n",
        "            \"frame_mapping\": frame_mapping,\n",
        "            \"anchor_frame\": anchor_frame,  # ‚úÖ AJOUT: Stockage de la frame d'ancrage\n",
        "            \"static_video\": False\n",
        "        },\n",
        "        \"calibration\": project_config['calibration'],\n",
        "        \"objects\": objects,\n",
        "        \"initial_annotations\": project_config['initial_annotations'],\n",
        "        \"annotations\": {}\n",
        "    }\n",
        "\n",
        "def run_sam2_propagation(cfg, project_config):\n",
        "    \"\"\"Ex√©cute la propagation SAM2 et g√©n√®re toutes les annotations\"\"\"\n",
        "    \n",
        "    print(f\"üîÑ D√©marrage de la propagation...\")\n",
        "    print(f\"   üé¨ {cfg.extracted_frames_count} frames √† traiter\")\n",
        "    print(f\"   üéØ {len(cfg.added_objects)} objets √† suivre\")\n",
        "\n",
        "    # Cr√©ation de la structure du projet\n",
        "    project = create_project_structure_with_config(cfg, project_config, cfg.added_objects)\n",
        "\n",
        "    # Propagation et annotation\n",
        "    frame_count = 0\n",
        "    for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(cfg.inference_state):\n",
        "\n",
        "        if str(out_frame_idx) not in project['annotations']:\n",
        "            project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "        for i, out_obj_id in enumerate(out_obj_ids):\n",
        "            annotation = create_mask_annotation(\n",
        "                obj_id=out_obj_id,\n",
        "                mask_logits=out_mask_logits[i],\n",
        "                predictor=cfg.predictor,\n",
        "                inference_state=cfg.inference_state,\n",
        "                frame_idx=out_frame_idx,\n",
        "                cam_params=project_config['calibration']['camera_parameters']\n",
        "            )\n",
        "            project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "        \n",
        "        frame_count += 1\n",
        "        if frame_count % 50 == 0:\n",
        "            progress = (frame_count / cfg.extracted_frames_count) * 100\n",
        "            print(f\"   üìä Progr√®s: {frame_count}/{cfg.extracted_frames_count} frames ({progress:.1f}%)\")\n",
        "    \n",
        "    print(f\"‚úÖ Propagation termin√©e: {frame_count} frames trait√©es\")\n",
        "    return project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_sam2_bidirectional_propagation(cfg, project_config):\n",
        "    \"\"\"Ex√©cute la propagation SAM2 bidirectionnelle depuis la frame d'annotation avec support segmentation\"\"\"\n",
        "    \n",
        "    print(f\"üîÑ D√©marrage de la propagation bidirectionnelle...\")\n",
        "    \n",
        "    # üéØ NOUVEAU: Gestion du mode segmentation\n",
        "    segment_mode = hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE\n",
        "    \n",
        "    if segment_mode:\n",
        "        print(f\"üéØ Mode segmentation activ√©\")\n",
        "        \n",
        "        # En mode segmentation, utiliser les indices ajust√©s\n",
        "        reference_frame_original = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Recalculer les bornes du segment\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        \n",
        "        # ‚úÖ AM√âLIORATION: Utiliser les offsets calcul√©s depuis les secondes\n",
        "        offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "        \n",
        "        start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "            reference_frame=reference_frame_original,\n",
        "            offset_before=offset_before_frames,\n",
        "            offset_after=offset_after_frames,\n",
        "            total_frames=total_frames,\n",
        "            frame_interval=cfg.FRAME_INTERVAL\n",
        "        )\n",
        "        \n",
        "        # Calculer l'index dans le segment\n",
        "        reference_frame_processed = reference_frame_original // cfg.FRAME_INTERVAL\n",
        "        segment_start_processed = start_frame // cfg.FRAME_INTERVAL\n",
        "        anchor_frame_in_segment = reference_frame_processed - segment_start_processed\n",
        "        \n",
        "        print(f\"   üìç Frame de r√©f√©rence originale: {reference_frame_original}\")\n",
        "        print(f\"   üìç Frame de r√©f√©rence trait√©e: {reference_frame_processed}\")\n",
        "        print(f\"   üìç Segment commence √†: {segment_start_processed} (trait√©)\")\n",
        "        print(f\"   üìç Index dans le segment: {anchor_frame_in_segment}\")\n",
        "        print(f\"   üìä Frames disponibles dans le segment: 0 √† {cfg.extracted_frames_count - 1}\")\n",
        "        \n",
        "        # V√©rifier que l'index est valide\n",
        "        if anchor_frame_in_segment < 0 or anchor_frame_in_segment >= cfg.extracted_frames_count:\n",
        "            raise ValueError(f\"‚ùå Index anchor {anchor_frame_in_segment} en dehors des frames disponibles [0, {cfg.extracted_frames_count - 1}]\")\n",
        "        \n",
        "        # En mode segmentation, les frames vont de 0 √† extracted_frames_count-1\n",
        "        anchor_processed_idx = anchor_frame_in_segment\n",
        "        total_frames_available = cfg.extracted_frames_count\n",
        "        \n",
        "        # Mapping simplifi√© pour la segmentation\n",
        "        processed_frames = list(range(total_frames_available))\n",
        "        frame_mapping = [None] * total_frames\n",
        "        \n",
        "        # Remplir le mapping pour le segment\n",
        "        for i, original_frame in enumerate(range(start_frame, end_frame + 1, cfg.FRAME_INTERVAL)):\n",
        "            if original_frame < total_frames:\n",
        "                frame_mapping[original_frame] = i\n",
        "        \n",
        "    else:\n",
        "        print(f\"üé¨ Mode complet activ√©\")\n",
        "        \n",
        "        # Mode complet : logique originale\n",
        "        reference_frame_original = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        reference_frame_processed = reference_frame_original // cfg.FRAME_INTERVAL\n",
        "        \n",
        "        # Informations vid√©o et mapping\n",
        "        video_info = get_video_info(cfg.video_path)\n",
        "        frame_mapping, processed_frames = generate_frame_mapping_with_anchor(\n",
        "            video_info['total_frames'], \n",
        "            cfg.FRAME_INTERVAL, \n",
        "            reference_frame_processed\n",
        "        )\n",
        "        \n",
        "        # Trouver l'index de la frame d'ancrage dans les frames trait√©es\n",
        "        anchor_processed_idx = frame_mapping[reference_frame_processed]\n",
        "        total_frames_available = len(processed_frames)\n",
        "        \n",
        "        print(f\"   üìç Frame d'ancrage: {reference_frame_original} (original) ‚Üí {reference_frame_processed} (trait√©e) ‚Üí {anchor_processed_idx} (index)\")\n",
        "        print(f\"   üìä Frames disponibles: 0 √† {total_frames_available - 1}\")\n",
        "    \n",
        "    print(f\"   üìç Anchor frame index: {anchor_processed_idx}\")\n",
        "    print(f\"   üìä Total frames disponibles: {total_frames_available}\")\n",
        "    \n",
        "    # Cr√©ation de la structure du projet\n",
        "    project = create_project_structure_with_config(cfg, project_config, cfg.added_objects)\n",
        "    \n",
        "    # ‚úÖ CORRECTION: M√©tadonn√©es harmonis√©es pour les deux modes\n",
        "    project['metadata']['segment_mode'] = segment_mode\n",
        "    project['metadata']['anchor_processed_idx'] = anchor_processed_idx  # ‚Üê Toujours pr√©sent\n",
        "    project['metadata']['frame_mapping'] = frame_mapping\n",
        "    \n",
        "    if segment_mode:\n",
        "        # M√©tadonn√©es sp√©cifiques au mode segmentation\n",
        "        project['metadata']['segment_start_frame'] = start_frame\n",
        "        project['metadata']['segment_end_frame'] = end_frame\n",
        "        project['metadata']['segment_reference_frame'] = reference_frame_original\n",
        "        project['metadata']['anchor_frame_in_segment'] = anchor_frame_in_segment\n",
        "        project['metadata']['anchor_frame'] = reference_frame_original  # ‚Üê Frame originale en mode segmentation\n",
        "    else:\n",
        "        # M√©tadonn√©es sp√©cifiques au mode complet\n",
        "        project['metadata']['anchor_frame'] = reference_frame_processed  # ‚Üê Frame trait√©e en mode complet\n",
        "    \n",
        "    # === PHASE 1: PROPAGATION INVERSE (anchor ‚Üí 0) ===\n",
        "    if anchor_processed_idx > 0:\n",
        "        print(f\"\\nüîÑ PHASE 1: Propagation inverse (frame {anchor_processed_idx} ‚Üí 0)\")\n",
        "        \n",
        "        reverse_frames_count = 0\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(\n",
        "            cfg.inference_state,\n",
        "            start_frame_idx=anchor_processed_idx,\n",
        "            max_frame_num_to_track=anchor_processed_idx + 1,  # +1 pour inclure l'anchor\n",
        "            reverse=True\n",
        "        ):\n",
        "            # Traitement identique √† la propagation normale\n",
        "            if str(out_frame_idx) not in project['annotations']:\n",
        "                project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "            for i, out_obj_id in enumerate(out_obj_ids):\n",
        "                annotation = create_mask_annotation(\n",
        "                    obj_id=out_obj_id,\n",
        "                    mask_logits=out_mask_logits[i],\n",
        "                    predictor=cfg.predictor,\n",
        "                    inference_state=cfg.inference_state,\n",
        "                    frame_idx=out_frame_idx,\n",
        "                    cam_params=project_config['calibration']['camera_parameters']\n",
        "                )\n",
        "                project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "            \n",
        "            reverse_frames_count += 1\n",
        "            if reverse_frames_count % 10 == 0:\n",
        "                print(f\"     üìä Frames trait√©es (reverse): {reverse_frames_count}\")\n",
        "        \n",
        "        print(f\"   ‚úÖ Phase reverse termin√©e: {reverse_frames_count} frames\")\n",
        "    else:\n",
        "        print(f\"\\n‚è≠Ô∏è PHASE 1: Propagation inverse skipp√©e (anchor √† la frame 0)\")\n",
        "    \n",
        "    # === PHASE 2: PROPAGATION AVANT (anchor ‚Üí fin) ===\n",
        "    remaining_frames = total_frames_available - anchor_processed_idx\n",
        "    if remaining_frames > 1:  # > 1 car anchor d√©j√† trait√©e\n",
        "        print(f\"\\nüîÑ PHASE 2: Propagation avant (frame {anchor_processed_idx} ‚Üí {total_frames_available - 1})\")\n",
        "        \n",
        "        forward_frames_count = 0\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(\n",
        "            cfg.inference_state,\n",
        "            start_frame_idx=anchor_processed_idx,\n",
        "            max_frame_num_to_track=remaining_frames,\n",
        "            reverse=False\n",
        "        ):\n",
        "            # √âviter de traiter √† nouveau l'anchor frame\n",
        "            if out_frame_idx == anchor_processed_idx and str(out_frame_idx) in project['annotations']:\n",
        "                continue\n",
        "                \n",
        "            if str(out_frame_idx) not in project['annotations']:\n",
        "                project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "            for i, out_obj_id in enumerate(out_obj_ids):\n",
        "                annotation = create_mask_annotation(\n",
        "                    obj_id=out_obj_id,\n",
        "                    mask_logits=out_mask_logits[i],\n",
        "                    predictor=cfg.predictor,\n",
        "                    inference_state=cfg.inference_state,\n",
        "                    frame_idx=out_frame_idx,\n",
        "                    cam_params=project_config['calibration']['camera_parameters']\n",
        "                )\n",
        "                project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "            \n",
        "            forward_frames_count += 1\n",
        "            if forward_frames_count % 20 == 0:\n",
        "                progress = (forward_frames_count / remaining_frames) * 100\n",
        "                print(f\"     üìä Frames trait√©es (forward): {forward_frames_count}/{remaining_frames} ({progress:.1f}%)\")\n",
        "        \n",
        "        print(f\"   ‚úÖ Phase forward termin√©e: {forward_frames_count} frames\")\n",
        "    else:\n",
        "        print(f\"\\n‚è≠Ô∏è PHASE 2: Propagation avant skipp√©e (anchor √† la fin)\")\n",
        "    \n",
        "    total_frames_processed = len(project['annotations'])\n",
        "    print(f\"\\n‚úÖ Propagation bidirectionnelle termin√©e:\")\n",
        "    print(f\"   üìä Total frames trait√©es: {total_frames_processed}\")\n",
        "    if segment_mode:\n",
        "        print(f\"   üéØ Mode segmentation: Frame d'ancrage √† l'index {anchor_processed_idx} dans le segment\")\n",
        "        print(f\"   üìç Segment original: frames {start_frame} √† {end_frame}\")\n",
        "    else:\n",
        "        print(f\"   üéØ Mode complet: Frame d'ancrage {anchor_processed_idx} sur {total_frames_available}\")\n",
        "    \n",
        "    return project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# üéØ PROPAGATION AVEC SEGMENTATION OPTIONNELLE\n",
        "# =============================================================================\n",
        "# V√©rifier le mode de segmentation\n",
        "if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "    print(\"üéØ MODE SEGMENTATION ACTIV√â\")\n",
        "    \n",
        "    # R√©cup√©rer la frame de r√©f√©rence\n",
        "    reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "    \n",
        "    # Obtenir les informations vid√©o\n",
        "    cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "    \n",
        "    # ‚úÖ AM√âLIORATION: Utiliser les offsets calcul√©s depuis les secondes\n",
        "    offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "    \n",
        "    # Calculer les bornes du segment\n",
        "    start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "        reference_frame=reference_frame,\n",
        "        offset_before=offset_before_frames,\n",
        "        offset_after=offset_after_frames,\n",
        "        total_frames=total_frames,\n",
        "        frame_interval=cfg.FRAME_INTERVAL\n",
        "    )\n",
        "    \n",
        "    # R√©ajuster l'extraction pour le segment\n",
        "    if cfg.EXTRACT_FRAMES:\n",
        "        extracted_count = extract_segment_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            start_frame=start_frame,\n",
        "            end_frame=end_frame,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "        cfg.extracted_frames_count = extracted_count\n",
        "        print(f\"‚úÖ {extracted_count} frames du segment extraites\")\n",
        "    \n",
        "    print(f\"üéØ SEGMENT CONFIGUR√â:\")\n",
        "    print(f\"   üìä Frames du segment: {end_frame - start_frame + 1}\")\n",
        "    print(f\"   üé¨ Plage: {start_frame} √† {end_frame}\")\n",
        "    print(f\"   üìç Frame de r√©f√©rence: {reference_frame}\")\n",
        "    \n",
        "else:\n",
        "    print(\"üé¨ MODE COMPLET ACTIV√â (toute la vid√©o)\")\n",
        "    \n",
        "# =============================================================================\n",
        "# üîÑ PROPAGATION BIDIRECTIONNELLE (REMPLACE L'ANCIENNE PROPAGATION)\n",
        "# =============================================================================\n",
        "\n",
        "# V√©rifications des pr√©requis\n",
        "missing_requirements = []\n",
        "required_attrs = ['predictor', 'inference_state', 'added_objects', 'extracted_frames_count']\n",
        "\n",
        "for attr in required_attrs:\n",
        "    if not hasattr(cfg, attr):\n",
        "        missing_requirements.append(attr)\n",
        "\n",
        "if project_config is None:\n",
        "    missing_requirements.append(\"project_config\")\n",
        "\n",
        "if missing_requirements:\n",
        "    print(f\"‚ùå Pr√©requis manquants: {missing_requirements}\")\n",
        "    print(\"üí° Ex√©cutez d'abord les cellules pr√©c√©dentes dans l'ordre:\")\n",
        "    print(\"   1. Configuration centralis√©e\")\n",
        "    print(\"   2. Installation SAM2\") \n",
        "    print(\"   3. Chargement configuration projet\")\n",
        "    print(\"   4. Extraction des frames\")\n",
        "    print(\"   5. Initialisation SAM2\")\n",
        "    print(\"   6. Ajout des annotations initiales\")\n",
        "else:\n",
        "    # üöÄ NOUVELLE PROPAGATION BIDIRECTIONNELLE\n",
        "    print(\"üéØ Utilisation de la propagation bidirectionnelle am√©lior√©e...\")\n",
        "    \n",
        "    project = run_sam2_bidirectional_propagation(cfg, project_config)\n",
        "    \n",
        "    # Ajout √† la configuration pour usage ult√©rieur\n",
        "    cfg.project = project\n",
        "    \n",
        "    # Statistiques finales am√©lior√©es\n",
        "    total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "    unique_frames = len(project['annotations'])\n",
        "    anchor_frame = project['metadata']['anchor_frame']\n",
        "    anchor_processed_idx = project['metadata']['anchor_processed_idx']\n",
        "    \n",
        "    print(f\"\\nüìä R√âSULTATS PROPAGATION BIDIRECTIONNELLE:\")\n",
        "    print(f\"   üéØ Frame d'ancrage: {anchor_frame} (original) ‚Üí {anchor_processed_idx} (trait√©e)\")\n",
        "    print(f\"   üé¨ Frames originales: {project['metadata']['frame_count_original']:,}\")\n",
        "    print(f\"   üé¨ Frames trait√©es: {project['metadata']['frame_count_processed']:,}\")\n",
        "    print(f\"   üìç Frames avec annotations: {unique_frames:,}\")\n",
        "    print(f\"   üìç Annotations totales: {total_annotations:,}\")\n",
        "    print(f\"   üéØ Objets suivis: {len(project['objects'])}\")\n",
        "    print(f\"   ‚èØÔ∏è  Intervalle frames: {project['metadata']['frame_interval']}\")\n",
        "    \n",
        "    # Affichage du mapping frame pour v√©rification\n",
        "    frame_mapping = project['metadata']['frame_mapping']\n",
        "    mapped_frames = [(i, v) for i, v in enumerate(frame_mapping) if v is not None]\n",
        "    sample_mapping = mapped_frames[:10] if len(mapped_frames) > 10 else mapped_frames\n",
        "    \n",
        "    print(f\"\\nüóÇÔ∏è  MAPPING FRAMES (√©chantillon):\")\n",
        "    print(f\"   üìã Format: (frame_originale ‚Üí frame_trait√©e)\")\n",
        "    print(f\"   üìã √âchantillon: {sample_mapping}\")\n",
        "    if len(mapped_frames) > 10:\n",
        "        print(f\"   üìã ... et {len(mapped_frames) - 10} autres frames\")\n",
        "    \n",
        "    # V√©rification que l'anchor frame est bien dans le mapping\n",
        "    if anchor_frame < len(frame_mapping) and frame_mapping[anchor_frame] is not None:\n",
        "        print(f\"   ‚úÖ Frame d'ancrage {anchor_frame} correctement mapp√©e ‚Üí {frame_mapping[anchor_frame]}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Attention: Frame d'ancrage {anchor_frame} non trouv√©e dans le mapping\")\n",
        "    \n",
        "    # R√©sum√© par type d'objet\n",
        "    if project['objects']:\n",
        "        type_counts = {}\n",
        "        for obj_data in project['objects'].values():\n",
        "            obj_type = obj_data.get('type', 'unknown')\n",
        "            type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "        print(f\"   üè∑Ô∏è  Types d'objets: {dict(type_counts)}\")\n",
        "    \n",
        "    # Informations sur la r√©partition temporelle\n",
        "    frame_numbers = [int(f) for f in project['annotations'].keys()]\n",
        "    if frame_numbers:\n",
        "        print(f\"\\nüìà R√âPARTITION TEMPORELLE:\")\n",
        "        print(f\"   üìä Premi√®re frame annot√©e: {min(frame_numbers)}\")\n",
        "        print(f\"   üìä Derni√®re frame annot√©e: {max(frame_numbers)}\")\n",
        "        print(f\"   üìä Plage totale: {max(frame_numbers) - min(frame_numbers) + 1} frames\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Propagation bidirectionnelle termin√©e avec succ√®s!\")\n",
        "    print(f\"üí° Le projet est maintenant pr√™t pour la sauvegarde et la visualisation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHlhY4X5Adf3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üíæ SAUVEGARDE DES R√âSULTATS\n",
        "# =============================================================================\n",
        "\n",
        "def save_project_results(cfg, project, verbose=True):\n",
        "    \"\"\"Sauvegarde les r√©sultats du projet avec gestion d'erreurs\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üíæ Sauvegarde des r√©sultats...\")\n",
        "        print(f\"   üìÑ Fichier JSON: {cfg.output_json_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Sauvegarde du JSON principal\n",
        "        with open(cfg.output_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(project, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"‚úÖ Fichier JSON sauv√©: {cfg.output_json_path}\")\n",
        "            \n",
        "        # Sauvegarde d'une version compacte (sans indent pour √©conomiser l'espace)\n",
        "        compact_json_path = cfg.output_dir / f\"{cfg.VIDEO_NAME}_project_compact.json\"\n",
        "        with open(compact_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(project, f, separators=(',', ':'), ensure_ascii=False)\n",
        "        \n",
        "        if verbose:\n",
        "            # Tailles des fichiers\n",
        "            json_size = cfg.output_json_path.stat().st_size / 1024**2  # MB\n",
        "            compact_size = compact_json_path.stat().st_size / 1024**2  # MB\n",
        "            print(f\"   üìÑ Version format√©e: {json_size:.1f}MB\")\n",
        "            print(f\"   üìÑ Version compacte: {compact_size:.1f}MB\")\n",
        "            \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
        "        return False\n",
        "\n",
        "def display_final_statistics(cfg, project):\n",
        "    \"\"\"Affiche les statistiques finales du projet\"\"\"\n",
        "    \n",
        "    # Calcul des statistiques\n",
        "    total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "    unique_frames = len(project['annotations'])\n",
        "    \n",
        "    # Calcul de la taille des donn√©es\n",
        "    json_size = cfg.output_json_path.stat().st_size / 1024**2 if cfg.output_json_path.exists() else 0\n",
        "    \n",
        "    print(f\"\\nüìä R√âSULTATS FINAUX:\")\n",
        "    print(f\"   üé¨ Frames originales: {project['metadata']['frame_count_original']:,}\")\n",
        "    print(f\"   üé¨ Frames trait√©es: {project['metadata']['frame_count_processed']:,}\")\n",
        "    print(f\"   üìç Frames avec annotations: {unique_frames:,}\")\n",
        "    print(f\"   üìç Annotations totales: {total_annotations:,}\")\n",
        "    print(f\"   üéØ Objets suivis: {len(project['objects'])}\")\n",
        "    print(f\"   ‚èØÔ∏è  Intervalle frames: {project['metadata']['frame_interval']}\")\n",
        "    print(f\"   üìÑ Taille JSON: {json_size:.1f}MB\")\n",
        "    print(f\"   üìÅ Dossier de sortie: {cfg.output_dir}\")\n",
        "    \n",
        "    # Affichage d'un √©chantillon du mapping\n",
        "    sample_mapping = [(i, v) for i, v in enumerate(project['metadata']['frame_mapping'][:50]) if v is not None]\n",
        "    print(f\"   üóÇÔ∏è  Mapping √©chantillon (original‚Üítrait√©): {sample_mapping[:5]}...\")\n",
        "    \n",
        "    # R√©sum√© par type d'objet\n",
        "    if project['objects']:\n",
        "        type_counts = {}\n",
        "        for obj_data in project['objects'].values():\n",
        "            obj_type = obj_data.get('type', 'unknown')\n",
        "            type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "        print(f\"   üè∑Ô∏è  Types d'objets: {dict(type_counts)}\")\n",
        "def create_colab_backup(cfg, use_timestamp=False, custom_suffix=\"\"):\n",
        "    \"\"\"Cr√©e une sauvegarde sur Google Drive pour Colab\"\"\"\n",
        "    \n",
        "    if not cfg.USING_COLAB:\n",
        "        print(\"üñ•Ô∏è Mode local: sauvegarde Google Drive skipp√©e\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        import shutil\n",
        "        from datetime import datetime\n",
        "        \n",
        "        print(\"üî¨ Mode Colab: Cr√©ation de la sauvegarde Google Drive...\")\n",
        "        \n",
        "        # Monter le Drive\n",
        "        print(\"   üìÅ Montage Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        \n",
        "        # Construction du nom selon les options\n",
        "        if use_timestamp:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            zip_name = f\"{cfg.VIDEO_NAME}_{timestamp}\"\n",
        "        elif custom_suffix:\n",
        "            zip_name = f\"{cfg.VIDEO_NAME}_{custom_suffix}\"\n",
        "        else:\n",
        "            zip_name = cfg.VIDEO_NAME\n",
        "        \n",
        "        drive_zip_path = f'/content/drive/MyDrive/{zip_name}'\n",
        "        \n",
        "        # V√©rifier si le fichier existe d√©j√†\n",
        "        existing_zip = Path(f\"{drive_zip_path}.zip\")\n",
        "        if existing_zip.exists():\n",
        "            existing_size = existing_zip.stat().st_size / 1024**2\n",
        "            print(f\"   üîÑ Fichier existant trouv√©: {zip_name}.zip ({existing_size:.1f}MB) - sera √©cras√©\")\n",
        "        \n",
        "        print(f\"   üì¶ Cr√©ation du ZIP: {zip_name}.zip\")\n",
        "        \n",
        "        # Cr√©er le ZIP avec tout le dossier videos\n",
        "        shutil.make_archive(drive_zip_path, 'zip', str(cfg.videos_dir))\n",
        "        \n",
        "        # V√©rifier la taille\n",
        "        zip_path = Path(f\"{drive_zip_path}.zip\")\n",
        "        if zip_path.exists():\n",
        "            zip_size = zip_path.stat().st_size / 1024**2  # MB\n",
        "            print(f\"‚úÖ Sauvegarde cr√©√©e: {zip_name}.zip ({zip_size:.1f}MB)\")\n",
        "            print(f\"   üìÅ Emplacement: MyDrive/{zip_name}.zip\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Erreur: fichier ZIP non cr√©√©\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur sauvegarde Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "def cleanup_temporary_files(cfg):\n",
        "    \"\"\"Nettoie les fichiers temporaires (optionnel)\"\"\"\n",
        "    \n",
        "    print(\"üßπ Nettoyage optionnel...\")\n",
        "    \n",
        "    # Comptage des frames\n",
        "    frame_count = len(list(cfg.frames_dir.glob(\"*.jpg\")))\n",
        "    frame_size = sum(f.stat().st_size for f in cfg.frames_dir.glob(\"*.jpg\")) / 1024**2\n",
        "    \n",
        "    print(f\"   üñºÔ∏è  Frames extraites: {frame_count} fichiers ({frame_size:.1f}MB)\")\n",
        "    print(f\"   üí° Pour lib√©rer l'espace, vous pouvez supprimer: {cfg.frames_dir}\")\n",
        "    print(f\"   üí° Les frames peuvent √™tre r√©-extraites avec FORCE_EXTRACTION=True\")\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ SAUVEGARDE AVEC √âCRASEMENT PAR D√âFAUT\n",
        "# =============================================================================\n",
        "\n",
        "# V√©rification des pr√©requis\n",
        "if not hasattr(cfg, 'project') or cfg.project is None:\n",
        "    print(\"‚ùå Projet non disponible. Ex√©cutez d'abord la propagation.\")\n",
        "else:\n",
        "    print(f\"üíæ D√©marrage de la sauvegarde...\")\n",
        "    \n",
        "    # Sauvegarde principale\n",
        "    save_success = save_project_results(cfg, cfg.project)\n",
        "    \n",
        "    if save_success:\n",
        "        # Affichage des statistiques\n",
        "        display_final_statistics(cfg, cfg.project)\n",
        "        \n",
        "        # Sauvegarde Colab si applicable\n",
        "        if cfg.USING_COLAB:\n",
        "            print(f\"\\nüî¨ Sauvegarde Google Drive...\")\n",
        "            \n",
        "            # Mode par d√©faut : √©crase l'ancienne version\n",
        "            colab_success = create_colab_backup(cfg)  # use_timestamp=False par d√©faut\n",
        "            \n",
        "            # Si vous voulez un timestamp occasionnellement, d√©commentez :\n",
        "            # colab_success = create_colab_backup(cfg, use_timestamp=True)\n",
        "            \n",
        "            if colab_success:\n",
        "                print(\"‚úÖ Sauvegarde Google Drive r√©ussie!\")\n",
        "        \n",
        "        # Nettoyage optionnel\n",
        "        print(f\"\\nüßπ Informations de nettoyage:\")\n",
        "        cleanup_temporary_files(cfg)\n",
        "        \n",
        "        print(f\"\\nüéâ Pipeline SAM2 termin√© avec succ√®s!\")\n",
        "        print(f\"üìÑ Fichier principal: {cfg.output_json_path}\")\n",
        "        if cfg.USING_COLAB:\n",
        "            print(f\"‚òÅÔ∏è Sauvegarde Drive: MyDrive/{cfg.VIDEO_NAME}.zip\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ùå √âchec de la sauvegarde\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
