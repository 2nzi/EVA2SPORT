{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—ï¸ CrÃ©ation de la structure de dossiers...\n",
            "   âœ… Dossiers crÃ©Ã©s dans: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "ğŸ“‹ CONFIGURATION CENTRALISÃ‰E:\n",
            "   ğŸŒ Environnement: ğŸ–¥ï¸ Local\n",
            "   ğŸ¬ VidÃ©o: SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "   â¯ï¸  Intervalle frames: 3\n",
            "   ğŸ¬ Extraction: âœ… ActivÃ©e\n",
            "   ğŸ”„ Force extraction: âœ… Oui\n",
            "   ğŸ¯ Segmentation: âœ… ActivÃ©e\n",
            "   ğŸ¥ FPS vidÃ©o: 25.0\n",
            "   ğŸ• Offset avant: 2.0s (50 frames)\n",
            "   ğŸ• Offset aprÃ¨s: 2.0s (50 frames)\n",
            "   ğŸ¤– ModÃ¨le SAM2: sam2.1_hiera_l\n",
            "   ğŸ“ Dossier vidÃ©os: ..\\data\\videos\n",
            "   ğŸ“„ Fichier config: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1_config.json\n",
            "   ğŸ“ Sortie: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "   ğŸ’¾ Checkpoint: ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "\n",
            "ğŸ–¥ï¸ Mode Local dÃ©tectÃ©:\n",
            "âœ… Tous les fichiers sont valides\n",
            "\n",
            "âœ… Configuration centralisÃ©e initialisÃ©e!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# âš™ï¸ CONFIGURATION CENTRALISÃ‰E\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸŒ DÃ‰TECTION AUTOMATIQUE DE L'ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def detect_environment():\n",
        "    \"\"\"DÃ©tecte automatiquement si on est sur Colab ou en local\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ“‹ CONFIGURATION PRINCIPALE\n",
        "# =============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration centralisÃ©e du projet\"\"\"\n",
        "    \n",
        "    # ğŸŒ Environnement\n",
        "    USING_COLAB = detect_environment()\n",
        "    \n",
        "    # ğŸ¬ VIDÃ‰O ET PROJET  \n",
        "    VIDEO_NAME = \"SD_13_06_2025_cam1_PdB_S1_T959s_1\"  # âš ï¸ MODIFIEZ ICI\n",
        "    FRAME_INTERVAL = 3                            # âš ï¸ MODIFIEZ ICI\n",
        "    \n",
        "    # ğŸ¬ OPTIONS D'EXTRACTION\n",
        "    EXTRACT_FRAMES = True                         # âš ï¸ MODIFIEZ ICI\n",
        "    FORCE_EXTRACTION = True                      # âš ï¸ MODIFIEZ ICI\n",
        "    \n",
        "    # ğŸ¯ SEGMENTATION VIDÃ‰O (NOUVEAU)\n",
        "    SEGMENT_MODE = True                          # âš ï¸ MODIFIEZ ICI - Active le mode segmentation\n",
        "    \n",
        "    # ğŸ• OFFSETS EN SECONDES (RECOMMANDÃ‰)\n",
        "    SEGMENT_OFFSET_BEFORE_SECONDS = 2.0         # âš ï¸ MODIFIEZ ICI - Secondes AVANT la frame de rÃ©fÃ©rence\n",
        "    SEGMENT_OFFSET_AFTER_SECONDS = 2.0          # âš ï¸ MODIFIEZ ICI - Secondes APRÃˆS la frame de rÃ©fÃ©rence\n",
        "    \n",
        "    # ğŸ¬ OFFSETS EN FRAMES (OPTIONNEL - sera calculÃ© automatiquement si secondes dÃ©finies)\n",
        "    SEGMENT_OFFSET_BEFORE = None                 # âš ï¸ MODIFIEZ ICI - Frames AVANT (ou None pour auto-calcul)\n",
        "    SEGMENT_OFFSET_AFTER = None                  # âš ï¸ MODIFIEZ ICI - Frames APRÃˆS (ou None pour auto-calcul)\n",
        "    \n",
        "    # ğŸ¤– SAM2 CONFIGURATION\n",
        "    SAM2_MODEL = \"sam2.1_hiera_l\"                # ou \"sam2.1_hiera_s\" pour small\n",
        "    SAM2_CHECKPOINT = \"sam2.1_hiera_large.pt\"    # ou \"sam2.1_hiera_small.pt\"\n",
        "    \n",
        "    # ğŸ—‚ï¸ CHEMINS AUTOMATIQUES\n",
        "    @property\n",
        "    def videos_dir(self):\n",
        "        return Path(\"./videos\") if self.USING_COLAB else Path(\"../data/videos\")\n",
        "    \n",
        "    @property \n",
        "    def checkpoint_path(self):\n",
        "        base = \"../../checkpoints\" if self.USING_COLAB else \"../checkpoints\"\n",
        "        return Path(base) / self.SAM2_CHECKPOINT\n",
        "    \n",
        "    @property\n",
        "    def model_config_path(self):\n",
        "        return f\"configs/sam2.1/{self.SAM2_MODEL}.yaml\"\n",
        "    \n",
        "    # ğŸ¬ CHEMINS VIDÃ‰O\n",
        "    @property\n",
        "    def video_path(self):\n",
        "        return self.videos_dir / f\"{self.VIDEO_NAME}.mp4\"\n",
        "    \n",
        "    @property\n",
        "    def config_path(self):\n",
        "        return self.videos_dir / f\"{self.VIDEO_NAME}_config.json\"\n",
        "    \n",
        "    @property\n",
        "    def output_dir(self):\n",
        "        return self.videos_dir / \"outputs\" / self.VIDEO_NAME\n",
        "    \n",
        "    @property\n",
        "    def frames_dir(self):\n",
        "        return self.output_dir / \"frames\"\n",
        "    \n",
        "    @property\n",
        "    def masks_dir(self):\n",
        "        return self.output_dir / \"masks\"\n",
        "    \n",
        "    @property\n",
        "    def output_video_path(self):\n",
        "        return self.output_dir / f\"{self.VIDEO_NAME}_annotated.mp4\"\n",
        "    \n",
        "    @property\n",
        "    def output_json_path(self):\n",
        "        return self.output_dir / f\"{self.VIDEO_NAME}_project.json\"\n",
        "    \n",
        "    def setup_directories(self):\n",
        "        \"\"\"CrÃ©e tous les dossiers nÃ©cessaires (sans vÃ©rifier les fichiers)\"\"\"\n",
        "        print(\"ğŸ—ï¸ CrÃ©ation de la structure de dossiers...\")\n",
        "        self.videos_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.frames_dir.mkdir(exist_ok=True)\n",
        "        self.masks_dir.mkdir(exist_ok=True)\n",
        "        print(f\"   âœ… Dossiers crÃ©Ã©s dans: {self.output_dir}\")\n",
        "    \n",
        "    def check_files_exist(self):\n",
        "        \"\"\"VÃ©rifie si les fichiers requis existent (sans lever d'erreur)\"\"\"\n",
        "        video_exists = self.video_path.exists()\n",
        "        config_exists = self.config_path.exists()\n",
        "        \n",
        "        print(f\"ğŸ“„ VÃ©rification des fichiers:\")\n",
        "        print(f\"   ğŸ¬ VidÃ©o: {'âœ…' if video_exists else 'âŒ'} {self.video_path}\")\n",
        "        print(f\"   ğŸ“„ Config: {'âœ…' if config_exists else 'âŒ'} {self.config_path}\")\n",
        "        \n",
        "        return video_exists and config_exists\n",
        "    \n",
        "    def wait_for_files(self, max_wait_minutes=10, check_interval=10):\n",
        "        \"\"\"Attend que les fichiers soient disponibles (utile pour Colab)\"\"\"\n",
        "        if not self.USING_COLAB:\n",
        "            # En local, validation immÃ©diate\n",
        "            if not self.check_files_exist():\n",
        "                missing = []\n",
        "                if not self.video_path.exists():\n",
        "                    missing.append(f\"vidÃ©o: {self.video_path}\")\n",
        "                if not self.config_path.exists():\n",
        "                    missing.append(f\"config: {self.config_path}\")\n",
        "                raise FileNotFoundError(f\"âŒ Fichiers manquants: {', '.join(missing)}\")\n",
        "            return True\n",
        "        \n",
        "        # Sur Colab, attente avec timeout\n",
        "        print(f\"â³ Attente des fichiers (max {max_wait_minutes}min)...\")\n",
        "        start_time = time.time()\n",
        "        max_wait_seconds = max_wait_minutes * 60\n",
        "        \n",
        "        while time.time() - start_time < max_wait_seconds:\n",
        "            if self.check_files_exist():\n",
        "                print(\"âœ… Tous les fichiers sont disponibles!\")\n",
        "                return True\n",
        "            \n",
        "            elapsed = int(time.time() - start_time)\n",
        "            remaining = max_wait_seconds - elapsed\n",
        "            print(f\"   â³ Attente... ({elapsed}s Ã©coulÃ©es, {remaining}s restantes)\")\n",
        "            time.sleep(check_interval)\n",
        "        \n",
        "        print(f\"âš ï¸ Timeout atteint ({max_wait_minutes}min)\")\n",
        "        return False\n",
        "    \n",
        "    def validate_files_now(self):\n",
        "        \"\"\"Validation immÃ©diate avec erreur si fichiers manquants\"\"\"\n",
        "        if not self.video_path.exists():\n",
        "            raise FileNotFoundError(f\"âŒ VidÃ©o non trouvÃ©e: {self.video_path}\")\n",
        "        if not self.config_path.exists():\n",
        "            raise FileNotFoundError(f\"âŒ Fichier config non trouvÃ©: {self.config_path}\")\n",
        "        print(\"âœ… Tous les fichiers sont valides\")\n",
        "    \n",
        "    def get_video_fps(self):\n",
        "        \"\"\"RÃ©cupÃ¨re le FPS de la vidÃ©o\"\"\"\n",
        "        if not self.video_path.exists():\n",
        "            print(f\"âš ï¸  VidÃ©o non trouvÃ©e pour rÃ©cupÃ©rer le FPS: {self.video_path}\")\n",
        "            return 25.0  # FPS par dÃ©faut\n",
        "        \n",
        "        import cv2\n",
        "        cap = cv2.VideoCapture(str(self.video_path))\n",
        "        if not cap.isOpened():\n",
        "            print(f\"âš ï¸  Impossible d'ouvrir la vidÃ©o pour rÃ©cupÃ©rer le FPS\")\n",
        "            return 25.0  # FPS par dÃ©faut\n",
        "        \n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        cap.release()\n",
        "        return fps if fps > 0 else 25.0  # FPS par dÃ©faut si invalide\n",
        "    \n",
        "    def seconds_to_frames(self, seconds: float, fps: float = None) -> int:\n",
        "        \"\"\"Convertit des secondes en nombre de frames ENTIÃˆRES\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        # âœ… CORRECTION: s'assurer que le rÃ©sultat est un entier (frame entiÃ¨re)\n",
        "        return int(round(seconds * fps))\n",
        "    \n",
        "    def frames_to_seconds(self, frames: int, fps: float = None) -> float:\n",
        "        \"\"\"Convertit des frames en secondes\"\"\"\n",
        "        if fps is None:\n",
        "            fps = self.get_video_fps()\n",
        "        return frames / fps\n",
        "    \n",
        "    def get_segment_offsets_frames(self):\n",
        "        \"\"\"Calcule les offsets en frames ENTIÃˆRES, prioritÃ© aux secondes si dÃ©finies\"\"\"\n",
        "        fps = self.get_video_fps()\n",
        "        \n",
        "        # PrioritÃ© aux offsets en secondes\n",
        "        if hasattr(self, 'SEGMENT_OFFSET_BEFORE_SECONDS') and self.SEGMENT_OFFSET_BEFORE_SECONDS is not None:\n",
        "            offset_before_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_BEFORE_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_BEFORE') and self.SEGMENT_OFFSET_BEFORE is not None:\n",
        "            # âœ… CORRECTION: s'assurer que mÃªme les frames dÃ©finies manuellement sont des entiers\n",
        "            offset_before_frames = int(self.SEGMENT_OFFSET_BEFORE)\n",
        "        else:\n",
        "            offset_before_frames = self.seconds_to_frames(2.0, fps)  # DÃ©faut: 2 secondes\n",
        "        \n",
        "        if hasattr(self, 'SEGMENT_OFFSET_AFTER_SECONDS') and self.SEGMENT_OFFSET_AFTER_SECONDS is not None:\n",
        "            offset_after_frames = self.seconds_to_frames(self.SEGMENT_OFFSET_AFTER_SECONDS, fps)\n",
        "        elif hasattr(self, 'SEGMENT_OFFSET_AFTER') and self.SEGMENT_OFFSET_AFTER is not None:\n",
        "            # âœ… CORRECTION: s'assurer que mÃªme les frames dÃ©finies manuellement sont des entiers\n",
        "            offset_after_frames = int(self.SEGMENT_OFFSET_AFTER)\n",
        "        else:\n",
        "            offset_after_frames = self.seconds_to_frames(2.0, fps)  # DÃ©faut: 2 secondes\n",
        "        \n",
        "        return offset_before_frames, offset_after_frames\n",
        "    \n",
        "    def display_config(self):\n",
        "        \"\"\"Affiche la configuration actuelle\"\"\"\n",
        "        print(f\"ğŸ“‹ CONFIGURATION CENTRALISÃ‰E:\")\n",
        "        print(f\"   ğŸŒ Environnement: {'ğŸ”¬ Colab' if self.USING_COLAB else 'ğŸ–¥ï¸ Local'}\")\n",
        "        print(f\"   ğŸ¬ VidÃ©o: {self.VIDEO_NAME}\")\n",
        "        print(f\"   â¯ï¸  Intervalle frames: {self.FRAME_INTERVAL}\")\n",
        "        print(f\"   ğŸ¬ Extraction: {'âœ… ActivÃ©e' if self.EXTRACT_FRAMES else 'âŒ DÃ©sactivÃ©e'}\")\n",
        "        print(f\"   ğŸ”„ Force extraction: {'âœ… Oui' if self.FORCE_EXTRACTION else 'âŒ Non'}\")\n",
        "        print(f\"   ğŸ¯ Segmentation: {'âœ… ActivÃ©e' if getattr(self, 'SEGMENT_MODE', False) else 'âŒ DÃ©sactivÃ©e'}\")\n",
        "        \n",
        "        if getattr(self, 'SEGMENT_MODE', False):\n",
        "            # RÃ©cupÃ©rer les informations vidÃ©o\n",
        "            fps = self.get_video_fps() if self.video_path.exists() else 25.0\n",
        "            offset_before_frames, offset_after_frames = self.get_segment_offsets_frames()\n",
        "            \n",
        "            # Afficher les offsets en secondes et frames\n",
        "            offset_before_seconds = self.frames_to_seconds(offset_before_frames, fps)\n",
        "            offset_after_seconds = self.frames_to_seconds(offset_after_frames, fps)\n",
        "            \n",
        "            print(f\"   ğŸ¥ FPS vidÃ©o: {fps:.1f}\")\n",
        "            print(f\"   ğŸ• Offset avant: {offset_before_seconds:.1f}s ({offset_before_frames} frames)\")\n",
        "            print(f\"   ğŸ• Offset aprÃ¨s: {offset_after_seconds:.1f}s ({offset_after_frames} frames)\")\n",
        "            \n",
        "        print(f\"   ğŸ¤– ModÃ¨le SAM2: {self.SAM2_MODEL}\")\n",
        "        print(f\"   ğŸ“ Dossier vidÃ©os: {self.videos_dir}\")\n",
        "        print(f\"   ğŸ“„ Fichier config: {self.config_path}\")\n",
        "        print(f\"   ğŸ“ Sortie: {self.output_dir}\")\n",
        "        print(f\"   ğŸ’¾ Checkpoint: {self.checkpoint_path}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ INITIALISATION\n",
        "# =============================================================================\n",
        "\n",
        "# CrÃ©ation de l'instance de configuration\n",
        "cfg = Config()\n",
        "\n",
        "# Setup automatique des dossiers (toujours safe)\n",
        "cfg.setup_directories()\n",
        "cfg.display_config()\n",
        "\n",
        "# VÃ©rification des fichiers selon l'environnement\n",
        "if cfg.USING_COLAB:\n",
        "    print(f\"\\nğŸ”¬ Mode Colab dÃ©tectÃ©:\")\n",
        "    print(f\"   ğŸ’¡ Les dossiers sont crÃ©Ã©s, vous pouvez maintenant uploader vos fichiers\")\n",
        "    print(f\"   ğŸ“¤ Uploadez dans: {cfg.videos_dir}\")\n",
        "    print(f\"   ğŸ“„ Fichiers attendus:\")\n",
        "    print(f\"      â€¢ {cfg.video_path.name}\")\n",
        "    print(f\"      â€¢ {cfg.config_path.name}\")\n",
        "    \n",
        "    # VÃ©rification simple sans erreur\n",
        "    cfg.check_files_exist()\n",
        "    print(f\"   ğŸ’¡ Utilisez cfg.wait_for_files() quand les uploads sont terminÃ©s\")\n",
        "else:\n",
        "    print(f\"\\nğŸ–¥ï¸ Mode Local dÃ©tectÃ©:\")\n",
        "    # Validation immÃ©diate en local\n",
        "    cfg.validate_files_now()\n",
        "\n",
        "print(\"\\nâœ… Configuration centralisÃ©e initialisÃ©e!\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ¯ Configuration de la Segmentation VidÃ©o\n",
        "\n",
        "## Mode Segmentation\n",
        "\n",
        "Pour activer le mode segmentation et traiter seulement une partie de votre vidÃ©o, modifiez la configuration dans la cellule prÃ©cÃ©dente :\n",
        "\n",
        "```python\n",
        "# ğŸ¯ SEGMENTATION VIDÃ‰O (MODIFIEZ CES VALEURS)\n",
        "SEGMENT_MODE = True                           # âœ… Activez ici !\n",
        "\n",
        "# ğŸ• OFFSETS EN SECONDES (RECOMMANDÃ‰ - PLUS SIMPLE)\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 5.0          # 5 secondes AVANT la frame de rÃ©fÃ©rence  \n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 10.0          # 10 secondes APRÃˆS la frame de rÃ©fÃ©rence\n",
        "\n",
        "# ğŸ¬ OFFSETS EN FRAMES (OPTIONNEL - CALCULÃ‰ AUTOMATIQUEMENT)\n",
        "SEGMENT_OFFSET_BEFORE = None                 # CalculÃ© depuis les secondes\n",
        "SEGMENT_OFFSET_AFTER = None                  # CalculÃ© depuis les secondes\n",
        "```\n",
        "\n",
        "## ğŸ†• NouveautÃ© : Offsets en Secondes\n",
        "\n",
        "Le systÃ¨me **privilÃ©gie maintenant les offsets en secondes** car c'est plus intuitif et universel !\n",
        "\n",
        "### âœ… **Avec les secondes** (recommandÃ©)\n",
        "```python\n",
        "# Simple et universel\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 3.0          # 3 secondes avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 5.0           # 5 secondes aprÃ¨s\n",
        "\n",
        "# âœ… Conversion automatique selon le FPS de votre vidÃ©o :\n",
        "# VidÃ©o 25 FPS â†’ 3s = 75 frames, 5s = 125 frames\n",
        "# VidÃ©o 30 FPS â†’ 3s = 90 frames, 5s = 150 frames\n",
        "```\n",
        "\n",
        "### âš ï¸ **Avec les frames** (optionnel)\n",
        "```python\n",
        "# Plus technique, dÃ©pend du FPS\n",
        "SEGMENT_OFFSET_BEFORE = 75                   # 75 frames avant\n",
        "SEGMENT_OFFSET_AFTER = 125                   # 125 frames aprÃ¨s\n",
        "```\n",
        "\n",
        "### Comment Ã§a fonctionne :\n",
        "\n",
        "1. **Frame de rÃ©fÃ©rence** : La frame avec votre annotation initiale (dÃ©finie dans le fichier config JSON)\n",
        "2. **OFFSET_BEFORE_SECONDS** : DurÃ©e en secondes Ã  inclure AVANT la frame de rÃ©fÃ©rence\n",
        "3. **OFFSET_AFTER_SECONDS** : DurÃ©e en secondes Ã  inclure APRÃˆS la frame de rÃ©fÃ©rence\n",
        "4. **Conversion automatique** : Le systÃ¨me dÃ©tecte le FPS et convertit automatiquement\n",
        "\n",
        "### Exemple concret :\n",
        "- Frame de rÃ©fÃ©rence : 1000 (Ã  40s dans une vidÃ©o 25 FPS)\n",
        "- OFFSET_BEFORE_SECONDS : 6.0 â†’ 6s Ã— 25 FPS = 150 frames â†’ Segment commence Ã  850 (34s)\n",
        "- OFFSET_AFTER_SECONDS : 12.0 â†’ 12s Ã— 25 FPS = 300 frames â†’ Segment finit Ã  1300 (52s)\n",
        "- **RÃ©sultat** : Segment de 18 secondes (450 frames) au lieu de toute la vidÃ©o\n",
        "\n",
        "## ğŸ¯ Cas d'usage pratiques\n",
        "\n",
        "### ğŸƒâ€â™‚ï¸ **Actions rapides** (3-6 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 1.5          # 1.5s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 1.5           # 1.5s aprÃ¨s\n",
        "```\n",
        "\n",
        "### âš½ **Actions moyennes** (10-20 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 5.0          # 5s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 5.0           # 5s aprÃ¨s\n",
        "```\n",
        "\n",
        "### ğŸ¯ **SÃ©quences longues** (30-60 secondes)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 15.0         # 15s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 15.0          # 15s aprÃ¨s\n",
        "```\n",
        "\n",
        "### ğŸ”¬ **Tests et debug** (courts segments)\n",
        "```python\n",
        "SEGMENT_OFFSET_BEFORE_SECONDS = 2.0          # 2s avant\n",
        "SEGMENT_OFFSET_AFTER_SECONDS = 2.0           # 2s aprÃ¨s\n",
        "```\n",
        "\n",
        "### Avantages du mode segmentation :\n",
        "- âš¡ **Plus rapide** : Traite seulement la partie pertinente de la vidÃ©o\n",
        "- ğŸ’¾ **Ã‰conomise l'espace** : Moins de frames extraites et stockÃ©es  \n",
        "- ğŸ¯ **Plus prÃ©cis** : Focus sur la zone d'intÃ©rÃªt autour de l'annotation\n",
        "- ğŸ• **Intuitif** : Configuration en secondes plutÃ´t qu'en frames techniques\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¡ Astuce :** Modifiez la configuration ci-dessus, puis relancez la cellule de configuration pour voir les nouvelles informations d'offset s'afficher !\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– VÃ©rification et setup SAM2...\n",
            "   ğŸ“¦ SAM2 installÃ©: âœ…\n",
            "   ğŸ’¾ Checkpoint disponible: âœ… ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "ğŸ–¥ï¸ Mode local dÃ©tectÃ©\n",
            "âœ… SAM2 prÃªt en mode local\n",
            "âœ… Setup SAM2 terminÃ©\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ”§ INSTALLATION ET SETUP SAM2 (AUTO-INSTALL COLAB)\n",
        "# =============================================================================\n",
        "\n",
        "def is_sam2_installed():\n",
        "    \"\"\"VÃ©rifie si SAM 2 est dÃ©jÃ  installÃ©\"\"\"\n",
        "    try:\n",
        "        import sam2\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def install_sam2_colab(cfg):\n",
        "    \"\"\"Installation complÃ¨te de SAM2 sur Colab avec chemins de la config\"\"\"\n",
        "    print(\"ğŸ”§ Installation de SAM 2 en cours...\")\n",
        "    \n",
        "    # Packages de base\n",
        "    print(\"   ğŸ“¦ Installation des dÃ©pendances...\")\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
        "    \n",
        "    # CrÃ©ation du dossier checkpoints (utilise la config)\n",
        "    checkpoint_dir = cfg.checkpoint_path.parent\n",
        "    print(f\"   ğŸ“ CrÃ©ation du dossier checkpoints: {checkpoint_dir}\")\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # TÃ©lÃ©chargement du modÃ¨le configurÃ©\n",
        "    checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything_2/092824\"\n",
        "    model_file = cfg.SAM2_CHECKPOINT\n",
        "    \n",
        "    print(f\"   â¬‡ï¸ TÃ©lÃ©chargement du modÃ¨le: {model_file}\")\n",
        "    !wget -P {checkpoint_dir} -q {checkpoint_url}/{model_file}\n",
        "    \n",
        "    # Optionnel: tÃ©lÃ©charger l'autre modÃ¨le si besoin\n",
        "    if \"large\" in model_file:\n",
        "        other_model = \"sam2.1_hiera_small.pt\"\n",
        "        print(f\"   ğŸ“¦ ModÃ¨le small Ã©galement disponible: {other_model}\")\n",
        "        # !wget -P {checkpoint_dir} -q {checkpoint_url}/{other_model}\n",
        "    else:\n",
        "        other_model = \"sam2.1_hiera_large.pt\"\n",
        "        print(f\"   ğŸ“¦ ModÃ¨le large Ã©galement disponible: {other_model}\")\n",
        "        # !wget -P {checkpoint_dir} -q {checkpoint_url}/{other_model}\n",
        "    \n",
        "    # Nettoyage Colab\n",
        "    if cfg.USING_COLAB:\n",
        "        print(\"   ğŸ§¹ Nettoyage des fichiers temporaires Colab...\")\n",
        "        !rm -rf /content/sample_data/* 2>/dev/null || true\n",
        "    \n",
        "    # Nettoyage mÃ©moire\n",
        "    import gc\n",
        "    import torch\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"   ğŸ”¥ MÃ©moire GPU nettoyÃ©e\")\n",
        "    \n",
        "    print(\"âœ… Installation de SAM 2 terminÃ©e\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ AUTO-SETUP SAM2 INTELLIGENT\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ğŸ¤– VÃ©rification et setup SAM2...\")\n",
        "\n",
        "# VÃ©rifications\n",
        "sam2_installed = is_sam2_installed()\n",
        "checkpoint_available = cfg.checkpoint_path.exists()\n",
        "\n",
        "print(f\"   ğŸ“¦ SAM2 installÃ©: {'âœ…' if sam2_installed else 'âŒ'}\")\n",
        "print(f\"   ğŸ’¾ Checkpoint disponible: {'âœ…' if checkpoint_available else 'âŒ'} {cfg.checkpoint_path}\")\n",
        "\n",
        "# Logique d'installation\n",
        "if cfg.USING_COLAB and (not sam2_installed or not checkpoint_available):\n",
        "    print(f\"ğŸ”¬ Colab dÃ©tectÃ© - Installation automatique...\")\n",
        "    install_sam2_colab(cfg)\n",
        "    \n",
        "elif cfg.USING_COLAB and sam2_installed and checkpoint_available:\n",
        "    print(\"âœ… SAM 2 dÃ©jÃ  installÃ© sur Colab - SKIP installation\")\n",
        "    \n",
        "elif not cfg.USING_COLAB:\n",
        "    print(\"ğŸ–¥ï¸ Mode local dÃ©tectÃ©\")\n",
        "    if not sam2_installed:\n",
        "        print(\"   âš ï¸ SAM2 non installÃ©. Installez avec: pip install git+https://github.com/facebookresearch/sam2.git\")\n",
        "    if not checkpoint_available:\n",
        "        print(f\"   âš ï¸ Checkpoint manquant: {cfg.checkpoint_path}\")\n",
        "        print(\"   ğŸ’¡ TÃ©lÃ©chargez depuis: https://github.com/facebookresearch/sam2#download-checkpoints\")\n",
        "    \n",
        "    if sam2_installed and checkpoint_available:\n",
        "        print(\"âœ… SAM2 prÃªt en mode local\")\n",
        "\n",
        "print(\"âœ… Setup SAM2 terminÃ©\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k83nNbwoAdf1",
        "outputId": "4f54ec69-dede-40eb-ff63-cac024309b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Configuration de l'environnement PyTorch...\n",
            "   ğŸ“¦ PyTorch version: 2.5.1+cu121\n",
            "   ğŸ“¦ Torchvision version: 0.20.1+cu121\n",
            "   ğŸ”¥ CUDA disponible: NVIDIA GeForce GTX 1650 with Max-Q Design (4.0GB)\n",
            "   âš¡ Optimisations activÃ©es: Autocast bfloat16, cuDNN benchmark\n",
            "âœ… Device ajoutÃ© Ã  la configuration: cfg.device = cuda\n",
            "\n",
            "âœ… Environnement PyTorch configurÃ© et optimisÃ©!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ“¦ IMPORTS ET CONFIGURATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "# ==================== IMPORTS SYSTÃˆME ====================\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import uuid\n",
        "import base64\n",
        "\n",
        "# ==================== IMPORTS SCIENTIFIQUES ====================\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==================== IMPORTS DEEP LEARNING ====================\n",
        "import torch\n",
        "import torchvision\n",
        "from pycocotools.mask import encode as encode_rle\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸ–¥ï¸ CONFIGURATION AUTOMATIQUE DU DEVICE ET OPTIMISATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def setup_torch_environment(verbose=True):\n",
        "    \"\"\"Configure automatiquement l'environnement PyTorch optimal\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"ğŸ”§ Configuration de l'environnement PyTorch...\")\n",
        "        print(f\"   ğŸ“¦ PyTorch version: {torch.__version__}\")\n",
        "        print(f\"   ğŸ“¦ Torchvision version: {torchvision.__version__}\")\n",
        "    \n",
        "    # === SÃ‰LECTION DU DEVICE ===\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        if verbose:\n",
        "            print(f\"   ğŸ”¥ CUDA disponible: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        if verbose:\n",
        "            print(f\"   ğŸ MPS (Apple Silicon) disponible\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        if verbose:\n",
        "            print(f\"   ğŸ’» CPU seulement\")\n",
        "    \n",
        "    # === OPTIMISATIONS CUDA ===\n",
        "    optimizations_applied = []\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        # Autocast pour Ã©conomiser la mÃ©moire\n",
        "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "        optimizations_applied.append(\"Autocast bfloat16\")\n",
        "        \n",
        "        # Optimisations TensorFloat-32 (si GPU rÃ©cent)\n",
        "        gpu_compute_capability = torch.cuda.get_device_properties(0).major\n",
        "        if gpu_compute_capability >= 8:  # Ampere et plus rÃ©cent\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "            optimizations_applied.append(\"TensorFloat-32\")\n",
        "        \n",
        "        # Optimisations mÃ©moire additionnelles\n",
        "        torch.backends.cudnn.benchmark = True  # Optimise pour tailles fixes\n",
        "        optimizations_applied.append(\"cuDNN benchmark\")\n",
        "    \n",
        "    if verbose and optimizations_applied:\n",
        "        print(f\"   âš¡ Optimisations activÃ©es: {', '.join(optimizations_applied)}\")\n",
        "    \n",
        "    return device, optimizations_applied\n",
        "\n",
        "def display_system_info(device):\n",
        "    \"\"\"Affiche les informations systÃ¨me dÃ©taillÃ©es\"\"\"\n",
        "    print(f\"\\nğŸ“Š INFORMATIONS SYSTÃˆME:\")\n",
        "    print(f\"   ğŸ–¥ï¸  Device principal: {device}\")\n",
        "    \n",
        "    if device.type == \"cuda\":\n",
        "        print(f\"   ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   ğŸ’¾ MÃ©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "        print(f\"   ğŸ§® Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
        "    \n",
        "    # CPU Info\n",
        "    import platform\n",
        "    print(f\"   ğŸ’» CPU: {platform.processor()}\")\n",
        "    print(f\"   ğŸ§  Threads: {torch.get_num_threads()}\")\n",
        "    \n",
        "    # Versions importantes\n",
        "    print(f\"   ğŸ Python: {platform.python_version()}\")\n",
        "    print(f\"   ğŸ“¦ NumPy: {np.__version__}\")\n",
        "    print(f\"   ğŸ¥ OpenCV: {cv2.__version__}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ INITIALISATION ENVIRONNEMENT\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration automatique\n",
        "device, optimizations = setup_torch_environment(verbose=True)\n",
        "\n",
        "# Ajout du device Ã  notre configuration centralisÃ©e\n",
        "if 'cfg' in globals():\n",
        "    cfg.device = device\n",
        "    cfg.torch_optimizations = optimizations\n",
        "    print(f\"âœ… Device ajoutÃ© Ã  la configuration: cfg.device = {device}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Configuration cfg non trouvÃ©e - device disponible en tant que variable globale\")\n",
        "\n",
        "# Affichage des informations systÃ¨me (optionnel, dÃ©commentez si besoin)\n",
        "# display_system_info(device)\n",
        "\n",
        "# Nettoyage initial de la mÃ©moire\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nâœ… Environnement PyTorch configurÃ© et optimisÃ©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¥ï¸ Mode local: chargement immÃ©diat\n",
            "ğŸ“„ Chargement de la configuration projet: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1_config.json\n",
            "âœ… Configuration projet chargÃ©e et validÃ©e:\n",
            "   ğŸ“· Calibration camÃ©ra: âœ… OK\n",
            "   ğŸ¯ Objets dÃ©finis: 13\n",
            "   ğŸ“ Annotations initiales: 13 sur 1 frames\n",
            "   ğŸ·ï¸  Types d'objets:\n",
            "      â€¢ player: 12 ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
            "      â€¢ ball: 1 ([13])\n",
            "\n",
            "âœ… Configuration projet prÃªte Ã  l'usage!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ“„ CHARGEMENT ET VALIDATION DE LA CONFIGURATION JSON\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_validate_project_config(config_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"Charge et valide le fichier de configuration JSON du projet.\"\"\"\n",
        "    print(f\"ğŸ“„ Chargement de la configuration projet: {config_path}\")\n",
        "\n",
        "    # VÃ©rification existence du fichier\n",
        "    if not config_path.exists():\n",
        "        raise FileNotFoundError(f\"âŒ Fichier config non trouvÃ©: {config_path}\")\n",
        "\n",
        "    try:\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            config = json.load(f)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"âŒ Erreur JSON dans {config_path}: {e}\")\n",
        "\n",
        "    # === VALIDATION DE LA STRUCTURE ===\n",
        "    required_sections = ['calibration', 'objects', 'initial_annotations']\n",
        "    missing_sections = [section for section in required_sections if section not in config]\n",
        "    \n",
        "    if missing_sections:\n",
        "        raise ValueError(f\"âŒ Sections manquantes dans le config: {missing_sections}\")\n",
        "\n",
        "    # === VALIDATION DES DONNÃ‰ES ===\n",
        "    \n",
        "    # Validation calibration\n",
        "    if 'camera_parameters' not in config['calibration']:\n",
        "        raise ValueError(\"âŒ 'camera_parameters' manquant dans la calibration\")\n",
        "    \n",
        "    # Validation objets\n",
        "    if not config['objects']:\n",
        "        raise ValueError(\"âŒ Aucun objet dÃ©fini dans la configuration\")\n",
        "    \n",
        "    # Validation annotations initiales\n",
        "    if not config['initial_annotations']:\n",
        "        raise ValueError(\"âŒ Aucune annotation initiale dÃ©finie\")\n",
        "\n",
        "    # === STATISTIQUES ET RÃ‰SUMÃ‰ ===\n",
        "    num_objects = len(config['objects'])\n",
        "    \n",
        "    # Comptage des annotations\n",
        "    total_annotations = 0\n",
        "    annotation_frames = set()\n",
        "    for frame_data in config['initial_annotations']:\n",
        "        frame_idx = frame_data.get('frame')\n",
        "        annotations = frame_data.get('annotations', [])\n",
        "        total_annotations += len(annotations)\n",
        "        annotation_frames.add(frame_idx)\n",
        "\n",
        "    # Types d'objets\n",
        "    obj_types = {}\n",
        "    obj_by_type = {}\n",
        "    for obj in config['objects']:\n",
        "        obj_type = obj.get('obj_type', 'unknown')\n",
        "        obj_types[obj_type] = obj_types.get(obj_type, 0) + 1\n",
        "        if obj_type not in obj_by_type:\n",
        "            obj_by_type[obj_type] = []\n",
        "        obj_by_type[obj_type].append(obj.get('obj_id', 'no_id'))\n",
        "\n",
        "    print(f\"âœ… Configuration projet chargÃ©e et validÃ©e:\")\n",
        "    print(f\"   ğŸ“· Calibration camÃ©ra: âœ… OK\")\n",
        "    print(f\"   ğŸ¯ Objets dÃ©finis: {num_objects}\")\n",
        "    print(f\"   ğŸ“ Annotations initiales: {total_annotations} sur {len(annotation_frames)} frames\")\n",
        "    print(f\"   ğŸ·ï¸  Types d'objets:\")\n",
        "    for obj_type, count in obj_types.items():\n",
        "        ids = obj_by_type[obj_type]\n",
        "        print(f\"      â€¢ {obj_type}: {count} ({ids})\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ CHARGEMENT DE LA CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Chargement avec gestion intelligente Colab/Local\n",
        "if cfg.USING_COLAB:\n",
        "    print(\"ğŸ”¬ Mode Colab dÃ©tectÃ© pour le chargement config\")\n",
        "    if cfg.config_path.exists():\n",
        "        project_config = load_and_validate_project_config(cfg.config_path)\n",
        "    else:\n",
        "        print(f\"â³ Fichier config pas encore uploadÃ©, utilisez:\")\n",
        "        print(f\"   project_config = wait_and_load_config(cfg)\")\n",
        "        project_config = None\n",
        "else:\n",
        "    print(\"ğŸ–¥ï¸ Mode local: chargement immÃ©diat\")\n",
        "    project_config = load_and_validate_project_config(cfg.config_path)\n",
        "\n",
        "if project_config:\n",
        "    print(\"\\nâœ… Configuration projet prÃªte Ã  l'usage!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fonctions de segmentation vidÃ©o chargÃ©es!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¯ FONCTIONS DE SEGMENTATION VIDÃ‰O\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_segment_bounds(reference_frame: int, offset_before: int, offset_after: int,\n",
        "                           total_frames: int, frame_interval: int = 1) -> tuple:\n",
        "    \"\"\"\n",
        "    Calcule les bornes du segment vidÃ©o Ã  traiter.\n",
        "    \n",
        "    Args:\n",
        "        reference_frame: Frame de rÃ©fÃ©rence (celle avec l'annotation initiale)\n",
        "        offset_before: Nombre de frames Ã  prendre AVANT la frame de rÃ©fÃ©rence\n",
        "        offset_after: Nombre de frames Ã  prendre APRÃˆS la frame de rÃ©fÃ©rence\n",
        "        total_frames: Nombre total de frames dans la vidÃ©o\n",
        "        frame_interval: Intervalle entre les frames traitÃ©es\n",
        "    \n",
        "    Returns:\n",
        "        (start_frame, end_frame, processed_start_idx, processed_end_idx)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calcul des bornes en frames originales\n",
        "    start_frame = max(0, reference_frame - offset_before)\n",
        "    end_frame = min(total_frames - 1, reference_frame + offset_after)\n",
        "    \n",
        "    # Conversion en indices de frames traitÃ©es\n",
        "    processed_start_idx = start_frame // frame_interval\n",
        "    processed_end_idx = end_frame // frame_interval\n",
        "    \n",
        "    print(f\"ğŸ¯ CALCUL DES BORNES DE SEGMENTATION:\")\n",
        "    print(f\"   ğŸ“ Frame de rÃ©fÃ©rence: {reference_frame}\")\n",
        "    print(f\"   ğŸ“‰ Offset avant: {offset_before} frames\")\n",
        "    print(f\"   ğŸ“ˆ Offset aprÃ¨s: {offset_after} frames\")\n",
        "    print(f\"   ğŸ¬ Segment original: frames {start_frame} Ã  {end_frame}\")\n",
        "    print(f\"   ğŸ¬ Segment traitÃ©: indices {processed_start_idx} Ã  {processed_end_idx}\")\n",
        "    print(f\"   ğŸ“Š Nombre de frames Ã  traiter: {processed_end_idx - processed_start_idx + 1}\")\n",
        "    \n",
        "    return start_frame, end_frame, processed_start_idx, processed_end_idx\n",
        "\n",
        "def extract_segment_frames(video_path, frames_dir, start_frame: int, end_frame: int,\n",
        "                         frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"\n",
        "    Extrait seulement les frames du segment spÃ©cifiÃ© avec nommage sÃ©quentiel.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Chemin vers la vidÃ©o\n",
        "        frames_dir: Dossier de destination\n",
        "        start_frame: Frame de dÃ©but\n",
        "        end_frame: Frame de fin\n",
        "        frame_interval: Intervalle entre frames\n",
        "        force_extraction: Force la rÃ©-extraction\n",
        "    \n",
        "    Returns:\n",
        "        Nombre de frames extraites\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"ğŸ¬ EXTRACTION DU SEGMENT:\")\n",
        "    print(f\"   ğŸ“¹ VidÃ©o: {video_path}\")\n",
        "    print(f\"   ğŸ“ Destination: {frames_dir}\")\n",
        "    print(f\"   ğŸ¯ Segment: frames {start_frame} Ã  {end_frame}\")\n",
        "    print(f\"   â¯ï¸  Intervalle: {frame_interval}\")    \n",
        "    \n",
        "    # âœ… CORRECTION: Calculer les frames attendues avec nommage sÃ©quentiel\n",
        "    expected_frames = []\n",
        "    sequential_idx = 0  # â† Compteur sÃ©quentiel pour le nommage\n",
        "    \n",
        "    for frame_idx in range(start_frame, end_frame + 1, frame_interval):\n",
        "        expected_frames.append((frame_idx, sequential_idx))  # (frame_originale, index_sÃ©quentiel)\n",
        "        sequential_idx += 1\n",
        "    \n",
        "    # VÃ©rifier si extraction dÃ©jÃ  faite avec le nouveau nommage\n",
        "    existing_frames = []\n",
        "    for frame_idx, seq_idx in expected_frames:\n",
        "        filename = frames_dir / f\"{seq_idx:05d}.jpg\"  # â† Utiliser l'index sÃ©quentiel\n",
        "        if filename.exists():\n",
        "            existing_frames.append(filename)\n",
        "    \n",
        "    if len(existing_frames) == len(expected_frames) and not force_extraction:\n",
        "        print(f\"ğŸ“‚ {len(existing_frames)} frames du segment dÃ©jÃ  extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"ğŸ”„ {len(existing_frames)} frames existantes - SUPPRESSION et rÃ©-extraction...\")\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"ğŸ—‘ï¸  Frames existantes supprimÃ©es\")\n",
        "    \n",
        "    # Extraction\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"âŒ Impossible d'ouvrir la vidÃ©o: {video_path}\")\n",
        "    \n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    \n",
        "    print(f\"ğŸ“Š VidÃ©o: {total_frames} frames total, {fps:.1f} FPS\")\n",
        "    print(f\"ğŸ“Š Nommage: sÃ©quentiel de 00000.jpg Ã  {len(expected_frames)-1:05d}.jpg\")\n",
        "    \n",
        "    extracted_count = 0\n",
        "    \n",
        "    try:\n",
        "        # Positionner Ã  la frame de dÃ©but\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        \n",
        "        # âœ… CORRECTION: Utiliser le mapping (frame_originale, index_sÃ©quentiel)\n",
        "        for frame_idx in range(start_frame, end_frame + 1):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            # Extraire selon l'intervalle\n",
        "            if (frame_idx - start_frame) % frame_interval == 0:\n",
        "                # Trouver l'index sÃ©quentiel pour cette frame\n",
        "                sequential_idx = (frame_idx - start_frame) // frame_interval\n",
        "                filename = frames_dir / f\"{sequential_idx:05d}.jpg\"  # â† Nommage sÃ©quentiel\n",
        "                \n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "                \n",
        "                if extracted_count % 10 == 0:\n",
        "                    progress = ((frame_idx - start_frame) / (end_frame - start_frame)) * 100\n",
        "                    print(f\"ğŸ“Š ProgrÃ¨s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "    \n",
        "    finally:\n",
        "        cap.release()\n",
        "    \n",
        "    print(f\"âœ… {extracted_count} frames du segment extraites\")\n",
        "    print(f\"ğŸ“‹ Nommage: 00000.jpg Ã  {extracted_count-1:05d}.jpg\")\n",
        "    return extracted_count\n",
        "\n",
        "print(\"âœ… Fonctions de segmentation vidÃ©o chargÃ©es!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¬ DÃ©marrage de l'extraction des frames...\n",
            "ğŸ–¥ï¸ Mode Local: Extraction directe\n",
            "ğŸ¯ MODE SEGMENTATION ACTIVÃ‰\n",
            "ğŸ¯ CALCUL DES BORNES DE SEGMENTATION:\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "   ğŸ“‰ Offset avant: 50 frames\n",
            "   ğŸ“ˆ Offset aprÃ¨s: 50 frames\n",
            "   ğŸ¬ Segment original: frames 230 Ã  330\n",
            "   ğŸ¬ Segment traitÃ©: indices 76 Ã  110\n",
            "   ğŸ“Š Nombre de frames Ã  traiter: 35\n",
            "ğŸ¬ EXTRACTION DU SEGMENT:\n",
            "   ğŸ“¹ VidÃ©o: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1.mp4\n",
            "   ğŸ“ Destination: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   ğŸ¯ Segment: frames 230 Ã  330\n",
            "   â¯ï¸  Intervalle: 3\n",
            "ğŸ”„ 34 frames existantes - SUPPRESSION et rÃ©-extraction...\n",
            "ğŸ—‘ï¸  Frames existantes supprimÃ©es\n",
            "ğŸ“Š VidÃ©o: 624 frames total, 25.0 FPS\n",
            "ğŸ“Š Nommage: sÃ©quentiel de 00000.jpg Ã  00033.jpg\n",
            "ğŸ“Š ProgrÃ¨s: 10 frames extraites (27.0%)\n",
            "ğŸ“Š ProgrÃ¨s: 20 frames extraites (57.0%)\n",
            "ğŸ“Š ProgrÃ¨s: 30 frames extraites (87.0%)\n",
            "âœ… 34 frames du segment extraites\n",
            "ğŸ“‹ Nommage: 00000.jpg Ã  00033.jpg\n",
            "\n",
            "ğŸ¯ SEGMENTATION TERMINÃ‰E:\n",
            "   ğŸ“Š Frames du segment: 101\n",
            "   ğŸ¬ Plage: 230 Ã  330\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "   ğŸ“Š Frames extraites: 34\n",
            "\n",
            "ğŸ“Š RÃ‰SUMÃ‰ EXTRACTION:\n",
            "   ğŸ¬ Frames extraites/comptÃ©es: 34\n",
            "   ğŸ“ Dossier: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   âœ… cfg.extracted_frames_count = 34\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¬ EXTRACTION DES FRAMES\n",
        "# =============================================================================\n",
        "\n",
        "def extract_frames(video_path: Path, frames_dir: Path, frame_interval: int = 1, force_extraction: bool = False) -> int:\n",
        "    \"\"\"Extrait les frames de la vidÃ©o selon l'intervalle spÃ©cifiÃ©.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ¬ Extraction des frames...\")\n",
        "    print(f\"   ğŸ“¹ Source: {video_path}\")\n",
        "    print(f\"   ğŸ“ Destination: {frames_dir}\")\n",
        "    print(f\"   â¯ï¸  Intervalle: {frame_interval}\")\n",
        "    print(f\"   ğŸ”„ Force extraction: {'âœ… Oui' if force_extraction else 'âŒ Non'}\")\n",
        "\n",
        "    # VÃ©rification existence du fichier vidÃ©o\n",
        "    if not video_path.exists():\n",
        "        raise FileNotFoundError(f\"âŒ VidÃ©o non trouvÃ©e: {video_path}\")\n",
        "\n",
        "    # VÃ©rification si extraction dÃ©jÃ  faite\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    if existing_frames and not force_extraction:\n",
        "        print(f\"ğŸ“‚ {len(existing_frames)} frames dÃ©jÃ  extraites - SKIP\")\n",
        "        return len(existing_frames)\n",
        "    elif existing_frames and force_extraction:\n",
        "        print(f\"ğŸ”„ {len(existing_frames)} frames existantes - SUPPRESSION et rÃ©-extraction...\")\n",
        "        # Supprimer les frames existantes\n",
        "        for frame_file in existing_frames:\n",
        "            frame_file.unlink()\n",
        "        print(f\"ğŸ—‘ï¸  Frames existantes supprimÃ©es\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"âŒ Impossible d'ouvrir la vidÃ©o: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"ğŸ“Š VidÃ©o: {total_frames} frames, {fps:.1f} FPS\")\n",
        "    print(f\"ğŸ“Š Frames Ã  extraire: ~{total_frames // frame_interval}\")\n",
        "\n",
        "    extracted_count = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Extraire seulement selon l'intervalle\n",
        "            if frame_idx % frame_interval == 0:\n",
        "                output_idx = frame_idx // frame_interval\n",
        "                filename = frames_dir / f\"{output_idx:05d}.jpg\"\n",
        "                cv2.imwrite(str(filename), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                extracted_count += 1\n",
        "\n",
        "                if extracted_count % 50 == 0:\n",
        "                    progress = (frame_idx / total_frames) * 100\n",
        "                    print(f\"ğŸ“Š ProgrÃ¨s: {extracted_count} frames extraites ({progress:.1f}%)\")\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"âœ… {extracted_count} frames extraites\")\n",
        "    return extracted_count\n",
        "\n",
        "def count_existing_frames(frames_dir: Path) -> int:\n",
        "    \"\"\"Compte les frames existantes dans le dossier\"\"\"\n",
        "    existing_frames = list(frames_dir.glob(\"*.jpg\"))\n",
        "    return len(existing_frames)\n",
        "\n",
        "def extract_frames_with_config(cfg) -> int:\n",
        "    \"\"\"Extrait les frames en utilisant la configuration centralisÃ©e avec support segmentation\"\"\"\n",
        "    \n",
        "    # VÃ©rification des fichiers (gestion Colab/Local)\n",
        "    if cfg.USING_COLAB and not cfg.video_path.exists():\n",
        "        print(\"ğŸ”¬ Mode Colab: VidÃ©o pas encore uploadÃ©e\")\n",
        "        print(f\"   ğŸ’¡ Uploadez {cfg.video_path.name} dans {cfg.videos_dir}\")\n",
        "        print(f\"   ğŸ’¡ Puis relancez cette cellule ou utilisez cfg.wait_for_files()\")\n",
        "        return 0\n",
        "    \n",
        "    # VÃ©rification si extraction activÃ©e\n",
        "    if not cfg.EXTRACT_FRAMES:\n",
        "        existing_count = count_existing_frames(cfg.frames_dir)\n",
        "        print(f\"â­ï¸  Extraction dÃ©sactivÃ©e (cfg.EXTRACT_FRAMES = False)\")\n",
        "        if existing_count > 0:\n",
        "            print(f\"ğŸ“‚ Utilisation de {existing_count} frames existantes\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Aucune frame trouvÃ©e dans {cfg.frames_dir}\")\n",
        "            print(f\"ğŸ’¡ Conseil: Activez cfg.EXTRACT_FRAMES = True pour extraire\")\n",
        "        return existing_count\n",
        "    \n",
        "    # DÃ©cider du mode d'extraction : segmentation ou complet\n",
        "    if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "        print(\"ğŸ¯ MODE SEGMENTATION ACTIVÃ‰\")\n",
        "        \n",
        "        # VÃ©rifier si project_config est disponible\n",
        "        if 'project_config' not in globals() or project_config is None:\n",
        "            print(\"âŒ project_config non disponible pour la segmentation\")\n",
        "            print(\"ğŸ’¡ Chargez d'abord la configuration projet ou utilisez le mode complet\")\n",
        "            print(\"ğŸ”„ Utilisation du mode complet par dÃ©faut...\")\n",
        "            return extract_frames(\n",
        "                video_path=cfg.video_path,\n",
        "                frames_dir=cfg.frames_dir,\n",
        "                frame_interval=cfg.FRAME_INTERVAL,\n",
        "                force_extraction=cfg.FORCE_EXTRACTION\n",
        "            )\n",
        "        \n",
        "        # RÃ©cupÃ©rer la frame de rÃ©fÃ©rence\n",
        "        reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Obtenir les informations vidÃ©o\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        \n",
        "        # âœ… AMÃ‰LIORATION: Utiliser les offsets calculÃ©s depuis les secondes\n",
        "        offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "        \n",
        "        # Calculer les bornes du segment\n",
        "        start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "            reference_frame=reference_frame,\n",
        "            offset_before=offset_before_frames,\n",
        "            offset_after=offset_after_frames,\n",
        "            total_frames=total_frames,\n",
        "            frame_interval=cfg.FRAME_INTERVAL\n",
        "        )\n",
        "        \n",
        "        # Extraire les frames du segment\n",
        "        extracted_count = extract_segment_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            start_frame=start_frame,\n",
        "            end_frame=end_frame,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nğŸ¯ SEGMENTATION TERMINÃ‰E:\")\n",
        "        print(f\"   ğŸ“Š Frames du segment: {end_frame - start_frame + 1}\")\n",
        "        print(f\"   ğŸ¬ Plage: {start_frame} Ã  {end_frame}\")\n",
        "        print(f\"   ğŸ“ Frame de rÃ©fÃ©rence: {reference_frame}\")\n",
        "        print(f\"   ğŸ“Š Frames extraites: {extracted_count}\")\n",
        "        \n",
        "        return extracted_count\n",
        "        \n",
        "    else:\n",
        "        print(\"ğŸ¬ MODE COMPLET ACTIVÃ‰ (toute la vidÃ©o)\")\n",
        "        return extract_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ EXTRACTION AVEC CONFIGURATION CENTRALISÃ‰E\n",
        "# =============================================================================\n",
        "\n",
        "# Extraction intelligente selon l'environnement\n",
        "print(\"ğŸ¬ DÃ©marrage de l'extraction des frames...\")\n",
        "\n",
        "# VÃ©rification prÃ©requis selon l'environnement\n",
        "if cfg.USING_COLAB:\n",
        "    print(\"ğŸ”¬ Mode Colab: VÃ©rification des fichiers uploadÃ©s...\")\n",
        "    if not cfg.video_path.exists():\n",
        "        print(f\"âš ï¸  VidÃ©o pas encore uploadÃ©e: {cfg.video_path.name}\")\n",
        "        print(f\"   ğŸ“¤ Uploadez dans: {cfg.videos_dir}\")\n",
        "        print(f\"   ğŸ”„ Puis relancez cette cellule\")\n",
        "        extracted_frames_count = 0\n",
        "    else:\n",
        "        extracted_frames_count = extract_frames_with_config(cfg)\n",
        "else:\n",
        "    print(\"ğŸ–¥ï¸ Mode Local: Extraction directe\")\n",
        "    extracted_frames_count = extract_frames_with_config(cfg)\n",
        "\n",
        "# Ajout du rÃ©sultat Ã  la configuration pour usage ultÃ©rieur\n",
        "cfg.extracted_frames_count = extracted_frames_count\n",
        "\n",
        "print(f\"\\nğŸ“Š RÃ‰SUMÃ‰ EXTRACTION:\")\n",
        "print(f\"   ğŸ¬ Frames extraites/comptÃ©es: {extracted_frames_count}\")\n",
        "print(f\"   ğŸ“ Dossier: {cfg.frames_dir}\")\n",
        "print(f\"   âœ… cfg.extracted_frames_count = {extracted_frames_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Initialisation SAM2...\n",
            "   ğŸ§  ModÃ¨le: configs/sam2.1/sam2.1_hiera_l.yaml\n",
            "   ğŸ’¾ Checkpoint: ..\\checkpoints\\sam2.1_hiera_large.pt\n",
            "   ğŸ–¥ï¸  Device: cuda\n",
            "\n",
            "ğŸ¬ Initialisation Ã©tat d'infÃ©rence...\n",
            "   ğŸ“ Frames: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "frame loading (JPEG): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:02<00:00, 16.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… SAM2 initialisÃ©:\n",
            "   ğŸ–¼ï¸  Frames extraites: 34\n",
            "   ğŸ¬ Frames chargÃ©es: 34\n",
            "   âœ… Correspondance: OK\n",
            "   ğŸ’¾ GPU Memory: 1.45GB\n",
            "\n",
            "âœ… SAM2 prÃªt pour l'ajout d'annotations!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¤– INITIALISATION SAM2\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_sam2_predictor(cfg, verbose=True):\n",
        "    \"\"\"Initialise le predictor SAM2 avec la configuration centralisÃ©e\"\"\"\n",
        "    \n",
        "    # Import SAM2\n",
        "    from sam2.build_sam import build_sam2_video_predictor\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"ğŸ¤– Initialisation SAM2...\")\n",
        "        print(f\"   ğŸ§  ModÃ¨le: {cfg.model_config_path}\")\n",
        "        print(f\"   ğŸ’¾ Checkpoint: {cfg.checkpoint_path}\")\n",
        "        print(f\"   ğŸ–¥ï¸  Device: {cfg.device}\")\n",
        "    \n",
        "    # VÃ©rification du checkpoint\n",
        "    if not cfg.checkpoint_path.exists():\n",
        "        raise FileNotFoundError(f\"âŒ Checkpoint SAM2 non trouvÃ©: {cfg.checkpoint_path}\")\n",
        "    \n",
        "    # Construction du predictor\n",
        "    predictor = build_sam2_video_predictor(\n",
        "        config_file=cfg.model_config_path,\n",
        "        ckpt_path=str(cfg.checkpoint_path),\n",
        "        device=cfg.device\n",
        "    )\n",
        "    \n",
        "    return predictor\n",
        "\n",
        "def initialize_inference_state(predictor, cfg, verbose=True):\n",
        "    \"\"\"Initialise l'Ã©tat d'infÃ©rence avec vÃ©rifications\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nğŸ¬ Initialisation Ã©tat d'infÃ©rence...\")\n",
        "        print(f\"   ğŸ“ Frames: {cfg.frames_dir}\")\n",
        "    \n",
        "    # VÃ©rification des frames\n",
        "    if cfg.extracted_frames_count == 0:\n",
        "        raise ValueError(f\"âŒ Aucune frame extraite. Extrayez d'abord les frames.\")\n",
        "    \n",
        "    # Initialisation de l'Ã©tat d'infÃ©rence\n",
        "    inference_state = predictor.init_state(\n",
        "        video_path=str(cfg.frames_dir),\n",
        "        offload_video_to_cpu=True,    # Ã‰conomise la mÃ©moire GPU\n",
        "        offload_state_to_cpu=False    # Garde l'Ã©tat en GPU\n",
        "    )\n",
        "    \n",
        "    # Reset de l'Ã©tat\n",
        "    predictor.reset_state(inference_state)\n",
        "    \n",
        "    # VÃ©rification\n",
        "    loaded_frames = inference_state[\"num_frames\"]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nâœ… SAM2 initialisÃ©:\")\n",
        "        print(f\"   ğŸ–¼ï¸  Frames extraites: {cfg.extracted_frames_count}\")\n",
        "        print(f\"   ğŸ¬ Frames chargÃ©es: {loaded_frames}\")\n",
        "        print(f\"   âœ… Correspondance: {'OK' if cfg.extracted_frames_count == loaded_frames else 'ERREUR'}\")\n",
        "        \n",
        "        if cfg.device.type == \"cuda\":\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            print(f\"   ğŸ’¾ GPU Memory: {allocated:.2f}GB\")\n",
        "    \n",
        "    if cfg.extracted_frames_count != loaded_frames:\n",
        "        print(f\"âš ï¸ IncohÃ©rence frames : {cfg.extracted_frames_count} extraites vs {loaded_frames} chargÃ©es\")\n",
        "    \n",
        "    return inference_state\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ INITIALISATION AVEC CONFIGURATION CENTRALISÃ‰E\n",
        "# =============================================================================\n",
        "\n",
        "# VÃ©rification prÃ©requis\n",
        "if not hasattr(cfg, 'extracted_frames_count') or cfg.extracted_frames_count == 0:\n",
        "    print(\"âŒ Frames non extraites. ExÃ©cutez d'abord la cellule d'extraction des frames.\")\n",
        "    print(\"ğŸ’¡ Ou dÃ©finissez cfg.extracted_frames_count manuellement si frames dÃ©jÃ  prÃ©sentes\")\n",
        "else:\n",
        "    # Initialisation SAM2\n",
        "    predictor = initialize_sam2_predictor(cfg)\n",
        "    inference_state = initialize_inference_state(predictor, cfg)\n",
        "    \n",
        "    # Ajout Ã  la configuration pour usage ultÃ©rieur\n",
        "    cfg.predictor = predictor\n",
        "    cfg.inference_state = inference_state\n",
        "    \n",
        "    print(\"\\nâœ… SAM2 prÃªt pour l'ajout d'annotations!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ CALCUL DES BORNES DE SEGMENTATION:\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "   ğŸ“‰ Offset avant: 50 frames\n",
            "   ğŸ“ˆ Offset aprÃ¨s: 50 frames\n",
            "   ğŸ¬ Segment original: frames 230 Ã  330\n",
            "   ğŸ¬ Segment traitÃ©: indices 76 Ã  110\n",
            "   ğŸ“Š Nombre de frames Ã  traiter: 35\n",
            "ğŸ¯ Informations segmentation prÃ©parÃ©es pour l'ajout d'annotations\n",
            "ğŸ¯ Ajout des annotations initiales...\n",
            "   ğŸ¯ MODE SEGMENTATION:\n",
            "      ğŸ“ Frame originale: 280\n",
            "      ğŸ“ Frame traitÃ©e: 93\n",
            "      ğŸ“ Segment commence Ã : 76 (traitÃ©)\n",
            "      ğŸ“ Index dans le segment: 17\n",
            "   ğŸ“ Frame 280 (original) â†’ 17 (pour SAM2): 13 annotations\n",
            "   ğŸ“Š Total: 13 annotations sur 1 frames\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 1 (player): 1 points Ã  (572, 444)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 2 (player): 1 points Ã  (604, 321)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 3 (player): 1 points Ã  (861, 325)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 4 (player): 1 points Ã  (1324, 328)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 5 (player): 1 points Ã  (1022, 238)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 6 (player): 1 points Ã  (778, 289)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 7 (player): 1 points Ã  (685, 257)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 8 (player): 1 points Ã  (1093, 305)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 9 (player): 1 points Ã  (874, 293)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 10 (player): 1 points Ã  (951, 238)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 11 (player): 1 points Ã  (993, 225)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 12 (player): 1 points Ã  (1199, 219)\n",
            "   ğŸ¯ Frame 280â†’17 - Objet 13 (ball): 1 points Ã  (721, 328)\n",
            "\n",
            "ğŸ“Š RÃ‰SUMÃ‰ ANNOTATIONS:\n",
            "   ğŸ¯ Annotations configurÃ©es: 13\n",
            "   ğŸ¯ Objets uniques: 13\n",
            "   âœ… Objets ajoutÃ©s Ã  SAM2: 13\n",
            "   ğŸ†” IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "   ğŸ¯ Mode segmentation: Frame d'annotation ajustÃ©e Ã  l'index 17 dans le segment\n",
            "   ğŸ·ï¸  Types: {'player': 12, 'ball': 1}\n",
            "\n",
            "âœ… Annotations initiales ajoutÃ©es avec succÃ¨s!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\antoi\\Documents\\Work_Learn\\Stage-Rennes\\EVA2SPORT\\.venv\\lib\\site-packages\\sam2\\sam2_video_predictor.py:786: UserWarning: cannot import name '_C' from 'sam2' (c:\\Users\\antoi\\Documents\\Work_Learn\\Stage-Rennes\\EVA2SPORT\\.venv\\lib\\site-packages\\sam2\\__init__.py)\n",
            "\n",
            "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
            "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ¯ AJOUT DES ANNOTATIONS INITIALES\n",
        "# =============================================================================\n",
        "def add_initial_annotations(predictor, inference_state, project_config: Dict[str, Any], frame_interval: int = 1, segment_info: Dict = None):\n",
        "    \"\"\"Ajoute les annotations initiales depuis la configuration projet avec conversion des frames et support segmentation.\"\"\"\n",
        "\n",
        "    print(f\"ğŸ¯ Ajout des annotations initiales...\")\n",
        "\n",
        "    # CrÃ©ation du mapping obj_id -> obj_type\n",
        "    obj_types = {}\n",
        "    for obj in project_config['objects']:\n",
        "        obj_types[obj['obj_id']] = obj['obj_type']\n",
        "\n",
        "    # Extraction automatique des annotations et frames depuis le JSON\n",
        "    all_annotations = []\n",
        "    annotation_frames = []\n",
        "\n",
        "    for frame_data in project_config['initial_annotations']:\n",
        "        frame_idx_original = frame_data['frame']  # â† Frame originale du JSON\n",
        "        \n",
        "        # âœ… CORRECTION: Conversion avec prise en compte de la segmentation\n",
        "        frame_idx_processed = frame_idx_original // frame_interval\n",
        "        \n",
        "        # ğŸ¯ NOUVEAU: Ajustement pour le mode segmentation\n",
        "        if segment_info:\n",
        "            # Calculer l'offset par rapport au dÃ©but du segment\n",
        "            segment_start_processed = segment_info['start_frame'] // frame_interval\n",
        "            frame_idx_segment = frame_idx_processed - segment_start_processed\n",
        "            \n",
        "            print(f\"   ğŸ¯ MODE SEGMENTATION:\")\n",
        "            print(f\"      ğŸ“ Frame originale: {frame_idx_original}\")\n",
        "            print(f\"      ğŸ“ Frame traitÃ©e: {frame_idx_processed}\")\n",
        "            print(f\"      ğŸ“ Segment commence Ã : {segment_start_processed} (traitÃ©)\")\n",
        "            print(f\"      ğŸ“ Index dans le segment: {frame_idx_segment}\")\n",
        "            \n",
        "            # VÃ©rifier que la frame est dans le segment\n",
        "            segment_end_processed = segment_info['end_frame'] // frame_interval\n",
        "            if frame_idx_processed < segment_start_processed or frame_idx_processed > segment_end_processed:\n",
        "                raise ValueError(f\"âŒ Frame d'annotation {frame_idx_processed} en dehors du segment [{segment_start_processed}, {segment_end_processed}]\")\n",
        "            \n",
        "            # Utiliser l'index dans le segment\n",
        "            frame_idx_for_sam = frame_idx_segment\n",
        "        else:\n",
        "            frame_idx_for_sam = frame_idx_processed\n",
        "        \n",
        "        annotations = frame_data['annotations']\n",
        "        annotation_frames.append(frame_idx_for_sam)\n",
        "\n",
        "        print(f\"   ğŸ“ Frame {frame_idx_original} (original) â†’ {frame_idx_for_sam} (pour SAM2): {len(annotations)} annotations\")\n",
        "\n",
        "        for annotation in annotations:\n",
        "            all_annotations.append({\n",
        "                'frame_original': frame_idx_original,\n",
        "                'frame_processed': frame_idx_processed,\n",
        "                'frame_for_sam': frame_idx_for_sam,  # â† Nouvel index pour SAM2\n",
        "                'obj_id': annotation['obj_id'],\n",
        "                'points': annotation['points'],\n",
        "                'obj_type': obj_types.get(annotation['obj_id'], f'unknown_{annotation[\"obj_id\"]}')\n",
        "            })\n",
        "\n",
        "    if not all_annotations:\n",
        "        raise ValueError(f\"âŒ Aucune annotation trouvÃ©e dans le fichier config\")\n",
        "\n",
        "    print(f\"   ğŸ“Š Total: {len(all_annotations)} annotations sur {len(set(annotation_frames))} frames\")\n",
        "\n",
        "    # Ajout des annotations Ã  SAM2 avec les frames converties\n",
        "    added_objects = []\n",
        "\n",
        "    for annotation_data in all_annotations:\n",
        "        frame_idx_for_sam = annotation_data['frame_for_sam']  # â† Utiliser l'index ajustÃ©\n",
        "        frame_idx_original = annotation_data['frame_original']\n",
        "        obj_id = annotation_data['obj_id']\n",
        "        obj_type = annotation_data['obj_type']\n",
        "        points_data = annotation_data['points']\n",
        "\n",
        "        # Extraction des coordonnÃ©es et labels\n",
        "        points = np.array([[p['x'], p['y']] for p in points_data], dtype=np.float32)\n",
        "        labels = np.array([p['label'] for p in points_data], dtype=np.int32)\n",
        "\n",
        "        print(f\"   ğŸ¯ Frame {frame_idx_original}â†’{frame_idx_for_sam} - Objet {obj_id} ({obj_type}): {len(points)} points Ã  ({points[0][0]:.0f}, {points[0][1]:.0f})\")\n",
        "\n",
        "        # âœ… CORRECTION: Utiliser frame_idx_for_sam (ajustÃ© pour le segment)\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state,\n",
        "            frame_idx_for_sam,  # â† Frame ajustÃ©e pour SAM2\n",
        "            obj_id,\n",
        "            points,\n",
        "            labels\n",
        "        )\n",
        "\n",
        "        # Ã‰viter les doublons dans added_objects\n",
        "        if not any(obj['obj_id'] == obj_id for obj in added_objects):\n",
        "            added_objects.append({\n",
        "                'obj_id': obj_id,\n",
        "                'obj_type': obj_type,\n",
        "                'points_count': len(points)\n",
        "            })\n",
        "\n",
        "    # VÃ©rification\n",
        "    sam_obj_ids = inference_state[\"obj_ids\"]\n",
        "\n",
        "    print(f\"\\nğŸ“Š RÃ‰SUMÃ‰ ANNOTATIONS:\")\n",
        "    print(f\"   ğŸ¯ Annotations configurÃ©es: {len(all_annotations)}\")\n",
        "    print(f\"   ğŸ¯ Objets uniques: {len(added_objects)}\")\n",
        "    print(f\"   âœ… Objets ajoutÃ©s Ã  SAM2: {len(sam_obj_ids)}\")\n",
        "    print(f\"   ğŸ†” IDs: {sorted(sam_obj_ids)}\")\n",
        "    \n",
        "    if segment_info:\n",
        "        print(f\"   ğŸ¯ Mode segmentation: Frame d'annotation ajustÃ©e Ã  l'index {annotation_data['frame_for_sam']} dans le segment\")\n",
        "    else:\n",
        "        print(f\"   ğŸ¬ Mode complet: Frames utilisÃ©es (originalâ†’traitÃ©e): {[(a['frame_original'], a['frame_processed']) for a in all_annotations[:3]]}...\")\n",
        "\n",
        "    # RÃ©sumÃ© par type\n",
        "    type_counts = {}\n",
        "    for obj in added_objects:\n",
        "        obj_type = obj['obj_type']\n",
        "        type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "    print(f\"   ğŸ·ï¸  Types: {dict(type_counts)}\")\n",
        "\n",
        "    return added_objects, all_annotations\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ AJOUT AVEC CONFIGURATION CENTRALISÃ‰E CORRIGÃ‰E\n",
        "# =============================================================================\n",
        "\n",
        "# VÃ©rifications prÃ©requis\n",
        "if not hasattr(cfg, 'predictor') or not hasattr(cfg, 'inference_state'):\n",
        "    print(\"âŒ SAM2 non initialisÃ©. ExÃ©cutez d'abord la cellule d'initialisation SAM2.\")\n",
        "elif project_config is None:\n",
        "    print(\"âŒ Configuration projet non chargÃ©e.\")\n",
        "    if cfg.USING_COLAB:\n",
        "        print(\"ğŸ’¡ Sur Colab, utilisez: project_config = wait_and_load_config(cfg)\")\n",
        "    else:\n",
        "        print(\"ğŸ’¡ La configuration devrait Ãªtre chargÃ©e automatiquement\")\n",
        "else:\n",
        "    # ğŸ¯ NOUVEAU: PrÃ©parer les informations de segmentation si applicable\n",
        "    segment_info = None\n",
        "    if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "        # RÃ©cupÃ©rer les informations du segment depuis la configuration\n",
        "        reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Obtenir les informations vidÃ©o\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        \n",
        "        # âœ… CORRECTION: Utiliser les offsets convertis en frames entiÃ¨res\n",
        "        offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "        \n",
        "        # Calculer les bornes du segment\n",
        "        start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "            reference_frame=reference_frame,\n",
        "            offset_before=offset_before_frames,\n",
        "            offset_after=offset_after_frames,\n",
        "            total_frames=total_frames,\n",
        "            frame_interval=cfg.FRAME_INTERVAL\n",
        "        )\n",
        "        \n",
        "        segment_info = {\n",
        "            'start_frame': start_frame,\n",
        "            'end_frame': end_frame,\n",
        "            'processed_start_idx': processed_start_idx,\n",
        "            'processed_end_idx': processed_end_idx\n",
        "        }\n",
        "        \n",
        "        print(\"ğŸ¯ Informations segmentation prÃ©parÃ©es pour l'ajout d'annotations\")\n",
        "    \n",
        "    # âœ… CORRECTION: Passer les informations de segmentation\n",
        "    added_objects, initial_annotations_data = add_initial_annotations(\n",
        "        cfg.predictor, \n",
        "        cfg.inference_state, \n",
        "        project_config,\n",
        "        cfg.FRAME_INTERVAL,\n",
        "        segment_info  # â† Nouvelles informations de segmentation\n",
        "    )\n",
        "    \n",
        "    # Ajout Ã  la configuration pour usage ultÃ©rieur\n",
        "    cfg.added_objects = added_objects\n",
        "    cfg.initial_annotations_data = initial_annotations_data\n",
        "    \n",
        "    print(\"\\nâœ… Annotations initiales ajoutÃ©es avec succÃ¨s!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fonctions utilitaires pour annotations chargÃ©es!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ”§ FONCTIONS UTILITAIRES POUR ANNOTATIONS COMPLÃˆTES\n",
        "# =============================================================================\n",
        "\n",
        "def get_object_scores(predictor, inference_state, frame_idx, obj_id):\n",
        "    \"\"\"RÃ©cupÃ¨re les scores d'objet de maniÃ¨re propre et sÃ»re\"\"\"\n",
        "    try:\n",
        "        obj_idx = predictor._obj_id_to_idx(inference_state, obj_id)\n",
        "        obj_output_dict = inference_state[\"output_dict_per_obj\"][obj_idx]\n",
        "        temp_output_dict = inference_state[\"temp_output_dict_per_obj\"][obj_idx]\n",
        "\n",
        "        # Chercher dans les outputs\n",
        "        frame_output = None\n",
        "        if frame_idx in temp_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in temp_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = temp_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"cond_frame_outputs\"][frame_idx]\n",
        "        elif frame_idx in obj_output_dict[\"non_cond_frame_outputs\"]:\n",
        "            frame_output = obj_output_dict[\"non_cond_frame_outputs\"][frame_idx]\n",
        "\n",
        "        if frame_output and \"object_score_logits\" in frame_output:\n",
        "            object_score_logits = frame_output[\"object_score_logits\"]\n",
        "            return torch.sigmoid(object_score_logits).item()\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def calculate_bbox_from_rle(rle_data: Dict[str, Any]) -> Optional[Dict[str, int]]:\n",
        "    \"\"\"Calcule la bounding box depuis un RLE base64.\"\"\"\n",
        "    from pycocotools.mask import toBbox\n",
        "\n",
        "    try:\n",
        "        rle = {\n",
        "            \"size\": rle_data[\"size\"],\n",
        "            \"counts\": base64.b64decode(rle_data[\"counts\"])\n",
        "        }\n",
        "\n",
        "        bbox = toBbox(rle)\n",
        "\n",
        "        result = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erreur calcul bbox: {e}\")\n",
        "        return None\n",
        "\n",
        "def image_to_world(point_2d, cam_params):\n",
        "    \"\"\"\n",
        "    Projette un point 2D de l'image vers le plan du terrain (Z=0).\n",
        "    \"\"\"\n",
        "    # Create projection matrix P\n",
        "    K = np.array([\n",
        "        [cam_params[\"cam_params\"][\"x_focal_length\"], 0, cam_params[\"cam_params\"][\"principal_point\"][0]],\n",
        "        [0, cam_params[\"cam_params\"][\"y_focal_length\"], cam_params[\"cam_params\"][\"principal_point\"][1]],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "    R = np.array(cam_params[\"cam_params\"][\"rotation_matrix\"])\n",
        "    t = -R @ np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    P = K @ np.hstack((R, t.reshape(-1,1)))\n",
        "\n",
        "    # Create point on image plane in homogeneous coordinates\n",
        "    point_2d_h = np.array([point_2d[0], point_2d[1], 1])\n",
        "\n",
        "    # Back-project ray from camera\n",
        "    ray = np.linalg.inv(K) @ point_2d_h\n",
        "    ray = R.T @ ray\n",
        "\n",
        "    # Find intersection with Z=0 plane\n",
        "    camera_pos = np.array(cam_params[\"cam_params\"][\"position_meters\"])\n",
        "    t = -camera_pos[2] / ray[2]\n",
        "    world_point = camera_pos + t * ray\n",
        "\n",
        "    return world_point[:2]  # Return only X,Y coordinates since Z=0\n",
        "\n",
        "def calculate_points_output(bbox_output: dict, cam_params: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Calcule les points de sortie Ã  partir de la bbox output.\n",
        "\n",
        "    Args:\n",
        "        bbox_output: Dict avec 'x', 'y', 'width', 'height'\n",
        "        cam_params: ParamÃ¨tres de calibration camÃ©ra pour projection terrain\n",
        "\n",
        "    Returns:\n",
        "        Dict avec les points calculÃ©s sÃ©parÃ©s par plan (image vs field)\n",
        "    \"\"\"\n",
        "    if not bbox_output:\n",
        "        return None\n",
        "\n",
        "    # Calculer le point CENTER_BOTTOM dans le plan image\n",
        "    center_bottom_x = bbox_output['x'] + bbox_output['width'] / 2\n",
        "    center_bottom_y = bbox_output['y'] + bbox_output['height']  # Bas de la bbox\n",
        "\n",
        "    # Structure avec sÃ©paration image/field\n",
        "    points_output = {\n",
        "        \"image\": {\n",
        "            \"CENTER_BOTTOM\": {\n",
        "                \"x\": float(center_bottom_x),\n",
        "                \"y\": float(center_bottom_y)\n",
        "            }\n",
        "        },\n",
        "        \"field\": {\n",
        "            \"CENTER_BOTTOM\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Projection vers le terrain si les paramÃ¨tres camÃ©ra sont fournis\n",
        "    if cam_params:\n",
        "        try:\n",
        "            # Projeter le point CENTER_BOTTOM vers le terrain\n",
        "            image_point = [center_bottom_x, center_bottom_y]\n",
        "            field_point = image_to_world(image_point, cam_params)\n",
        "\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = {\n",
        "                \"x\": float(field_point[0]),\n",
        "                \"y\": float(field_point[1])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erreur projection terrain: {e}\")\n",
        "            points_output[\"field\"][\"CENTER_BOTTOM\"] = None\n",
        "\n",
        "    return points_output\n",
        "\n",
        "def create_mask_annotation(obj_id: int, mask_logits, predictor=None, inference_state=None,\n",
        "                         frame_idx=None, cam_params: Dict = None) -> Dict:\n",
        "    \"\"\"\n",
        "    CrÃ©e une annotation de masque complÃ¨te avec points input/output, bbox et scores.\n",
        "    \"\"\"\n",
        "    # Conversion en masque binaire\n",
        "    mask = (mask_logits > 0.0).cpu().numpy()\n",
        "    if mask.ndim == 3 and mask.shape[0] == 1:\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "\n",
        "    # Encodage RLE\n",
        "    if not mask.flags['F_CONTIGUOUS']:\n",
        "        mask = np.asfortranarray(mask)\n",
        "\n",
        "    rle = encode_rle(mask.astype(np.uint8))\n",
        "    base64_counts = base64.b64encode(rle[\"counts\"]).decode('ascii')\n",
        "\n",
        "    # Calcul bbox et points output si masque non vide\n",
        "    bbox_output = None\n",
        "    points_output = None\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        from pycocotools.mask import toBbox\n",
        "        bbox = toBbox(rle)\n",
        "        bbox_output = {\n",
        "            \"x\": int(bbox[0]),\n",
        "            \"y\": int(bbox[1]),\n",
        "            \"width\": int(bbox[2]),\n",
        "            \"height\": int(bbox[3])\n",
        "        }\n",
        "        # Calcul des points output depuis la bbox\n",
        "        points_output = calculate_points_output(bbox_output, cam_params)\n",
        "\n",
        "    # RÃ©cupÃ©ration du score du masque\n",
        "    mask_score = None\n",
        "\n",
        "    if predictor and inference_state and frame_idx is not None:\n",
        "        # Score du masque\n",
        "        mask_score = get_object_scores(predictor, inference_state, frame_idx, obj_id)\n",
        "\n",
        "    # Structure d'annotation complÃ¨te\n",
        "    return {\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"objectId\": str(obj_id),\n",
        "        \"type\": \"mask\",\n",
        "        \"mask\": {\n",
        "            \"format\": \"rle_coco_base64\",\n",
        "            \"size\": [int(rle[\"size\"][0]), int(rle[\"size\"][1])],\n",
        "            \"counts\": base64_counts\n",
        "        },\n",
        "        \"bbox\": {\n",
        "            \"output\": bbox_output\n",
        "        },\n",
        "        \"points\": {\n",
        "            \"output\": points_output\n",
        "        },\n",
        "        \"maskScore\": mask_score,\n",
        "        \"pose\": None,\n",
        "        \"warning\": False\n",
        "    }\n",
        "\n",
        "print(\"âœ… Fonctions utilitaires pour annotations chargÃ©es!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ğŸ”„ PROPAGATION ET GÃ‰NÃ‰RATION DES ANNOTATIONS\n",
        "# =============================================================================\n",
        "def generate_frame_mapping_with_anchor(total_frames: int, frame_interval: int, anchor_frame: int) -> List[Optional[int]]:\n",
        "    \"\"\"\n",
        "    GÃ©nÃ¨re le mapping en s'assurant que anchor_frame est incluse.\n",
        "    \n",
        "    Args:\n",
        "        total_frames: Nombre total de frames dans la vidÃ©o originale\n",
        "        frame_interval: Intervalle entre frames (ex: 10 = 1 frame sur 10)\n",
        "        anchor_frame: Frame qui DOIT Ãªtre incluse (frame d'annotation initiale)\n",
        "    \n",
        "    Returns:\n",
        "        Liste oÃ¹ l'index = frame originale, valeur = frame traitÃ©e (ou None si pas traitÃ©e)\n",
        "    \"\"\"\n",
        "    frame_mapping = [None] * total_frames\n",
        "    processed_frames = set()\n",
        "    processed_idx = 0\n",
        "    \n",
        "    # 1. S'assurer que anchor_frame est incluse\n",
        "    if 0 <= anchor_frame < total_frames:\n",
        "        frame_mapping[anchor_frame] = processed_idx\n",
        "        processed_frames.add(anchor_frame)\n",
        "        processed_idx += 1\n",
        "    \n",
        "    # 2. Ajouter les frames selon l'intervalle, en Ã©vitant les doublons\n",
        "    for original_idx in range(0, total_frames, frame_interval):\n",
        "        if original_idx not in processed_frames:\n",
        "            frame_mapping[original_idx] = processed_idx\n",
        "            processed_frames.add(original_idx)\n",
        "            processed_idx += 1\n",
        "    \n",
        "    # 3. RÃ©organiser les indices pour que anchor_frame ait l'index correspondant Ã  sa position chronologique\n",
        "    # Tri des frames traitÃ©es par ordre chronologique\n",
        "    sorted_frames = sorted(processed_frames)\n",
        "    final_mapping = [None] * total_frames\n",
        "    \n",
        "    for new_idx, original_frame in enumerate(sorted_frames):\n",
        "        final_mapping[original_frame] = new_idx\n",
        "    \n",
        "    return final_mapping, sorted_frames\n",
        "\n",
        "def get_initial_annotation_frame(project_config: Dict[str, Any], frame_interval: int = 1) -> int:\n",
        "    \"\"\"RÃ©cupÃ¨re la frame d'annotation initiale depuis la config et la convertit selon l'intervalle\"\"\"\n",
        "    if not project_config.get('initial_annotations'):\n",
        "        return 0\n",
        "    \n",
        "    # Prendre la premiÃ¨re frame d'annotation comme anchor (frame originale)\n",
        "    first_annotation = project_config['initial_annotations'][0]\n",
        "    original_frame = first_annotation.get('frame', 0)\n",
        "    \n",
        "    # âœ… CONVERSION: Frame originale â†’ Frame traitÃ©e selon l'intervalle\n",
        "    processed_frame = original_frame // frame_interval\n",
        "    \n",
        "    print(f\"   ğŸ”„ Conversion anchor frame: {original_frame} (original) â†’ {processed_frame} (traitÃ©e avec intervalle {frame_interval})\")\n",
        "    \n",
        "    return processed_frame\n",
        "\n",
        "def get_video_info(video_path: Path) -> Dict[str, Any]:\n",
        "    \"\"\"RÃ©cupÃ¨re les informations de la vidÃ©o\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"âŒ Impossible d'ouvrir la vidÃ©o: {video_path}\")\n",
        "    \n",
        "    video_info = {\n",
        "        'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
        "        'fps': cap.get(cv2.CAP_PROP_FPS),\n",
        "        'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "        'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    }\n",
        "    cap.release()\n",
        "    return video_info\n",
        "def create_project_structure_with_config(cfg, project_config: Dict[str, Any], added_objects: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"CrÃ©e la structure JSON du projet avec la configuration centralisÃ©e.\"\"\"\n",
        "    \n",
        "    # Informations vidÃ©o\n",
        "    video_info = get_video_info(cfg.video_path)\n",
        "    \n",
        "    # âœ… CORRECTION: Passer l'intervalle pour la conversion\n",
        "    anchor_frame = get_initial_annotation_frame(project_config, cfg.FRAME_INTERVAL)\n",
        "    \n",
        "    # Calcul du mapping avec la frame dÃ©jÃ  convertie\n",
        "    frame_mapping, processed_frames = generate_frame_mapping_with_anchor(\n",
        "        video_info['total_frames'], \n",
        "        cfg.FRAME_INTERVAL,\n",
        "        anchor_frame * cfg.FRAME_INTERVAL  # â† Reconvertir pour le mapping original\n",
        "    )\n",
        "    processed_frame_count = len(processed_frames)\n",
        "\n",
        "    # Structure objects avec couleurs\n",
        "    import random\n",
        "    import colorsys\n",
        "\n",
        "    config_objects_mapping = {}\n",
        "    for obj in project_config['objects']:\n",
        "        config_objects_mapping[obj['obj_id']] = obj\n",
        "\n",
        "    objects = {}\n",
        "    for obj_data in added_objects:\n",
        "        obj_id = str(obj_data['obj_id'])\n",
        "        obj_type = obj_data['obj_type']\n",
        "\n",
        "        # RÃ©cupÃ©rer les informations complÃ¨tes depuis le config\n",
        "        config_obj = config_objects_mapping.get(int(obj_id), {})\n",
        "\n",
        "        # Couleur alÃ©atoire reproductible\n",
        "        random.seed(int(obj_id) * 12345)\n",
        "        hue = random.random()\n",
        "        rgb = colorsys.hsv_to_rgb(hue, 0.8, 0.9)\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n",
        "        )\n",
        "\n",
        "        objects[obj_id] = {\n",
        "            \"id\": obj_id,\n",
        "            \"type\": obj_type,\n",
        "            \"team\": config_obj.get('team', None),\n",
        "            \"jersey_number\": config_obj.get('jersey_number', None),\n",
        "            \"jersey_color\": config_obj.get('jersey_color', None),\n",
        "            \"role\": config_obj.get('role', None),\n",
        "            \"display_color\": hex_color\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"format_version\": \"1.0\",\n",
        "        \"video\": f\"{cfg.VIDEO_NAME}.mp4\",\n",
        "        \"metadata\": {\n",
        "            \"project_id\": str(uuid.uuid4()),\n",
        "            \"created_at\": datetime.now().isoformat() + \"Z\",\n",
        "            \"fps\": video_info['fps'],\n",
        "            \"resolution\": {\n",
        "                \"width\": video_info['width'],\n",
        "                \"height\": video_info['height'],\n",
        "                \"aspect_ratio\": round(video_info['width'] / video_info['height'], 2)\n",
        "            },\n",
        "            \"frame_interval\": cfg.FRAME_INTERVAL,\n",
        "            \"frame_count_original\": video_info['total_frames'],\n",
        "            \"frame_count_processed\": processed_frame_count,\n",
        "            \"frame_mapping\": frame_mapping,\n",
        "            \"anchor_frame\": anchor_frame,  # âœ… AJOUT: Stockage de la frame d'ancrage\n",
        "            \"static_video\": False\n",
        "        },\n",
        "        \"calibration\": project_config['calibration'],\n",
        "        \"objects\": objects,\n",
        "        \"initial_annotations\": project_config['initial_annotations'],\n",
        "        \"annotations\": {}\n",
        "    }\n",
        "\n",
        "def run_sam2_propagation(cfg, project_config):\n",
        "    \"\"\"ExÃ©cute la propagation SAM2 et gÃ©nÃ¨re toutes les annotations\"\"\"\n",
        "    \n",
        "    print(f\"ğŸ”„ DÃ©marrage de la propagation...\")\n",
        "    print(f\"   ğŸ¬ {cfg.extracted_frames_count} frames Ã  traiter\")\n",
        "    print(f\"   ğŸ¯ {len(cfg.added_objects)} objets Ã  suivre\")\n",
        "\n",
        "    # CrÃ©ation de la structure du projet\n",
        "    project = create_project_structure_with_config(cfg, project_config, cfg.added_objects)\n",
        "\n",
        "    # Propagation et annotation\n",
        "    frame_count = 0\n",
        "    for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(cfg.inference_state):\n",
        "\n",
        "        if str(out_frame_idx) not in project['annotations']:\n",
        "            project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "        for i, out_obj_id in enumerate(out_obj_ids):\n",
        "            annotation = create_mask_annotation(\n",
        "                obj_id=out_obj_id,\n",
        "                mask_logits=out_mask_logits[i],\n",
        "                predictor=cfg.predictor,\n",
        "                inference_state=cfg.inference_state,\n",
        "                frame_idx=out_frame_idx,\n",
        "                cam_params=project_config['calibration']['camera_parameters']\n",
        "            )\n",
        "            project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "        \n",
        "        frame_count += 1\n",
        "        if frame_count % 50 == 0:\n",
        "            progress = (frame_count / cfg.extracted_frames_count) * 100\n",
        "            print(f\"   ğŸ“Š ProgrÃ¨s: {frame_count}/{cfg.extracted_frames_count} frames ({progress:.1f}%)\")\n",
        "    \n",
        "    print(f\"âœ… Propagation terminÃ©e: {frame_count} frames traitÃ©es\")\n",
        "    return project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_sam2_bidirectional_propagation(cfg, project_config):\n",
        "    \"\"\"ExÃ©cute la propagation SAM2 bidirectionnelle depuis la frame d'annotation avec support segmentation\"\"\"\n",
        "    \n",
        "    print(f\"ğŸ”„ DÃ©marrage de la propagation bidirectionnelle...\")\n",
        "    \n",
        "    # ğŸ¯ NOUVEAU: Gestion du mode segmentation\n",
        "    segment_mode = hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE\n",
        "    \n",
        "    if segment_mode:\n",
        "        print(f\"ğŸ¯ Mode segmentation activÃ©\")\n",
        "        \n",
        "        # En mode segmentation, utiliser les indices ajustÃ©s\n",
        "        reference_frame_original = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        \n",
        "        # Recalculer les bornes du segment\n",
        "        cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        \n",
        "        # âœ… AMÃ‰LIORATION: Utiliser les offsets calculÃ©s depuis les secondes\n",
        "        offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "        \n",
        "        start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "            reference_frame=reference_frame_original,\n",
        "            offset_before=offset_before_frames,\n",
        "            offset_after=offset_after_frames,\n",
        "            total_frames=total_frames,\n",
        "            frame_interval=cfg.FRAME_INTERVAL\n",
        "        )\n",
        "        \n",
        "        # Calculer l'index dans le segment\n",
        "        reference_frame_processed = reference_frame_original // cfg.FRAME_INTERVAL\n",
        "        segment_start_processed = start_frame // cfg.FRAME_INTERVAL\n",
        "        anchor_frame_in_segment = reference_frame_processed - segment_start_processed\n",
        "        \n",
        "        print(f\"   ğŸ“ Frame de rÃ©fÃ©rence originale: {reference_frame_original}\")\n",
        "        print(f\"   ğŸ“ Frame de rÃ©fÃ©rence traitÃ©e: {reference_frame_processed}\")\n",
        "        print(f\"   ğŸ“ Segment commence Ã : {segment_start_processed} (traitÃ©)\")\n",
        "        print(f\"   ğŸ“ Index dans le segment: {anchor_frame_in_segment}\")\n",
        "        print(f\"   ğŸ“Š Frames disponibles dans le segment: 0 Ã  {cfg.extracted_frames_count - 1}\")\n",
        "        \n",
        "        # VÃ©rifier que l'index est valide\n",
        "        if anchor_frame_in_segment < 0 or anchor_frame_in_segment >= cfg.extracted_frames_count:\n",
        "            raise ValueError(f\"âŒ Index anchor {anchor_frame_in_segment} en dehors des frames disponibles [0, {cfg.extracted_frames_count - 1}]\")\n",
        "        \n",
        "        # En mode segmentation, les frames vont de 0 Ã  extracted_frames_count-1\n",
        "        anchor_processed_idx = anchor_frame_in_segment\n",
        "        total_frames_available = cfg.extracted_frames_count\n",
        "        \n",
        "        # Mapping simplifiÃ© pour la segmentation\n",
        "        processed_frames = list(range(total_frames_available))\n",
        "        frame_mapping = [None] * total_frames\n",
        "        \n",
        "        # Remplir le mapping pour le segment\n",
        "        for i, original_frame in enumerate(range(start_frame, end_frame + 1, cfg.FRAME_INTERVAL)):\n",
        "            if original_frame < total_frames:\n",
        "                frame_mapping[original_frame] = i\n",
        "        \n",
        "    else:\n",
        "        print(f\"ğŸ¬ Mode complet activÃ©\")\n",
        "        \n",
        "        # Mode complet : logique originale\n",
        "        reference_frame_original = project_config['initial_annotations'][0].get('frame', 0)\n",
        "        reference_frame_processed = reference_frame_original // cfg.FRAME_INTERVAL\n",
        "        \n",
        "        # Informations vidÃ©o et mapping\n",
        "        video_info = get_video_info(cfg.video_path)\n",
        "        frame_mapping, processed_frames = generate_frame_mapping_with_anchor(\n",
        "            video_info['total_frames'], \n",
        "            cfg.FRAME_INTERVAL, \n",
        "            reference_frame_processed\n",
        "        )\n",
        "        \n",
        "        # Trouver l'index de la frame d'ancrage dans les frames traitÃ©es\n",
        "        anchor_processed_idx = frame_mapping[reference_frame_processed]\n",
        "        total_frames_available = len(processed_frames)\n",
        "        \n",
        "        print(f\"   ğŸ“ Frame d'ancrage: {reference_frame_original} (original) â†’ {reference_frame_processed} (traitÃ©e) â†’ {anchor_processed_idx} (index)\")\n",
        "        print(f\"   ğŸ“Š Frames disponibles: 0 Ã  {total_frames_available - 1}\")\n",
        "    \n",
        "    print(f\"   ğŸ“ Anchor frame index: {anchor_processed_idx}\")\n",
        "    print(f\"   ğŸ“Š Total frames disponibles: {total_frames_available}\")\n",
        "    \n",
        "    # CrÃ©ation de la structure du projet\n",
        "    project = create_project_structure_with_config(cfg, project_config, cfg.added_objects)\n",
        "    \n",
        "    # âœ… CORRECTION: MÃ©tadonnÃ©es harmonisÃ©es pour les deux modes\n",
        "    project['metadata']['segment_mode'] = segment_mode\n",
        "    project['metadata']['anchor_processed_idx'] = anchor_processed_idx  # â† Toujours prÃ©sent\n",
        "    project['metadata']['frame_mapping'] = frame_mapping\n",
        "    \n",
        "    if segment_mode:\n",
        "        # MÃ©tadonnÃ©es spÃ©cifiques au mode segmentation\n",
        "        project['metadata']['segment_start_frame'] = start_frame\n",
        "        project['metadata']['segment_end_frame'] = end_frame\n",
        "        project['metadata']['segment_reference_frame'] = reference_frame_original\n",
        "        project['metadata']['anchor_frame_in_segment'] = anchor_frame_in_segment\n",
        "        project['metadata']['anchor_frame'] = reference_frame_original  # â† Frame originale en mode segmentation\n",
        "    else:\n",
        "        # MÃ©tadonnÃ©es spÃ©cifiques au mode complet\n",
        "        project['metadata']['anchor_frame'] = reference_frame_processed  # â† Frame traitÃ©e en mode complet\n",
        "    \n",
        "    # === PHASE 1: PROPAGATION INVERSE (anchor â†’ 0) ===\n",
        "    if anchor_processed_idx > 0:\n",
        "        print(f\"\\nğŸ”„ PHASE 1: Propagation inverse (frame {anchor_processed_idx} â†’ 0)\")\n",
        "        \n",
        "        reverse_frames_count = 0\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(\n",
        "            cfg.inference_state,\n",
        "            start_frame_idx=anchor_processed_idx,\n",
        "            max_frame_num_to_track=anchor_processed_idx + 1,  # +1 pour inclure l'anchor\n",
        "            reverse=True\n",
        "        ):\n",
        "            # Traitement identique Ã  la propagation normale\n",
        "            if str(out_frame_idx) not in project['annotations']:\n",
        "                project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "            for i, out_obj_id in enumerate(out_obj_ids):\n",
        "                annotation = create_mask_annotation(\n",
        "                    obj_id=out_obj_id,\n",
        "                    mask_logits=out_mask_logits[i],\n",
        "                    predictor=cfg.predictor,\n",
        "                    inference_state=cfg.inference_state,\n",
        "                    frame_idx=out_frame_idx,\n",
        "                    cam_params=project_config['calibration']['camera_parameters']\n",
        "                )\n",
        "                project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "            \n",
        "            reverse_frames_count += 1\n",
        "            if reverse_frames_count % 10 == 0:\n",
        "                print(f\"     ğŸ“Š Frames traitÃ©es (reverse): {reverse_frames_count}\")\n",
        "        \n",
        "        print(f\"   âœ… Phase reverse terminÃ©e: {reverse_frames_count} frames\")\n",
        "    else:\n",
        "        print(f\"\\nâ­ï¸ PHASE 1: Propagation inverse skippÃ©e (anchor Ã  la frame 0)\")\n",
        "    \n",
        "    # === PHASE 2: PROPAGATION AVANT (anchor â†’ fin) ===\n",
        "    remaining_frames = total_frames_available - anchor_processed_idx\n",
        "    if remaining_frames > 1:  # > 1 car anchor dÃ©jÃ  traitÃ©e\n",
        "        print(f\"\\nğŸ”„ PHASE 2: Propagation avant (frame {anchor_processed_idx} â†’ {total_frames_available - 1})\")\n",
        "        \n",
        "        forward_frames_count = 0\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in cfg.predictor.propagate_in_video(\n",
        "            cfg.inference_state,\n",
        "            start_frame_idx=anchor_processed_idx,\n",
        "            max_frame_num_to_track=remaining_frames,\n",
        "            reverse=False\n",
        "        ):\n",
        "            # Ã‰viter de traiter Ã  nouveau l'anchor frame\n",
        "            if out_frame_idx == anchor_processed_idx and str(out_frame_idx) in project['annotations']:\n",
        "                continue\n",
        "                \n",
        "            if str(out_frame_idx) not in project['annotations']:\n",
        "                project['annotations'][str(out_frame_idx)] = []\n",
        "\n",
        "            for i, out_obj_id in enumerate(out_obj_ids):\n",
        "                annotation = create_mask_annotation(\n",
        "                    obj_id=out_obj_id,\n",
        "                    mask_logits=out_mask_logits[i],\n",
        "                    predictor=cfg.predictor,\n",
        "                    inference_state=cfg.inference_state,\n",
        "                    frame_idx=out_frame_idx,\n",
        "                    cam_params=project_config['calibration']['camera_parameters']\n",
        "                )\n",
        "                project['annotations'][str(out_frame_idx)].append(annotation)\n",
        "            \n",
        "            forward_frames_count += 1\n",
        "            if forward_frames_count % 20 == 0:\n",
        "                progress = (forward_frames_count / remaining_frames) * 100\n",
        "                print(f\"     ğŸ“Š Frames traitÃ©es (forward): {forward_frames_count}/{remaining_frames} ({progress:.1f}%)\")\n",
        "        \n",
        "        print(f\"   âœ… Phase forward terminÃ©e: {forward_frames_count} frames\")\n",
        "    else:\n",
        "        print(f\"\\nâ­ï¸ PHASE 2: Propagation avant skippÃ©e (anchor Ã  la fin)\")\n",
        "    \n",
        "    total_frames_processed = len(project['annotations'])\n",
        "    print(f\"\\nâœ… Propagation bidirectionnelle terminÃ©e:\")\n",
        "    print(f\"   ğŸ“Š Total frames traitÃ©es: {total_frames_processed}\")\n",
        "    if segment_mode:\n",
        "        print(f\"   ğŸ¯ Mode segmentation: Frame d'ancrage Ã  l'index {anchor_processed_idx} dans le segment\")\n",
        "        print(f\"   ğŸ“ Segment original: frames {start_frame} Ã  {end_frame}\")\n",
        "    else:\n",
        "        print(f\"   ğŸ¯ Mode complet: Frame d'ancrage {anchor_processed_idx} sur {total_frames_available}\")\n",
        "    \n",
        "    return project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ MODE SEGMENTATION ACTIVÃ‰\n",
            "ğŸ¯ CALCUL DES BORNES DE SEGMENTATION:\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "   ğŸ“‰ Offset avant: 50 frames\n",
            "   ğŸ“ˆ Offset aprÃ¨s: 50 frames\n",
            "   ğŸ¬ Segment original: frames 230 Ã  330\n",
            "   ğŸ¬ Segment traitÃ©: indices 76 Ã  110\n",
            "   ğŸ“Š Nombre de frames Ã  traiter: 35\n",
            "ğŸ¬ EXTRACTION DU SEGMENT:\n",
            "   ğŸ“¹ VidÃ©o: ..\\data\\videos\\SD_13_06_2025_cam1_PdB_S1_T959s_1.mp4\n",
            "   ğŸ“ Destination: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   ğŸ¯ Segment: frames 230 Ã  330\n",
            "   â¯ï¸  Intervalle: 3\n",
            "ğŸ”„ 34 frames existantes - SUPPRESSION et rÃ©-extraction...\n",
            "ğŸ—‘ï¸  Frames existantes supprimÃ©es\n",
            "ğŸ“Š VidÃ©o: 624 frames total, 25.0 FPS\n",
            "ğŸ“Š Nommage: sÃ©quentiel de 00000.jpg Ã  00033.jpg\n",
            "ğŸ“Š ProgrÃ¨s: 10 frames extraites (27.0%)\n",
            "ğŸ“Š ProgrÃ¨s: 20 frames extraites (57.0%)\n",
            "ğŸ“Š ProgrÃ¨s: 30 frames extraites (87.0%)\n",
            "âœ… 34 frames du segment extraites\n",
            "ğŸ“‹ Nommage: 00000.jpg Ã  00033.jpg\n",
            "âœ… 34 frames du segment extraites\n",
            "ğŸ¯ SEGMENT CONFIGURÃ‰:\n",
            "   ğŸ“Š Frames du segment: 101\n",
            "   ğŸ¬ Plage: 230 Ã  330\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "ğŸ¯ Utilisation de la propagation bidirectionnelle amÃ©liorÃ©e...\n",
            "ğŸ”„ DÃ©marrage de la propagation bidirectionnelle...\n",
            "ğŸ¯ Mode segmentation activÃ©\n",
            "ğŸ¯ CALCUL DES BORNES DE SEGMENTATION:\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence: 280\n",
            "   ğŸ“‰ Offset avant: 50 frames\n",
            "   ğŸ“ˆ Offset aprÃ¨s: 50 frames\n",
            "   ğŸ¬ Segment original: frames 230 Ã  330\n",
            "   ğŸ¬ Segment traitÃ©: indices 76 Ã  110\n",
            "   ğŸ“Š Nombre de frames Ã  traiter: 35\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence originale: 280\n",
            "   ğŸ“ Frame de rÃ©fÃ©rence traitÃ©e: 93\n",
            "   ğŸ“ Segment commence Ã : 76 (traitÃ©)\n",
            "   ğŸ“ Index dans le segment: 17\n",
            "   ğŸ“Š Frames disponibles dans le segment: 0 Ã  33\n",
            "   ğŸ“ Anchor frame index: 17\n",
            "   ğŸ“Š Total frames disponibles: 34\n",
            "   ğŸ”„ Conversion anchor frame: 280 (original) â†’ 93 (traitÃ©e avec intervalle 3)\n",
            "\n",
            "ğŸ”„ PHASE 1: Propagation inverse (frame 17 â†’ 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "propagate in video:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [02:00<02:26, 18.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ğŸ“Š Frames traitÃ©es (reverse): 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "propagate in video: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [05:20<00:00, 17.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Phase reverse terminÃ©e: 18 frames\n",
            "\n",
            "ğŸ”„ PHASE 2: Propagation avant (frame 17 â†’ 33)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "propagate in video: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [06:40<00:00, 23.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Phase forward terminÃ©e: 16 frames\n",
            "\n",
            "âœ… Propagation bidirectionnelle terminÃ©e:\n",
            "   ğŸ“Š Total frames traitÃ©es: 34\n",
            "   ğŸ¯ Mode segmentation: Frame d'ancrage Ã  l'index 17 dans le segment\n",
            "   ğŸ“ Segment original: frames 230 Ã  330\n",
            "\n",
            "ğŸ“Š RÃ‰SULTATS PROPAGATION BIDIRECTIONNELLE:\n",
            "   ğŸ¯ Frame d'ancrage: 280 (original) â†’ 17 (traitÃ©e)\n",
            "   ğŸ¬ Frames originales: 624\n",
            "   ğŸ¬ Frames traitÃ©es: 208\n",
            "   ğŸ“ Frames avec annotations: 34\n",
            "   ğŸ“ Annotations totales: 442\n",
            "   ğŸ¯ Objets suivis: 13\n",
            "   â¯ï¸  Intervalle frames: 3\n",
            "\n",
            "ğŸ—‚ï¸  MAPPING FRAMES (Ã©chantillon):\n",
            "   ğŸ“‹ Format: (frame_originale â†’ frame_traitÃ©e)\n",
            "   ğŸ“‹ Ã‰chantillon: [(230, 0), (233, 1), (236, 2), (239, 3), (242, 4), (245, 5), (248, 6), (251, 7), (254, 8), (257, 9)]\n",
            "   ğŸ“‹ ... et 24 autres frames\n",
            "   âš ï¸  Attention: Frame d'ancrage 280 non trouvÃ©e dans le mapping\n",
            "   ğŸ·ï¸  Types d'objets: {'player': 12, 'ball': 1}\n",
            "\n",
            "ğŸ“ˆ RÃ‰PARTITION TEMPORELLE:\n",
            "   ğŸ“Š PremiÃ¨re frame annotÃ©e: 0\n",
            "   ğŸ“Š DerniÃ¨re frame annotÃ©e: 33\n",
            "   ğŸ“Š Plage totale: 34 frames\n",
            "\n",
            "âœ… Propagation bidirectionnelle terminÃ©e avec succÃ¨s!\n",
            "ğŸ’¡ Le projet est maintenant prÃªt pour la sauvegarde et la visualisation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ğŸ¯ PROPAGATION AVEC SEGMENTATION OPTIONNELLE\n",
        "# =============================================================================\n",
        "# VÃ©rifier le mode de segmentation\n",
        "if hasattr(cfg, 'SEGMENT_MODE') and cfg.SEGMENT_MODE:\n",
        "    print(\"ğŸ¯ MODE SEGMENTATION ACTIVÃ‰\")\n",
        "    \n",
        "    # RÃ©cupÃ©rer la frame de rÃ©fÃ©rence\n",
        "    reference_frame = project_config['initial_annotations'][0].get('frame', 0)\n",
        "    \n",
        "    # Obtenir les informations vidÃ©o\n",
        "    cap = cv2.VideoCapture(str(cfg.video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "    \n",
        "    # âœ… AMÃ‰LIORATION: Utiliser les offsets calculÃ©s depuis les secondes\n",
        "    offset_before_frames, offset_after_frames = cfg.get_segment_offsets_frames()\n",
        "    \n",
        "    # Calculer les bornes du segment\n",
        "    start_frame, end_frame, processed_start_idx, processed_end_idx = calculate_segment_bounds(\n",
        "        reference_frame=reference_frame,\n",
        "        offset_before=offset_before_frames,\n",
        "        offset_after=offset_after_frames,\n",
        "        total_frames=total_frames,\n",
        "        frame_interval=cfg.FRAME_INTERVAL\n",
        "    )\n",
        "    \n",
        "    # RÃ©ajuster l'extraction pour le segment\n",
        "    if cfg.EXTRACT_FRAMES:\n",
        "        extracted_count = extract_segment_frames(\n",
        "            video_path=cfg.video_path,\n",
        "            frames_dir=cfg.frames_dir,\n",
        "            start_frame=start_frame,\n",
        "            end_frame=end_frame,\n",
        "            frame_interval=cfg.FRAME_INTERVAL,\n",
        "            force_extraction=cfg.FORCE_EXTRACTION\n",
        "        )\n",
        "        cfg.extracted_frames_count = extracted_count\n",
        "        print(f\"âœ… {extracted_count} frames du segment extraites\")\n",
        "    \n",
        "    print(f\"ğŸ¯ SEGMENT CONFIGURÃ‰:\")\n",
        "    print(f\"   ğŸ“Š Frames du segment: {end_frame - start_frame + 1}\")\n",
        "    print(f\"   ğŸ¬ Plage: {start_frame} Ã  {end_frame}\")\n",
        "    print(f\"   ğŸ“ Frame de rÃ©fÃ©rence: {reference_frame}\")\n",
        "    \n",
        "else:\n",
        "    print(\"ğŸ¬ MODE COMPLET ACTIVÃ‰ (toute la vidÃ©o)\")\n",
        "    \n",
        "# =============================================================================\n",
        "# ğŸ”„ PROPAGATION BIDIRECTIONNELLE (REMPLACE L'ANCIENNE PROPAGATION)\n",
        "# =============================================================================\n",
        "\n",
        "# VÃ©rifications des prÃ©requis\n",
        "missing_requirements = []\n",
        "required_attrs = ['predictor', 'inference_state', 'added_objects', 'extracted_frames_count']\n",
        "\n",
        "for attr in required_attrs:\n",
        "    if not hasattr(cfg, attr):\n",
        "        missing_requirements.append(attr)\n",
        "\n",
        "if project_config is None:\n",
        "    missing_requirements.append(\"project_config\")\n",
        "\n",
        "if missing_requirements:\n",
        "    print(f\"âŒ PrÃ©requis manquants: {missing_requirements}\")\n",
        "    print(\"ğŸ’¡ ExÃ©cutez d'abord les cellules prÃ©cÃ©dentes dans l'ordre:\")\n",
        "    print(\"   1. Configuration centralisÃ©e\")\n",
        "    print(\"   2. Installation SAM2\") \n",
        "    print(\"   3. Chargement configuration projet\")\n",
        "    print(\"   4. Extraction des frames\")\n",
        "    print(\"   5. Initialisation SAM2\")\n",
        "    print(\"   6. Ajout des annotations initiales\")\n",
        "else:\n",
        "    # ğŸš€ NOUVELLE PROPAGATION BIDIRECTIONNELLE\n",
        "    print(\"ğŸ¯ Utilisation de la propagation bidirectionnelle amÃ©liorÃ©e...\")\n",
        "    \n",
        "    project = run_sam2_bidirectional_propagation(cfg, project_config)\n",
        "    \n",
        "    # Ajout Ã  la configuration pour usage ultÃ©rieur\n",
        "    cfg.project = project\n",
        "    \n",
        "    # Statistiques finales amÃ©liorÃ©es\n",
        "    total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "    unique_frames = len(project['annotations'])\n",
        "    anchor_frame = project['metadata']['anchor_frame']\n",
        "    anchor_processed_idx = project['metadata']['anchor_processed_idx']\n",
        "    \n",
        "    print(f\"\\nğŸ“Š RÃ‰SULTATS PROPAGATION BIDIRECTIONNELLE:\")\n",
        "    print(f\"   ğŸ¯ Frame d'ancrage: {anchor_frame} (original) â†’ {anchor_processed_idx} (traitÃ©e)\")\n",
        "    print(f\"   ğŸ¬ Frames originales: {project['metadata']['frame_count_original']:,}\")\n",
        "    print(f\"   ğŸ¬ Frames traitÃ©es: {project['metadata']['frame_count_processed']:,}\")\n",
        "    print(f\"   ğŸ“ Frames avec annotations: {unique_frames:,}\")\n",
        "    print(f\"   ğŸ“ Annotations totales: {total_annotations:,}\")\n",
        "    print(f\"   ğŸ¯ Objets suivis: {len(project['objects'])}\")\n",
        "    print(f\"   â¯ï¸  Intervalle frames: {project['metadata']['frame_interval']}\")\n",
        "    \n",
        "    # Affichage du mapping frame pour vÃ©rification\n",
        "    frame_mapping = project['metadata']['frame_mapping']\n",
        "    mapped_frames = [(i, v) for i, v in enumerate(frame_mapping) if v is not None]\n",
        "    sample_mapping = mapped_frames[:10] if len(mapped_frames) > 10 else mapped_frames\n",
        "    \n",
        "    print(f\"\\nğŸ—‚ï¸  MAPPING FRAMES (Ã©chantillon):\")\n",
        "    print(f\"   ğŸ“‹ Format: (frame_originale â†’ frame_traitÃ©e)\")\n",
        "    print(f\"   ğŸ“‹ Ã‰chantillon: {sample_mapping}\")\n",
        "    if len(mapped_frames) > 10:\n",
        "        print(f\"   ğŸ“‹ ... et {len(mapped_frames) - 10} autres frames\")\n",
        "    \n",
        "    # VÃ©rification que l'anchor frame est bien dans le mapping\n",
        "    if anchor_frame < len(frame_mapping) and frame_mapping[anchor_frame] is not None:\n",
        "        print(f\"   âœ… Frame d'ancrage {anchor_frame} correctement mappÃ©e â†’ {frame_mapping[anchor_frame]}\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸  Attention: Frame d'ancrage {anchor_frame} non trouvÃ©e dans le mapping\")\n",
        "    \n",
        "    # RÃ©sumÃ© par type d'objet\n",
        "    if project['objects']:\n",
        "        type_counts = {}\n",
        "        for obj_data in project['objects'].values():\n",
        "            obj_type = obj_data.get('type', 'unknown')\n",
        "            type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "        print(f\"   ğŸ·ï¸  Types d'objets: {dict(type_counts)}\")\n",
        "    \n",
        "    # Informations sur la rÃ©partition temporelle\n",
        "    frame_numbers = [int(f) for f in project['annotations'].keys()]\n",
        "    if frame_numbers:\n",
        "        print(f\"\\nğŸ“ˆ RÃ‰PARTITION TEMPORELLE:\")\n",
        "        print(f\"   ğŸ“Š PremiÃ¨re frame annotÃ©e: {min(frame_numbers)}\")\n",
        "        print(f\"   ğŸ“Š DerniÃ¨re frame annotÃ©e: {max(frame_numbers)}\")\n",
        "        print(f\"   ğŸ“Š Plage totale: {max(frame_numbers) - min(frame_numbers) + 1} frames\")\n",
        "    \n",
        "    print(f\"\\nâœ… Propagation bidirectionnelle terminÃ©e avec succÃ¨s!\")\n",
        "    print(f\"ğŸ’¡ Le projet est maintenant prÃªt pour la sauvegarde et la visualisation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GHlhY4X5Adf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ DÃ©marrage de la sauvegarde...\n",
            "ğŸ’¾ Sauvegarde des rÃ©sultats...\n",
            "   ğŸ“„ Fichier JSON: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\SD_13_06_2025_cam1_PdB_S1_T959s_1_project.json\n",
            "âœ… Fichier JSON sauvÃ©: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\SD_13_06_2025_cam1_PdB_S1_T959s_1_project.json\n",
            "   ğŸ“„ Version formatÃ©e: 0.4MB\n",
            "   ğŸ“„ Version compacte: 0.2MB\n",
            "\n",
            "ğŸ“Š RÃ‰SULTATS FINAUX:\n",
            "   ğŸ¬ Frames originales: 624\n",
            "   ğŸ¬ Frames traitÃ©es: 208\n",
            "   ğŸ“ Frames avec annotations: 34\n",
            "   ğŸ“ Annotations totales: 442\n",
            "   ğŸ¯ Objets suivis: 13\n",
            "   â¯ï¸  Intervalle frames: 3\n",
            "   ğŸ“„ Taille JSON: 0.4MB\n",
            "   ğŸ“ Dossier de sortie: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\n",
            "   ğŸ—‚ï¸  Mapping Ã©chantillon (originalâ†’traitÃ©): []...\n",
            "   ğŸ·ï¸  Types d'objets: {'player': 12, 'ball': 1}\n",
            "\n",
            "ğŸ§¹ Informations de nettoyage:\n",
            "ğŸ§¹ Nettoyage optionnel...\n",
            "   ğŸ–¼ï¸  Frames extraites: 34 fichiers (24.8MB)\n",
            "   ğŸ’¡ Pour libÃ©rer l'espace, vous pouvez supprimer: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\frames\n",
            "   ğŸ’¡ Les frames peuvent Ãªtre rÃ©-extraites avec FORCE_EXTRACTION=True\n",
            "\n",
            "ğŸ‰ Pipeline SAM2 terminÃ© avec succÃ¨s!\n",
            "ğŸ“„ Fichier principal: ..\\data\\videos\\outputs\\SD_13_06_2025_cam1_PdB_S1_T959s_1\\SD_13_06_2025_cam1_PdB_S1_T959s_1_project.json\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ğŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\n",
        "# =============================================================================\n",
        "\n",
        "def save_project_results(cfg, project, verbose=True):\n",
        "    \"\"\"Sauvegarde les rÃ©sultats du projet avec gestion d'erreurs\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"ğŸ’¾ Sauvegarde des rÃ©sultats...\")\n",
        "        print(f\"   ğŸ“„ Fichier JSON: {cfg.output_json_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Sauvegarde du JSON principal\n",
        "        with open(cfg.output_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(project, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"âœ… Fichier JSON sauvÃ©: {cfg.output_json_path}\")\n",
        "            \n",
        "        # Sauvegarde d'une version compacte (sans indent pour Ã©conomiser l'espace)\n",
        "        compact_json_path = cfg.output_dir / f\"{cfg.VIDEO_NAME}_project_compact.json\"\n",
        "        with open(compact_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(project, f, separators=(',', ':'), ensure_ascii=False)\n",
        "        \n",
        "        if verbose:\n",
        "            # Tailles des fichiers\n",
        "            json_size = cfg.output_json_path.stat().st_size / 1024**2  # MB\n",
        "            compact_size = compact_json_path.stat().st_size / 1024**2  # MB\n",
        "            print(f\"   ğŸ“„ Version formatÃ©e: {json_size:.1f}MB\")\n",
        "            print(f\"   ğŸ“„ Version compacte: {compact_size:.1f}MB\")\n",
        "            \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur lors de la sauvegarde: {e}\")\n",
        "        return False\n",
        "\n",
        "def display_final_statistics(cfg, project):\n",
        "    \"\"\"Affiche les statistiques finales du projet\"\"\"\n",
        "    \n",
        "    # Calcul des statistiques\n",
        "    total_annotations = sum(len(annotations) for annotations in project['annotations'].values())\n",
        "    unique_frames = len(project['annotations'])\n",
        "    \n",
        "    # Calcul de la taille des donnÃ©es\n",
        "    json_size = cfg.output_json_path.stat().st_size / 1024**2 if cfg.output_json_path.exists() else 0\n",
        "    \n",
        "    print(f\"\\nğŸ“Š RÃ‰SULTATS FINAUX:\")\n",
        "    print(f\"   ğŸ¬ Frames originales: {project['metadata']['frame_count_original']:,}\")\n",
        "    print(f\"   ğŸ¬ Frames traitÃ©es: {project['metadata']['frame_count_processed']:,}\")\n",
        "    print(f\"   ğŸ“ Frames avec annotations: {unique_frames:,}\")\n",
        "    print(f\"   ğŸ“ Annotations totales: {total_annotations:,}\")\n",
        "    print(f\"   ğŸ¯ Objets suivis: {len(project['objects'])}\")\n",
        "    print(f\"   â¯ï¸  Intervalle frames: {project['metadata']['frame_interval']}\")\n",
        "    print(f\"   ğŸ“„ Taille JSON: {json_size:.1f}MB\")\n",
        "    print(f\"   ğŸ“ Dossier de sortie: {cfg.output_dir}\")\n",
        "    \n",
        "    # Affichage d'un Ã©chantillon du mapping\n",
        "    sample_mapping = [(i, v) for i, v in enumerate(project['metadata']['frame_mapping'][:50]) if v is not None]\n",
        "    print(f\"   ğŸ—‚ï¸  Mapping Ã©chantillon (originalâ†’traitÃ©): {sample_mapping[:5]}...\")\n",
        "    \n",
        "    # RÃ©sumÃ© par type d'objet\n",
        "    if project['objects']:\n",
        "        type_counts = {}\n",
        "        for obj_data in project['objects'].values():\n",
        "            obj_type = obj_data.get('type', 'unknown')\n",
        "            type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "        print(f\"   ğŸ·ï¸  Types d'objets: {dict(type_counts)}\")\n",
        "def create_colab_backup(cfg, use_timestamp=False, custom_suffix=\"\"):\n",
        "    \"\"\"CrÃ©e une sauvegarde sur Google Drive pour Colab\"\"\"\n",
        "    \n",
        "    if not cfg.USING_COLAB:\n",
        "        print(\"ğŸ–¥ï¸ Mode local: sauvegarde Google Drive skippÃ©e\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        import shutil\n",
        "        from datetime import datetime\n",
        "        \n",
        "        print(\"ğŸ”¬ Mode Colab: CrÃ©ation de la sauvegarde Google Drive...\")\n",
        "        \n",
        "        # Monter le Drive\n",
        "        print(\"   ğŸ“ Montage Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        \n",
        "        # Construction du nom selon les options\n",
        "        if use_timestamp:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            zip_name = f\"{cfg.VIDEO_NAME}_{timestamp}\"\n",
        "        elif custom_suffix:\n",
        "            zip_name = f\"{cfg.VIDEO_NAME}_{custom_suffix}\"\n",
        "        else:\n",
        "            zip_name = cfg.VIDEO_NAME\n",
        "        \n",
        "        drive_zip_path = f'/content/drive/MyDrive/{zip_name}'\n",
        "        \n",
        "        # VÃ©rifier si le fichier existe dÃ©jÃ \n",
        "        existing_zip = Path(f\"{drive_zip_path}.zip\")\n",
        "        if existing_zip.exists():\n",
        "            existing_size = existing_zip.stat().st_size / 1024**2\n",
        "            print(f\"   ğŸ”„ Fichier existant trouvÃ©: {zip_name}.zip ({existing_size:.1f}MB) - sera Ã©crasÃ©\")\n",
        "        \n",
        "        print(f\"   ğŸ“¦ CrÃ©ation du ZIP: {zip_name}.zip\")\n",
        "        \n",
        "        # CrÃ©er le ZIP avec tout le dossier videos\n",
        "        shutil.make_archive(drive_zip_path, 'zip', str(cfg.videos_dir))\n",
        "        \n",
        "        # VÃ©rifier la taille\n",
        "        zip_path = Path(f\"{drive_zip_path}.zip\")\n",
        "        if zip_path.exists():\n",
        "            zip_size = zip_path.stat().st_size / 1024**2  # MB\n",
        "            print(f\"âœ… Sauvegarde crÃ©Ã©e: {zip_name}.zip ({zip_size:.1f}MB)\")\n",
        "            print(f\"   ğŸ“ Emplacement: MyDrive/{zip_name}.zip\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âŒ Erreur: fichier ZIP non crÃ©Ã©\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erreur sauvegarde Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "def cleanup_temporary_files(cfg):\n",
        "    \"\"\"Nettoie les fichiers temporaires (optionnel)\"\"\"\n",
        "    \n",
        "    print(\"ğŸ§¹ Nettoyage optionnel...\")\n",
        "    \n",
        "    # Comptage des frames\n",
        "    frame_count = len(list(cfg.frames_dir.glob(\"*.jpg\")))\n",
        "    frame_size = sum(f.stat().st_size for f in cfg.frames_dir.glob(\"*.jpg\")) / 1024**2\n",
        "    \n",
        "    print(f\"   ğŸ–¼ï¸  Frames extraites: {frame_count} fichiers ({frame_size:.1f}MB)\")\n",
        "    print(f\"   ğŸ’¡ Pour libÃ©rer l'espace, vous pouvez supprimer: {cfg.frames_dir}\")\n",
        "    print(f\"   ğŸ’¡ Les frames peuvent Ãªtre rÃ©-extraites avec FORCE_EXTRACTION=True\")\n",
        "\n",
        "# =============================================================================\n",
        "# ğŸš€ SAUVEGARDE AVEC Ã‰CRASEMENT PAR DÃ‰FAUT\n",
        "# =============================================================================\n",
        "\n",
        "# VÃ©rification des prÃ©requis\n",
        "if not hasattr(cfg, 'project') or cfg.project is None:\n",
        "    print(\"âŒ Projet non disponible. ExÃ©cutez d'abord la propagation.\")\n",
        "else:\n",
        "    print(f\"ğŸ’¾ DÃ©marrage de la sauvegarde...\")\n",
        "    \n",
        "    # Sauvegarde principale\n",
        "    save_success = save_project_results(cfg, cfg.project)\n",
        "    \n",
        "    if save_success:\n",
        "        # Affichage des statistiques\n",
        "        display_final_statistics(cfg, cfg.project)\n",
        "        \n",
        "        # Sauvegarde Colab si applicable\n",
        "        if cfg.USING_COLAB:\n",
        "            print(f\"\\nğŸ”¬ Sauvegarde Google Drive...\")\n",
        "            \n",
        "            # Mode par dÃ©faut : Ã©crase l'ancienne version\n",
        "            colab_success = create_colab_backup(cfg)  # use_timestamp=False par dÃ©faut\n",
        "            \n",
        "            # Si vous voulez un timestamp occasionnellement, dÃ©commentez :\n",
        "            # colab_success = create_colab_backup(cfg, use_timestamp=True)\n",
        "            \n",
        "            if colab_success:\n",
        "                print(\"âœ… Sauvegarde Google Drive rÃ©ussie!\")\n",
        "        \n",
        "        # Nettoyage optionnel\n",
        "        print(f\"\\nğŸ§¹ Informations de nettoyage:\")\n",
        "        cleanup_temporary_files(cfg)\n",
        "        \n",
        "        print(f\"\\nğŸ‰ Pipeline SAM2 terminÃ© avec succÃ¨s!\")\n",
        "        print(f\"ğŸ“„ Fichier principal: {cfg.output_json_path}\")\n",
        "        if cfg.USING_COLAB:\n",
        "            print(f\"â˜ï¸ Sauvegarde Drive: MyDrive/{cfg.VIDEO_NAME}.zip\")\n",
        "            \n",
        "    else:\n",
        "        print(\"âŒ Ã‰chec de la sauvegarde\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
